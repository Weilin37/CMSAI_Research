{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset generation -- Sequence based\n",
    "**Author: Lin Lee Cheong <br>\n",
    "Updated by: Tesfagabir Meharizghi<br>\n",
    "Date created: 12/12/ 2020 <br>\n",
    "Date updated: 02/18/2021 <br>**\n",
    "\n",
    "Goal of this synthetic dataset is to create datasets to help understand how different relationships between tokens affect attention, SHAP and other interpretability factors.\n",
    "- length of events (30, 300)\n",
    "- spacing between 2+ coupled events, i.e. order of sequence matters\n",
    "- amount of noise, i.e. performance vs interpretability\n",
    "- vocabulary space\n",
    "\n",
    "### Sequence dataset\n",
    "\n",
    "Positive label is driven by a sequence of tokens\n",
    "- Positive label probability is driven by the following formula\n",
    "``` min(1.0, math.exp(-(a * ta)) + math.exp(-(h * th)) - math.exp(-(u * tu))) ```\n",
    "Where:\n",
    "- `a` is a constant related to `_A` events. It is the inverse of the contribution of `_A` events for positive label\n",
    "- `h` is a constant related to `_H` events. It is the inverse of the contribution of `_H` events for positive label\n",
    "- `u` is a constant related to `_U` events. It is the inverse of the contribution of `_U` events for positive label\n",
    "\n",
    "- `ta` is the absolute position of the `_A` event in the sequence from the end.\n",
    "- `th` is the absolute position of the `_H` event in the sequence from the end.\n",
    "- `tu` is the absolute position of the `_U` event in the sequence from the end.\n",
    "\n",
    "Note:\n",
    "- All patients have one `_A`, one `_H` and one `_U` events each.\n",
    "- since `_U` events have opposite effect to the adverse event, their contribution is subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_NAMES_FP = \"./tokens_v2.yaml\"\n",
    "\n",
    "SEQ_LEN = 300\n",
    "\n",
    "TRAIN_FP = \"data/final_final/raw/{}/train.json\".format(SEQ_LEN)\n",
    "VAL_FP = \"data/final_final/raw/{}/val.json\".format(SEQ_LEN)\n",
    "TEST_FP = \"data/final_final/raw/{}/test.json\".format(SEQ_LEN)\n",
    "\n",
    "TRAIN_FP_DF0 = \"data/final_final/event_based/{}/train_org0.csv\".format(SEQ_LEN)\n",
    "VAL_FP_DF0 = \"data/final_final/event_based/{}/val_org0.csv\".format(SEQ_LEN)\n",
    "TEST_FP_DF0 = \"data/final_final/event_based/{}/test_org0.csv\".format(SEQ_LEN)\n",
    "\n",
    "TRAIN_FP_DF = \"data/final_final/event_based/{}/train_orig.csv\".format(SEQ_LEN)\n",
    "VAL_FP_DF = \"data/final_final/event_based/{}/val_orig.csv\".format(SEQ_LEN)\n",
    "TEST_FP_DF = \"data/final_final/event_based/{}/test_orig.csv\".format(SEQ_LEN)\n",
    "\n",
    "UID_COLNAME = \"patient_id\"\n",
    "\n",
    "TRAIN_NROWS = 4000\n",
    "VAL_NROWS = 2000\n",
    "TEST_NROWS = 2000\n",
    "\n",
    "UID_LEN = 10\n",
    "\n",
    "# Total patients in the each split (will be balanced)\n",
    "TOTAL_TRAIN = 18000\n",
    "TOTAL_VAL = 6000\n",
    "TOTAL_TEST = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(TRAIN_FP_DF0)\n",
    "# df_val = pd.read_csv(VAL_FP_DF0)\n",
    "# df_test = pd.read_csv(TEST_FP_DF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reverse_events(row):\n",
    "#     seq_len = 30\n",
    "#     columns = [str(i) for i in range(seq_len - 1, -1, -1)]\n",
    "#     row2 = row[columns].tolist()\n",
    "#     row2.reverse()\n",
    "#     row[columns] = row2[:]\n",
    "#     return row.tolist()\n",
    "\n",
    "\n",
    "# # Train data\n",
    "# columns = df_train.columns\n",
    "# results = df_train.apply(reverse_events, axis=1)\n",
    "# results = [np.array(res) for res in results]\n",
    "# df = pd.DataFrame(np.array(results), columns=columns)\n",
    "# # df.sample(frac=1)\n",
    "# save_csv(df, TRAIN_FP_DF)\n",
    "\n",
    "# # Val data\n",
    "# columns = df_val.columns\n",
    "# results = df_val.apply(reverse_events, axis=1)\n",
    "# results = [np.array(res) for res in results]\n",
    "# df = pd.DataFrame(np.array(results), columns=columns)\n",
    "# # df.sample(frac=1)\n",
    "# save_csv(df, VAL_FP_DF)\n",
    "\n",
    "# # Test data\n",
    "# columns = df_test.columns\n",
    "# results = df_test.apply(reverse_events, axis=1)\n",
    "# results = [np.array(res) for res in results]\n",
    "# df = pd.DataFrame(np.array(results), columns=columns)\n",
    "# # df.sample(frac=1)\n",
    "# save_csv(df, TEST_FP_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse_tokens: 10 tokens\n",
      "adverse_helper_tokens: 10 tokens\n",
      "adverse_unhelper_tokens: 10 tokens\n",
      "noise_tokens: 15 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load tokens from yaml file path\n",
    "tokens = load_tokens(TOKEN_NAMES_FP)\n",
    "for key in tokens.keys():\n",
    "    print(f\"{key}: {len(tokens[key])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, tok in tokens.items():\n",
    "#     print(key)\n",
    "#     print(tok)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 90%\n",
    "    * 2 adverse + 1 helper\n",
    "* 80%\n",
    "    * 1 adverse + 2 helper\n",
    "* 70%\n",
    "    * 1 adverse + 1 helper\n",
    "* 40%\n",
    "    * 1 helper + 1 unhelper\n",
    "* 30%\n",
    "    * 1 adverse + 2 unhelper\n",
    "* 20%\n",
    "    * 1 helper + 2 unhelper\n",
    "* 10%\n",
    "    * 2 unhelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_FP, \"r\") as fp:\n",
    "    json_train = json.load(fp)\n",
    "\n",
    "with open(VAL_FP, \"r\") as fp:\n",
    "    json_val = json.load(fp)\n",
    "\n",
    "with open(TEST_FP, \"r\") as fp:\n",
    "    json_test = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_val[\"AAH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COUNTS = 3000\n",
    "VAL_COUNTS = 1000\n",
    "TEST_COUNTS = 1000\n",
    "\n",
    "TRAIN_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, TRAIN_COUNTS],\n",
    "    \"AHH\": [0.8, TRAIN_COUNTS],\n",
    "    \"AH\": [0.7, TRAIN_COUNTS],\n",
    "    \"HU\": [0.4, TRAIN_COUNTS],\n",
    "    \"AUU\": [0.3, TRAIN_COUNTS],\n",
    "    \"HUU\": [0.2, TRAIN_COUNTS],\n",
    "    \"UU\": [0.1, TRAIN_COUNTS],\n",
    "}\n",
    "\n",
    "VAL_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, VAL_COUNTS],\n",
    "    \"AHH\": [0.8, VAL_COUNTS],\n",
    "    \"AH\": [0.7, VAL_COUNTS],\n",
    "    \"HU\": [0.4, VAL_COUNTS],\n",
    "    \"AUU\": [0.3, VAL_COUNTS],\n",
    "    \"HUU\": [0.2, VAL_COUNTS],\n",
    "    \"UU\": [0.1, VAL_COUNTS],\n",
    "}\n",
    "\n",
    "TEST_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, TEST_COUNTS],\n",
    "    \"AHH\": [0.8, TEST_COUNTS],\n",
    "    \"AH\": [0.7, TEST_COUNTS],\n",
    "    \"HU\": [0.4, TEST_COUNTS],\n",
    "    \"AUU\": [0.3, TEST_COUNTS],\n",
    "    \"HUU\": [0.2, TEST_COUNTS],\n",
    "    \"UU\": [0.1, TEST_COUNTS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings of the token groups with the abbreviation\n",
    "TOKEN_MAPPINGS = {\n",
    "    \"A\": \"adverse_tokens\",\n",
    "    \"H\": \"adverse_helper_tokens\",\n",
    "    \"U\": \"adverse_unhelper_tokens\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(df0, label, total):\n",
    "#     \"\"\"Downsample the dataset to make it balanced class.\"\"\"\n",
    "#     df = df0.copy()\n",
    "#     df_c0 = df[df[label] == 0]\n",
    "#     df_c1 = df[df[label] == 1]\n",
    "\n",
    "#     df_c0 = df_c0.sample(int(total / 2))\n",
    "#     df_c1 = df_c1.sample(int(total / 2))\n",
    "\n",
    "#     df = pd.concat([df_c0, df_c1], axis=0)\n",
    "#     df = df.sample(frac=1)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def get_proba(seq, base_seq_len=30):\n",
    "#     \"\"\"Get probability of being positive label for a sequence.\"\"\"\n",
    "\n",
    "#     def get_position(seq, substring, base_seq_len):\n",
    "#         \"\"\"Get position of event with substring from end of sequence\"\"\"\n",
    "#         pos = -1\n",
    "#         for i, event in enumerate(seq):\n",
    "#             if event.endswith(substring):\n",
    "#                 pos = i\n",
    "#                 break\n",
    "#         if pos == -1:\n",
    "#             raise ValueError(f\"Error! {substring} not found!\")\n",
    "\n",
    "#         pos = len(seq) - pos - 1\n",
    "#         return pos\n",
    "\n",
    "#         a = 0.1  # Constant for Adverse\n",
    "#         h = 0.5  # Constant for helper\n",
    "#         u = 0.95  # Constant for unhelper\n",
    "\n",
    "#     #     a = 0.03  # Constant for Adverse\n",
    "#     #     h = 0.05  # Constant for helper\n",
    "#     #     u = 0.09  # Constant for unhelper\n",
    "\n",
    "#     seq_len = len(seq)\n",
    "#     multiplier = float(base_seq_len) / seq_len\n",
    "#     ta = get_position(seq, \"_A\", base_seq_len) * multiplier\n",
    "#     th = get_position(seq, \"_H\", base_seq_len) * multiplier\n",
    "#     tu = get_position(seq, \"_U\", base_seq_len) * multiplier\n",
    "\n",
    "#     prob = min(1.0, math.exp(-(a * ta)) + math.exp(-(h * th)) - math.exp(-(u * tu)))\n",
    "#     prob = round(prob, 4)\n",
    "#     return prob\n",
    "\n",
    "\n",
    "def get_a_sequence_seq_v2(\n",
    "    seq_len, label, tokens, token_mappings, seq_tokens, proba, json_row=None\n",
    "):\n",
    "    \"\"\"creates sequence + label (at the end of list) with specific orderings.\n",
    "    returns list of list\"\"\"\n",
    "    n_seq_tokens = len(seq_tokens)\n",
    "\n",
    "    min_n_noise = 10\n",
    "    if json_row is not None:\n",
    "        # {'39': 'electrolyte_imbalance_H', '45': 'hypertension_A', '137': 'cardiomyopathy_A'}\n",
    "        row_inds = list(json_row.keys())\n",
    "        row_tokens = list(json_row.values())\n",
    "        row_inds = [int(indx) for indx in row_inds]\n",
    "        min_n_noise = min(seq_len, max(row_inds) + 1)\n",
    "\n",
    "    n_noise = (\n",
    "        np.max(\n",
    "            (\n",
    "                min_n_noise,\n",
    "                random.choices(range(n_seq_tokens, seq_len), k=1)[0],\n",
    "            )\n",
    "        )\n",
    "        - (n_seq_tokens)\n",
    "    )\n",
    "    if json_row is None:\n",
    "        sel_positions = sorted(random.sample(range(n_noise), k=n_seq_tokens))\n",
    "        sel_tokens = []\n",
    "        for key in seq_tokens:\n",
    "            key_mapping = token_mappings[key]\n",
    "            sel_tokens.append(random.choices(tokens[key_mapping])[0])\n",
    "\n",
    "        # Randomize sequence\n",
    "        random.shuffle(sel_tokens)\n",
    "\n",
    "        sel_tokens = list(zip(sel_positions, sel_tokens))\n",
    "    else:\n",
    "        sel_tokens = list(zip(row_inds, row_tokens))\n",
    "\n",
    "    sel_noise = get_tokens(seq_len, tokens, \"noise_tokens\", n_noise)\n",
    "\n",
    "    for idx, event in sel_tokens:\n",
    "        sel_noise.insert(idx, event)\n",
    "\n",
    "    sel_noise = [\"<pad>\"] * (seq_len - len(sel_noise)) + sel_noise\n",
    "\n",
    "    # Get probability of being positive label\n",
    "    # sel_noise.reverse()\n",
    "    sim_lab = get_label(proba, target=label)\n",
    "\n",
    "    sequence = sel_noise + [proba] + [sim_lab]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def get_sequences_v2(\n",
    "    seq_len,\n",
    "    label,\n",
    "    uid_len,\n",
    "    uid_colname,\n",
    "    tokens,\n",
    "    token_mappings,\n",
    "    seq_tokens,\n",
    "    n_seq,\n",
    "    proba,\n",
    "    json_group=None,\n",
    "):\n",
    "    \"\"\"Get multiple sequences.\"\"\"\n",
    "\n",
    "    if json_group is None:\n",
    "        sequences = [\n",
    "            get_a_sequence_seq_v2(\n",
    "                seq_len, label, tokens, token_mappings, seq_tokens, proba, None\n",
    "            )\n",
    "            + [get_uid(uid_len)]\n",
    "            for _ in range(n_seq)\n",
    "        ]\n",
    "    else:\n",
    "        sequences = [\n",
    "            get_a_sequence_seq_v2(\n",
    "                seq_len, label, tokens, token_mappings, seq_tokens, proba, json_row\n",
    "            )\n",
    "            + [get_uid(uid_len)]\n",
    "            for json_row in json_group\n",
    "        ]\n",
    "    # print(f\"seq based events generated\")\n",
    "\n",
    "    seq_df = pd.DataFrame(sequences)\n",
    "    seq_df.columns = [str(x) for x in range(seq_len - 1, -1, -1)] + [\n",
    "        \"proba\",\n",
    "        \"label\",\n",
    "        uid_colname,\n",
    "    ]\n",
    "\n",
    "    return seq_df\n",
    "\n",
    "\n",
    "def get_sequence_dataset(\n",
    "    seq_len,\n",
    "    uid_len,\n",
    "    uid_colname,\n",
    "    count_dict,\n",
    "    tokens,\n",
    "    token_mappings,\n",
    "    total_rows,\n",
    "    json_data=None,\n",
    "):\n",
    "    \"\"\"Generate a simple toy dataset.\n",
    "\n",
    "    Arg:\n",
    "    -----\n",
    "        seq_len (int) : length of the generated sequence\n",
    "        uid_len (int) : length of uid token\n",
    "        uid_colname (str) : name of uid column, usually patient_id\n",
    "        count_dict (dict) : dictionary of various sequence types.\n",
    "            6 different types are allowed:\n",
    "                n_ppp_adverse, n_pp_adverse, n_p_adverse\n",
    "                n_nnn_adverse, n_nn_adverse, n_n_adverse\n",
    "        tokens (dict) : dictionary of the various token types\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        dataset (dataframe) : dataframe containing all the\n",
    "                              generated dataset, randomly mixed\n",
    "\n",
    "    \"\"\"\n",
    "    label = 1\n",
    "    cat_lst = []\n",
    "    for seq_tokens, (proba, n_seq) in count_dict.items():\n",
    "        json_group = None\n",
    "        if json_data is not None:\n",
    "            json_group = json_data[seq_tokens]\n",
    "        df = get_sequences_v2(\n",
    "            seq_len,\n",
    "            label,\n",
    "            uid_len,\n",
    "            uid_colname,\n",
    "            tokens,\n",
    "            token_mappings,\n",
    "            seq_tokens,\n",
    "            n_seq,\n",
    "            proba,\n",
    "            json_group,\n",
    "        )\n",
    "\n",
    "        df[\"seq_event\"] = seq_tokens\n",
    "        cat_lst.append(df.copy())\n",
    "    dataset = pd.concat(cat_lst, axis=0)\n",
    "    dataset.reset_index(inplace=True)\n",
    "    indexes = [idx for idx in range(dataset.shape[0])]\n",
    "    if json_group is None:\n",
    "        random.shuffle(indexes)\n",
    "        dataset = dataset.iloc[indexes, :]\n",
    "    # dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # dataset = downsample(dataset, \"label\", total_rows)\n",
    "    print(f\"dataset: {dataset.shape}\")\n",
    "    print(f\"ratio:\\n{dataset.label.value_counts(normalize=True)}\\n\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Imbalance for seq_len=300...\n",
      "dataset: (21000, 305)\n",
      "ratio:\n",
      "0    0.514286\n",
      "1    0.485714\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Val Data Imbalance for seq_len=300...\n",
      "dataset: (7000, 305)\n",
      "ratio:\n",
      "0    0.523429\n",
      "1    0.476571\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Test Data Imbalance for seq_len=300...\n",
      "dataset: (7000, 305)\n",
      "ratio:\n",
      "0    0.513\n",
      "1    0.487\n",
      "Name: label, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_train = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=TRAIN_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=TRAIN_COUNTS,\n",
    "    json_data=json_train,\n",
    ")\n",
    "\n",
    "print(f\"Val Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_val = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=VAL_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=VAL_COUNTS,\n",
    "    json_data=json_val,\n",
    ")\n",
    "\n",
    "print(f\"Test Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_test = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=TEST_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=TEST_COUNTS,\n",
    "    json_data=json_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 305)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>proba</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seq_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>12DP15TQ9W</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>JET8LDGL4Y</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>Acute_Myocardial_Infarction_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>GNS7WLNPUP</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>hypertension_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>IJ650PXV3S</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>arrhythmia_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>9H8H8D7V97</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    299    298    297    296    295    294    293    292    291  ...  \\\n",
       "0      0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1      1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2      2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3      3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4      4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "                   5                 4               3               2  \\\n",
       "0  annual_physical_N     quad_injury_N      backache_N      backache_N   \n",
       "1        foot_pain_N       hay_fever_N        myopia_N     hay_fever_N   \n",
       "2   peanut_allergy_N  peanut_allergy_N  ankle_sprain_N      ACL_tear_N   \n",
       "3         eye_exam_N          myopia_N  ingrown_nail_N  ingrown_nail_N   \n",
       "4      dental_exam_N        backache_N      ACL_tear_N     hay_fever_N   \n",
       "\n",
       "                  1                              0 proba label  patient_id  \\\n",
       "0  peanut_allergy_N              annual_physical_N   0.9     0  12DP15TQ9W   \n",
       "1       cold_sore_N                     eye_exam_N   0.9     1  JET8LDGL4Y   \n",
       "2     dental_exam_N  Acute_Myocardial_Infarction_A   0.9     1  GNS7WLNPUP   \n",
       "3      cut_finger_N                 hypertension_A   0.9     1  IJ650PXV3S   \n",
       "4        eye_exam_N                   arrhythmia_A   0.9     1  9H8H8D7V97   \n",
       "\n",
       "  seq_event  \n",
       "0       AAH  \n",
       "1       AAH  \n",
       "2       AAH  \n",
       "3       AAH  \n",
       "4       AAH  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# df_train.sort_values(\"proba\")[::-1]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/final_final/event_based/300/train_orig.csv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FP_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(df_train, TRAIN_FP_DF)\n",
    "save_csv(df_val, VAL_FP_DF)\n",
    "save_csv(df_test, TEST_FP_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sequence-based dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 300\n",
    "\n",
    "# Input from event-based\n",
    "TRAIN_FP_DF = \"data/final_final/event_based/{}/train_orig.csv\".format(SEQ_LEN)\n",
    "VAL_FP_DF = \"data/final_final/event_based/{}/val_orig.csv\".format(SEQ_LEN)\n",
    "TEST_FP_DF = \"data/final_final/event_based/{}/test_orig.csv\".format(SEQ_LEN)\n",
    "\n",
    "# Labels from Seq-based\n",
    "TRAIN_SEQ_LABELS_FP = \"data/final_final/raw/{}/seq_based_labels/label_train.json\".format(\n",
    "    SEQ_LEN\n",
    ")\n",
    "VAL_SEQ_LABELS_FP = \"data/final_final/raw/{}/seq_based_labels/label_val.json\".format(SEQ_LEN)\n",
    "TEST_SEQ_LABELS_FP = \"data/final_final/raw/{}/seq_based_labels/label_test.json\".format(SEQ_LEN)\n",
    "\n",
    "# Output for seq-based from event-based\n",
    "TRAIN_FP_DF2 = \"data/final_final/seq_based/{}/train_orig.csv\".format(SEQ_LEN)\n",
    "VAL_FP_DF2 = \"data/final_final/seq_based/{}/val_orig.csv\".format(SEQ_LEN)\n",
    "TEST_FP_DF2 = \"data/final_final/seq_based/{}/test_orig.csv\".format(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_FP_DF)\n",
    "df_val = pd.read_csv(VAL_FP_DF)\n",
    "df_test = pd.read_csv(TEST_FP_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_SEQ_LABELS_FP, \"r\") as fp:\n",
    "    json_train_seq = json.load(fp)\n",
    "\n",
    "with open(VAL_SEQ_LABELS_FP, \"r\") as fp:\n",
    "    json_val_seq = json.load(fp)\n",
    "\n",
    "with open(TEST_SEQ_LABELS_FP, \"r\") as fp:\n",
    "    json_test_seq = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>proba</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>12DP15TQ9W</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>JET8LDGL4Y</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>Acute_Myocardial_Infarction_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>GNS7WLNPUP</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>hypertension_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>IJ650PXV3S</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>arrhythmia_A</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>9H8H8D7V97</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    299    298    297    296    295    294    293    292    291  ...  \\\n",
       "0      0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1      1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2      2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3      3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4      4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "                   5                 4               3               2  \\\n",
       "0  annual_physical_N     quad_injury_N      backache_N      backache_N   \n",
       "1        foot_pain_N       hay_fever_N        myopia_N     hay_fever_N   \n",
       "2   peanut_allergy_N  peanut_allergy_N  ankle_sprain_N      ACL_tear_N   \n",
       "3         eye_exam_N          myopia_N  ingrown_nail_N  ingrown_nail_N   \n",
       "4      dental_exam_N        backache_N      ACL_tear_N     hay_fever_N   \n",
       "\n",
       "                  1                              0 proba label  patient_id  \\\n",
       "0  peanut_allergy_N              annual_physical_N   0.9     0  12DP15TQ9W   \n",
       "1       cold_sore_N                     eye_exam_N   0.9     1  JET8LDGL4Y   \n",
       "2     dental_exam_N  Acute_Myocardial_Infarction_A   0.9     1  GNS7WLNPUP   \n",
       "3      cut_finger_N                 hypertension_A   0.9     1  IJ650PXV3S   \n",
       "4        eye_exam_N                   arrhythmia_A   0.9     1  9H8H8D7V97   \n",
       "\n",
       "  category  \n",
       "0      AAH  \n",
       "1      AAH  \n",
       "2      AAH  \n",
       "3      AAH  \n",
       "4      AAH  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (21000, 306)\n",
      "21000 21000\n",
      "Val (7000, 306)\n",
      "7000 7000\n",
      "Test (7000, 306)\n",
      "7000 7000\n"
     ]
    }
   ],
   "source": [
    "# Train Data\n",
    "df = df_train.copy()\n",
    "json_data = json_train_seq.copy()\n",
    "\n",
    "all_labels = []\n",
    "all_seq_events = []\n",
    "for seq_event, labels in json_data.items():\n",
    "    all_labels += labels\n",
    "    all_seq_events += [seq_event] * len(labels)\n",
    "df[\"label\"] = all_labels[:]\n",
    "df[\"seq_event\"] = all_seq_events\n",
    "print(\"Train\", df.shape)\n",
    "print(df.shape[0], sum(df[\"category\"] == df[\"seq_event\"]))\n",
    "if \"proba\" in df.columns:\n",
    "    del df[\"proba\"]\n",
    "del df[\"category\"]\n",
    "\n",
    "save_csv(df, TRAIN_FP_DF2)\n",
    "\n",
    "# Val Data\n",
    "df = df_val.copy()\n",
    "json_data = json_val_seq.copy()\n",
    "\n",
    "all_labels = []\n",
    "all_seq_events = []\n",
    "for seq_event, labels in json_data.items():\n",
    "    all_labels += labels\n",
    "    all_seq_events += [seq_event] * len(labels)\n",
    "df[\"label\"] = all_labels[:]\n",
    "df[\"seq_event\"] = all_seq_events\n",
    "print(\"Val\", df.shape)\n",
    "print(df.shape[0], sum(df[\"category\"] == df[\"seq_event\"]))\n",
    "if \"proba\" in df.columns:\n",
    "    del df[\"proba\"]\n",
    "del df[\"category\"]\n",
    "save_csv(df, VAL_FP_DF2)\n",
    "\n",
    "# Test Data\n",
    "df = df_test.copy()\n",
    "json_data = json_test_seq.copy()\n",
    "\n",
    "all_labels = []\n",
    "all_seq_events = []\n",
    "for seq_event, labels in json_data.items():\n",
    "    all_labels += labels\n",
    "    all_seq_events += [seq_event] * len(labels)\n",
    "df[\"label\"] = all_labels[:]\n",
    "df[\"seq_event\"] = all_seq_events\n",
    "print(\"Test\", df.shape)\n",
    "print(df.shape[0], sum(df[\"category\"] == df[\"seq_event\"]))\n",
    "if \"proba\" in df.columns:\n",
    "    del df[\"proba\"]\n",
    "del df[\"category\"]\n",
    "save_csv(df, TEST_FP_DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seq_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>1</td>\n",
       "      <td>MEVFK5VO3K</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>1</td>\n",
       "      <td>D33UGF3XH6</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>Chronic_Obstructive_Pulmonary_Disease_A</td>\n",
       "      <td>Brain_Natriuretic_Peptide_H</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>1</td>\n",
       "      <td>RS0NOAZ6KR</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>1</td>\n",
       "      <td>WCOHV9D8Q2</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>1</td>\n",
       "      <td>UPT4WL3MHA</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    299    298    297    296    295    294    293    292    291  ...  \\\n",
       "0      0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1      1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2      2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3      3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4      4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "                   6                                        5  \\\n",
       "0       cut_finger_N                               eye_exam_N   \n",
       "1  annual_physical_N                         peanut_allergy_N   \n",
       "2      dental_exam_N  Chronic_Obstructive_Pulmonary_Disease_A   \n",
       "3     ankle_sprain_N                             cut_finger_N   \n",
       "4   peanut_allergy_N                               headache_N   \n",
       "\n",
       "                             4              3            2               1  \\\n",
       "0                dental_exam_N     backache_N     myopia_N  ankle_sprain_N   \n",
       "1                  hay_fever_N       myopia_N     myopia_N     cold_sore_N   \n",
       "2  Brain_Natriuretic_Peptide_H  quad_injury_N   ACL_tear_N        myopia_N   \n",
       "3            annual_physical_N       myopia_N  hay_fever_N  ingrown_nail_N   \n",
       "4                quad_injury_N     ACL_tear_N  cold_sore_N      eye_exam_N   \n",
       "\n",
       "                   0 label  patient_id seq_event  \n",
       "0         eye_exam_N     1  MEVFK5VO3K       AAH  \n",
       "1        hay_fever_N     1  D33UGF3XH6       AAH  \n",
       "2           myopia_N     1  RS0NOAZ6KR       AAH  \n",
       "3      quad_injury_N     1  WCOHV9D8Q2       AAH  \n",
       "4  annual_physical_N     1  UPT4WL3MHA       AAH  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/final_final/event_based/300/train_orig.csv'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FP_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next\n",
    "# SHUFFLE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling Data...\n",
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [30, 300]\n",
    "dtypes = [\"event_based\", \"seq_based\"]\n",
    "print(\"Shuffling Data...\")\n",
    "for SEQ_LEN in seq_lens:\n",
    "    for dtype in dtypes:\n",
    "        train_path_in = f\"data/final_final/{dtype}/{SEQ_LEN}/train_orig.csv\"\n",
    "        val_path_in = f\"data/final_final/{dtype}/{SEQ_LEN}/val_orig.csv\"\n",
    "        test_path_in = f\"data/final_final/{dtype}/{SEQ_LEN}/test_orig.csv\"\n",
    "\n",
    "        train_path_out = f\"data/final_final/{dtype}/{SEQ_LEN}/train.csv\"\n",
    "        val_path_out = f\"data/final_final/{dtype}/{SEQ_LEN}/val.csv\"\n",
    "        test_path_out = f\"data/final_final/{dtype}/{SEQ_LEN}/test.csv\"\n",
    "\n",
    "        df_train = pd.read_csv(train_path_in)\n",
    "        df_val = pd.read_csv(val_path_in)\n",
    "        df_test = pd.read_csv(test_path_in)\n",
    "\n",
    "        df_train = df_train.sample(frac=1, random_state=42)\n",
    "        df_val = df_val.sample(frac=1, random_state=42)\n",
    "        df_test = df_test.sample(frac=1, random_state=42)\n",
    "\n",
    "        save_csv(df_train, train_path_out)\n",
    "        save_csv(df_val, val_path_out)\n",
    "        save_csv(df_test, test_path_out)\n",
    "print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df0, count_dict, seq_len, output_path):\n",
    "    \"\"\"Process data and converting to list of dicts.\"\"\"\n",
    "    print(\"Processing data...\")\n",
    "    feature_names = [str(i) for i in range(seq_len - 1, -1, -1)]\n",
    "\n",
    "    data = {}\n",
    "    for category, values in count_dict.items():\n",
    "        data[category] = []\n",
    "        df = df0[df0[\"seq_event\"] == category]\n",
    "        df = df[feature_names]\n",
    "        n_rows = df.shape[0]\n",
    "        for idx in range(n_rows):\n",
    "            row = df.iloc[idx].tolist()\n",
    "            row.reverse()\n",
    "            row = dict(zip(range(SEQ_LEN), row))\n",
    "            row2 = row.copy()\n",
    "            for key, value in row.items():\n",
    "                if value.endswith(\"_N\") or value == \"<pad>\":\n",
    "                    del row2[key]\n",
    "            data[category].append(row2.copy())\n",
    "\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "SUCCESS!\n",
      "Processing data...\n",
      "SUCCESS!\n",
      "Processing data...\n",
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "# process(df_train, TRAIN_COUNT_DICT, SEQ_LEN, TRAIN_FP)\n",
    "# process(df_val, VAL_COUNT_DICT, SEQ_LEN, VAL_FP)\n",
    "# process(df_test, TEST_COUNT_DICT, SEQ_LEN, TEST_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(TRAIN_FP)\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "train_data_org = f\"/home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/data/final_final/event_based/30/{split}_org.csv\"\n",
    "train_json_org = f\"/home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/data/final_final/raw/30/{split}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_data_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_json_org, \"r\") as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>29</th>\n",
       "      <th>28</th>\n",
       "      <th>27</th>\n",
       "      <th>26</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>23</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>20</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>pneumonia_H</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>...</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>XLDQ61URWF</td>\n",
       "      <td>1</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>...</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>electrolyte_imbalance_H</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>SSC6U2R69W</td>\n",
       "      <td>1</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>...</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>ILIDFNY4YX</td>\n",
       "      <td>1</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>ventricular_aneurysm_A</td>\n",
       "      <td>hypertension_A</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>coronary_artery_disease_H</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>...</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>D5KAFPVMK4</td>\n",
       "      <td>1</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>pulmonary_embolism_A</td>\n",
       "      <td>cardiomyopathy_A</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>sleep_apnea_H</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>CBC97BW27T</td>\n",
       "      <td>1</td>\n",
       "      <td>AAH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  29                      28                 27  \\\n",
       "0  annual_physical_N             pneumonia_H  annual_physical_N   \n",
       "1        hay_fever_N              eye_exam_N           myopia_N   \n",
       "2      dental_exam_N       annual_physical_N         eye_exam_N   \n",
       "3        cold_sore_N  ventricular_aneurysm_A     hypertension_A   \n",
       "4       cut_finger_N           quad_injury_N         eye_exam_N   \n",
       "\n",
       "                     26                         25                 24  \\\n",
       "0            eye_exam_N              quad_injury_N       cut_finger_N   \n",
       "1           hay_fever_N                hay_fever_N         headache_N   \n",
       "2            eye_exam_N                hay_fever_N      dental_exam_N   \n",
       "3              myopia_N  coronary_artery_disease_H  annual_physical_N   \n",
       "4  pulmonary_embolism_A           cardiomyopathy_A         headache_N   \n",
       "\n",
       "                  23             22                 21                 20  \\\n",
       "0           myopia_N  dental_exam_N         ACL_tear_N        foot_pain_N   \n",
       "1  annual_physical_N     headache_N  annual_physical_N  annual_physical_N   \n",
       "2  annual_physical_N     backache_N        foot_pain_N         eye_exam_N   \n",
       "3         ACL_tear_N     ACL_tear_N         backache_N  annual_physical_N   \n",
       "4  annual_physical_N  sleep_apnea_H   peanut_allergy_N        cold_sore_N   \n",
       "\n",
       "   ...             6                        5               4               3  \\\n",
       "0  ...    headache_N                    <pad>           <pad>           <pad>   \n",
       "1  ...  cut_finger_N  electrolyte_imbalance_H      headache_N   quad_injury_N   \n",
       "2  ...    ACL_tear_N               backache_N      eye_exam_N   dental_exam_N   \n",
       "3  ...   cold_sore_N           ingrown_nail_N  ingrown_nail_N  ankle_sprain_N   \n",
       "4  ...         <pad>                    <pad>           <pad>           <pad>   \n",
       "\n",
       "                2      1      0  patient_id label category  \n",
       "0           <pad>  <pad>  <pad>  XLDQ61URWF     1      AAH  \n",
       "1  ingrown_nail_N  <pad>  <pad>  SSC6U2R69W     1      AAH  \n",
       "2     hay_fever_N  <pad>  <pad>  ILIDFNY4YX     1      AAH  \n",
       "3           <pad>  <pad>  <pad>  D5KAFPVMK4     1      AAH  \n",
       "4           <pad>  <pad>  <pad>  CBC97BW27T     1      AAH  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myopia_N', 'headache_N', 'ingrown_nail_N', 'peanut_allergy_N', 'cut_finger_N', 'cut_finger_N', 'peanut_allergy_N', 'Acute_Myocardial_Infarction_A', 'headache_N', 'Acute_Myocardial_Infarction_A', 'troponin_H', 'annual_physical_N', 'cut_finger_N', 'annual_physical_N', 'cold_sore_N', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'HBTMH73L7U', 1, 'AAH']\n"
     ]
    }
   ],
   "source": [
    "# print(df.shape)\n",
    "print(df.iloc[5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4': 'troponin_H',\n",
       " '5': 'Acute_Myocardial_Infarction_A',\n",
       " '7': 'Acute_Myocardial_Infarction_A'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"AAH\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
