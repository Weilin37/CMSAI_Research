{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset generation -- Sequence based\n",
    "**Author: Lin Lee Cheong <br>\n",
    "Updated by: Tesfagabir Meharizghi<br>\n",
    "Date created: 12/12/ 2020 <br>\n",
    "Date updated: 02/18/2021 <br>**\n",
    "\n",
    "Goal of this synthetic dataset is to create datasets to help understand how different relationships between tokens affect attention, SHAP and other interpretability factors.\n",
    "- length of events (30, 300)\n",
    "- spacing between 2+ coupled events, i.e. order of sequence matters\n",
    "- amount of noise, i.e. performance vs interpretability\n",
    "- vocabulary space\n",
    "\n",
    "### Sequence dataset\n",
    "\n",
    "Positive label is driven by a sequence of tokens\n",
    "- Positive label probability is driven by the following formula\n",
    "``` min(1.0, math.exp(-(a * ta)) + math.exp(-(h * th)) - math.exp(-(u * tu))) ```\n",
    "Where:\n",
    "- `a` is a constant related to `_A` events. It is the inverse of the contribution of `_A` events for positive label\n",
    "- `h` is a constant related to `_H` events. It is the inverse of the contribution of `_H` events for positive label\n",
    "- `u` is a constant related to `_U` events. It is the inverse of the contribution of `_U` events for positive label\n",
    "\n",
    "- `ta` is the absolute position of the `_A` event in the sequence from the end.\n",
    "- `th` is the absolute position of the `_H` event in the sequence from the end.\n",
    "- `tu` is the absolute position of the `_U` event in the sequence from the end.\n",
    "\n",
    "Note:\n",
    "- All patients have one `_A`, one `_H` and one `_U` events each.\n",
    "- since `_U` events have opposite effect to the adverse event, their contribution is subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_NAMES_FP = \"./tokens_v2.yaml\"\n",
    "\n",
    "SEQ_LEN = 300\n",
    "\n",
    "TRAIN_FP = \"data/final_final/raw/{}/train.json\".format(SEQ_LEN)\n",
    "VAL_FP = \"data/final_final/raw/{}/val.json\".format(SEQ_LEN)\n",
    "TEST_FP = \"data/final_final/raw/{}/test.json\".format(SEQ_LEN)\n",
    "\n",
    "UID_COLNAME = \"patient_id\"\n",
    "\n",
    "TRAIN_NROWS = 4000\n",
    "VAL_NROWS = 2000\n",
    "TEST_NROWS = 2000\n",
    "\n",
    "UID_LEN = 10\n",
    "\n",
    "# Total patients in the each split (will be balanced)\n",
    "TOTAL_TRAIN = 18000\n",
    "TOTAL_VAL = 6000\n",
    "TOTAL_TEST = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse_tokens: 10 tokens\n",
      "adverse_helper_tokens: 10 tokens\n",
      "adverse_unhelper_tokens: 10 tokens\n",
      "noise_tokens: 15 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load tokens from yaml file path\n",
    "tokens = load_tokens(TOKEN_NAMES_FP)\n",
    "for key in tokens.keys():\n",
    "    print(f\"{key}: {len(tokens[key])} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse_tokens\n",
      "['Acute_Myocardial_Infarction_A', 'hypertension_A', 'arrhythmia_A', 'congestive_heart_failure_A', 'heart_valve_failure_A', 'pulmonary_embolism_A', 'ventricular_aneurysm_A', 'ventricular_hypertrophy_A', 'cardiomyopathy_A', 'Chronic_Obstructive_Pulmonary_Disease_A']\n",
      "--------------------------------------------------\n",
      "adverse_helper_tokens\n",
      "['sleep_apnea_H', 'pneumonia_H', 'coronary_artery_disease_H', 'edema_H', 'troponin_H', 'Brain_Natriuretic_Peptide_H', 'alchoholism_H', 'metabolic_disorder_H', 'elevated_creatinine_H', 'electrolyte_imbalance_H']\n",
      "--------------------------------------------------\n",
      "adverse_unhelper_tokens\n",
      "['Percutaneous_Coronary_Intervention_U', 'electrical_cardioversion_U', 'catheter_ablation_U', 'pacemaker_U', 'cardiac_rehab_U', 'sleep_apnea_treatment_U', 'ACE_inhibitors_U', 'ARB_U', 'diuretics_U', 'beta_blockers_U']\n",
      "--------------------------------------------------\n",
      "noise_tokens\n",
      "['eye_exam_N', 'annual_physical_N', 'hay_fever_N', 'headache_N', 'foot_pain_N', 'backache_N', 'cold_sore_N', 'myopia_N', 'cut_finger_N', 'ankle_sprain_N', 'ACL_tear_N', 'quad_injury_N', 'dental_exam_N', 'ingrown_nail_N', 'peanut_allergy_N']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, tok in tokens.items():\n",
    "    print(key)\n",
    "    print(tok)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 90%\n",
    "    * 2 adverse + 1 helper\n",
    "* 80%\n",
    "    * 1 adverse + 2 helper\n",
    "* 70%\n",
    "    * 1 adverse + 1 helper\n",
    "* 40%\n",
    "    * 1 helper + 1 unhelper\n",
    "* 30%\n",
    "    * 1 adverse + 2 unhelper\n",
    "* 20%\n",
    "    * 1 helper + 2 unhelper\n",
    "* 10%\n",
    "    * 2 unhelpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COUNTS = 3000\n",
    "VAL_COUNTS = 1000\n",
    "TEST_COUNTS = 1000\n",
    "\n",
    "TRAIN_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, TRAIN_COUNTS],\n",
    "    \"AHH\": [0.8, TRAIN_COUNTS],\n",
    "    \"AH\": [0.7, TRAIN_COUNTS],\n",
    "    \"HU\": [0.4, TRAIN_COUNTS],\n",
    "    \"AUU\": [0.3, TRAIN_COUNTS],\n",
    "    \"HUU\": [0.2, TRAIN_COUNTS],\n",
    "    \"UU\": [0.1, TRAIN_COUNTS],\n",
    "}\n",
    "\n",
    "VAL_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, VAL_COUNTS],\n",
    "    \"AHH\": [0.8, VAL_COUNTS],\n",
    "    \"AH\": [0.7, VAL_COUNTS],\n",
    "    \"HU\": [0.4, VAL_COUNTS],\n",
    "    \"AUU\": [0.3, VAL_COUNTS],\n",
    "    \"HUU\": [0.2, VAL_COUNTS],\n",
    "    \"UU\": [0.1, VAL_COUNTS],\n",
    "}\n",
    "\n",
    "TEST_COUNT_DICT = {\n",
    "    \"AAH\": [0.9, TEST_COUNTS],\n",
    "    \"AHH\": [0.8, TEST_COUNTS],\n",
    "    \"AH\": [0.7, TEST_COUNTS],\n",
    "    \"HU\": [0.4, TEST_COUNTS],\n",
    "    \"AUU\": [0.3, TEST_COUNTS],\n",
    "    \"HUU\": [0.2, TEST_COUNTS],\n",
    "    \"UU\": [0.1, TEST_COUNTS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings of the token groups with the abbreviation\n",
    "TOKEN_MAPPINGS = {\n",
    "    \"A\": \"adverse_tokens\",\n",
    "    \"H\": \"adverse_helper_tokens\",\n",
    "    \"U\": \"adverse_unhelper_tokens\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(df0, label, total):\n",
    "    \"\"\"Downsample the dataset to make it balanced class.\"\"\"\n",
    "    df = df0.copy()\n",
    "    df_c0 = df[df[label] == 0]\n",
    "    df_c1 = df[df[label] == 1]\n",
    "\n",
    "    df_c0 = df_c0.sample(int(total / 2))\n",
    "    df_c1 = df_c1.sample(int(total / 2))\n",
    "\n",
    "    df = pd.concat([df_c0, df_c1], axis=0)\n",
    "    df = df.sample(frac=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_proba(seq, base_seq_len=30):\n",
    "    \"\"\"Get probability of being positive label for a sequence.\"\"\"\n",
    "\n",
    "    def get_position(seq, substring, base_seq_len):\n",
    "        \"\"\"Get position of event with substring from end of sequence\"\"\"\n",
    "        pos = -1\n",
    "        for i, event in enumerate(seq):\n",
    "            if event.endswith(substring):\n",
    "                pos = i\n",
    "                break\n",
    "        if pos == -1:\n",
    "            raise ValueError(f\"Error! {substring} not found!\")\n",
    "\n",
    "        pos = len(seq) - pos - 1\n",
    "        return pos\n",
    "\n",
    "        a = 0.1  # Constant for Adverse\n",
    "        h = 0.5  # Constant for helper\n",
    "        u = 0.95  # Constant for unhelper\n",
    "\n",
    "    #     a = 0.03  # Constant for Adverse\n",
    "    #     h = 0.05  # Constant for helper\n",
    "    #     u = 0.09  # Constant for unhelper\n",
    "\n",
    "    seq_len = len(seq)\n",
    "    multiplier = float(base_seq_len) / seq_len\n",
    "    ta = get_position(seq, \"_A\", base_seq_len) * multiplier\n",
    "    th = get_position(seq, \"_H\", base_seq_len) * multiplier\n",
    "    tu = get_position(seq, \"_U\", base_seq_len) * multiplier\n",
    "\n",
    "    prob = min(1.0, math.exp(-(a * ta)) + math.exp(-(h * th)) - math.exp(-(u * tu)))\n",
    "    prob = round(prob, 4)\n",
    "    return prob\n",
    "\n",
    "\n",
    "def get_a_sequence_seq_v2(seq_len, label, tokens, token_mappings, seq_tokens, proba):\n",
    "    \"\"\"creates sequence + label (at the end of list) with specific orderings.\n",
    "    returns list of list\"\"\"\n",
    "    n_seq_tokens = len(seq_tokens)\n",
    "    n_noise = (\n",
    "        np.max(\n",
    "            (\n",
    "                10,\n",
    "                random.choices(range(n_seq_tokens, seq_len), k=1)[0],\n",
    "            )\n",
    "        )\n",
    "        - (n_seq_tokens)\n",
    "    )\n",
    "    sel_positions = sorted(random.sample(range(n_noise), k=n_seq_tokens))\n",
    "    sel_tokens = []\n",
    "    for key in seq_tokens:\n",
    "        key_mapping = token_mappings[key]\n",
    "        sel_tokens.append(random.choices(tokens[key_mapping])[0])\n",
    "\n",
    "    # Randomize sequence\n",
    "    random.shuffle(sel_tokens)\n",
    "\n",
    "    sel_tokens = list(zip(sel_positions, sel_tokens))\n",
    "    sel_noise = get_tokens(seq_len, tokens, \"noise_tokens\", n_noise)\n",
    "\n",
    "    for idx, event in sel_tokens:\n",
    "        sel_noise.insert(idx, event)\n",
    "\n",
    "    sel_noise = [\"<pad>\"] * (seq_len - len(sel_noise)) + sel_noise\n",
    "\n",
    "    # Get probability of being positive label\n",
    "    # sel_noise.reverse()\n",
    "    sim_lab = get_label(proba, target=label)\n",
    "\n",
    "    sequence = sel_noise + [proba] + [sim_lab]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def get_sequences_v2(\n",
    "    seq_len,\n",
    "    label,\n",
    "    uid_len,\n",
    "    uid_colname,\n",
    "    tokens,\n",
    "    token_mappings,\n",
    "    seq_tokens,\n",
    "    n_seq,\n",
    "    proba,\n",
    "):\n",
    "    \"\"\"Get multiple sequences.\"\"\"\n",
    "\n",
    "    sequences = [\n",
    "        get_a_sequence_seq_v2(seq_len, label, tokens, token_mappings, seq_tokens, proba)\n",
    "        + [get_uid(uid_len)]\n",
    "        for _ in range(n_seq)\n",
    "    ]\n",
    "    # print(f\"seq based events generated\")\n",
    "\n",
    "    seq_df = pd.DataFrame(sequences)\n",
    "    seq_df.columns = [str(x) for x in range(seq_len - 1, -1, -1)] + [\n",
    "        \"proba\",\n",
    "        \"label\",\n",
    "        uid_colname,\n",
    "    ]\n",
    "\n",
    "    return seq_df\n",
    "\n",
    "\n",
    "def get_sequence_dataset(\n",
    "    seq_len, uid_len, uid_colname, count_dict, tokens, token_mappings, total_rows\n",
    "):\n",
    "    \"\"\"Generate a simple toy dataset.\n",
    "\n",
    "    Arg:\n",
    "    -----\n",
    "        seq_len (int) : length of the generated sequence\n",
    "        uid_len (int) : length of uid token\n",
    "        uid_colname (str) : name of uid column, usually patient_id\n",
    "        count_dict (dict) : dictionary of various sequence types.\n",
    "            6 different types are allowed:\n",
    "                n_ppp_adverse, n_pp_adverse, n_p_adverse\n",
    "                n_nnn_adverse, n_nn_adverse, n_n_adverse\n",
    "        tokens (dict) : dictionary of the various token types\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        dataset (dataframe) : dataframe containing all the\n",
    "                              generated dataset, randomly mixed\n",
    "\n",
    "    \"\"\"\n",
    "    label = 1\n",
    "    cat_lst = []\n",
    "    for seq_tokens, (proba, n_seq) in count_dict.items():\n",
    "        df = get_sequences_v2(\n",
    "            seq_len,\n",
    "            label,\n",
    "            uid_len,\n",
    "            uid_colname,\n",
    "            tokens,\n",
    "            token_mappings,\n",
    "            seq_tokens,\n",
    "            n_seq,\n",
    "            proba,\n",
    "        )\n",
    "\n",
    "        df[\"seq_event\"] = seq_tokens\n",
    "        cat_lst.append(df.copy())\n",
    "    dataset = pd.concat(cat_lst, axis=0)\n",
    "    dataset.reset_index(inplace=True)\n",
    "    indexes = [idx for idx in range(dataset.shape[0])]\n",
    "    random.shuffle(indexes)\n",
    "    dataset = dataset.iloc[indexes, :]\n",
    "    # dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # dataset = downsample(dataset, \"label\", total_rows)\n",
    "    print(f\"dataset: {dataset.shape}\")\n",
    "    print(f\"ratio:\\n{dataset.label.value_counts(normalize=True)}\\n\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Imbalance for seq_len=300...\n",
      "dataset: (21000, 305)\n",
      "ratio:\n",
      "0    0.516381\n",
      "1    0.483619\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Val Data Imbalance for seq_len=300...\n",
      "dataset: (7000, 305)\n",
      "ratio:\n",
      "0    0.508429\n",
      "1    0.491571\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Test Data Imbalance for seq_len=300...\n",
      "dataset: (7000, 305)\n",
      "ratio:\n",
      "0    0.521143\n",
      "1    0.478857\n",
      "Name: label, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_train = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=TRAIN_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=TRAIN_COUNTS,\n",
    ")\n",
    "\n",
    "print(f\"Val Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_val = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=VAL_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=VAL_COUNTS,\n",
    ")\n",
    "\n",
    "print(f\"Test Data Imbalance for seq_len={SEQ_LEN}...\")\n",
    "df_test = get_sequence_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=TEST_COUNT_DICT,\n",
    "    tokens=tokens,\n",
    "    token_mappings=TOKEN_MAPPINGS,\n",
    "    total_rows=TEST_COUNTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 305)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>proba</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seq_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>1256</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>ZTKBJ30YFM</td>\n",
       "      <td>AHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>2612</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>HIUSSBYZAB</td>\n",
       "      <td>AUU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13098</th>\n",
       "      <td>1098</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>pacemaker_U</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>348J3FNVNH</td>\n",
       "      <td>AUU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8562</th>\n",
       "      <td>2562</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>...</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>PJ342UBGHK</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9745</th>\n",
       "      <td>745</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>E4AGUBC3FE</td>\n",
       "      <td>HU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index    299    298    297    296    295    294    293    292  \\\n",
       "4256    1256  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
       "14612   2612  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
       "13098   1098  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
       "8562    2562  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
       "9745     745  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
       "\n",
       "              291  ...               5                  4               3  \\\n",
       "4256        <pad>  ...   quad_injury_N         eye_exam_N    cut_finger_N   \n",
       "14612       <pad>  ...  ankle_sprain_N         backache_N     foot_pain_N   \n",
       "13098       <pad>  ...   dental_exam_N        pacemaker_U     hay_fever_N   \n",
       "8562   eye_exam_N  ...  ingrown_nail_N        hay_fever_N  ankle_sprain_N   \n",
       "9745        <pad>  ...      backache_N  annual_physical_N     foot_pain_N   \n",
       "\n",
       "                    2              1                 0 proba label  \\\n",
       "4256   ingrown_nail_N  dental_exam_N    ingrown_nail_N   0.8     1   \n",
       "14612      headache_N    foot_pain_N  peanut_allergy_N   0.3     0   \n",
       "13098      headache_N    cold_sore_N     dental_exam_N   0.3     0   \n",
       "8562       ACL_tear_N     ACL_tear_N    ingrown_nail_N   0.7     1   \n",
       "9745       ACL_tear_N    cold_sore_N        eye_exam_N   0.4     0   \n",
       "\n",
       "       patient_id seq_event  \n",
       "4256   ZTKBJ30YFM       AHH  \n",
       "14612  HIUSSBYZAB       AUU  \n",
       "13098  348J3FNVNH       AUU  \n",
       "8562   PJ342UBGHK        AH  \n",
       "9745   E4AGUBC3FE        HU  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# df_train.sort_values(\"proba\")[::-1]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 305)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 305)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df0, count_dict, seq_len, output_path):\n",
    "    \"\"\"Process data and converting to list of dicts.\"\"\"\n",
    "    print(\"Processing data...\")\n",
    "    feature_names = [str(i) for i in range(seq_len - 1, -1, -1)]\n",
    "\n",
    "    data = {}\n",
    "    for category, values in count_dict.items():\n",
    "        data[category] = []\n",
    "        df = df0[df0[\"seq_event\"] == category]\n",
    "        df = df[feature_names]\n",
    "        n_rows = df.shape[0]\n",
    "        for idx in range(n_rows):\n",
    "            row = df.iloc[idx].tolist()\n",
    "            row.reverse()\n",
    "            row = dict(zip(range(SEQ_LEN), row))\n",
    "            row2 = row.copy()\n",
    "            for key, value in row.items():\n",
    "                if value.endswith(\"_N\") or value == \"<pad>\":\n",
    "                    del row2[key]\n",
    "            data[category].append(row2.copy())\n",
    "\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "SUCCESS!\n",
      "Processing data...\n",
      "SUCCESS!\n",
      "Processing data...\n",
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "process(df_train, TRAIN_COUNT_DICT, SEQ_LEN, TRAIN_FP)\n",
    "process(df_val, VAL_COUNT_DICT, SEQ_LEN, VAL_FP)\n",
    "process(df_test, TEST_COUNT_DICT, SEQ_LEN, TEST_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_csv(df_train, TRAIN_FP)\n",
    "# save_csv(df_val, VAL_FP)\n",
    "# save_csv(df_test, TEST_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(TRAIN_FP)\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
