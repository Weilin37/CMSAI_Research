{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset generation\n",
    "Author: Lin Lee Cheong <br>\n",
    "Date: 12/12/ 2020 <br>\n",
    "\n",
    "Goal of this synthetic dataset is to create datasets to help understand how different relationships between tokens affect attention, SHAP and other interpretability factors.\n",
    "- length of events (30, 300, 900)\n",
    "- spacing between 2+ coupled events, i.e. order of sequence matters\n",
    "- amount of noise, i.e. performance vs interpretability\n",
    "- vocabulary space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nb-black\n",
    "\n",
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_NAMES_FP = \"./tokens_v2.yaml\"\n",
    "\n",
    "SEQ_LEN = 300\n",
    "\n",
    "TRAIN_FP = \"data/event_final_v2/{}/train.csv\".format(SEQ_LEN)\n",
    "VAL_FP = \"data/event_final_v2/{}/val.csv\".format(SEQ_LEN)\n",
    "TEST_FP = \"data/event_final_v2/{}/test.csv\".format(SEQ_LEN)\n",
    "\n",
    "UID_COLNAME = \"patient_id\"\n",
    "\n",
    "TRAIN_NROWS = 3000\n",
    "VAL_NROWS = 1000\n",
    "TEST_NROWS = 1000\n",
    "\n",
    "UID_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse_tokens: 10 tokens\n",
      "adverse_helper_tokens: 10 tokens\n",
      "adverse_unhelper_tokens: 10 tokens\n",
      "noise_tokens: 15 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load tokens from yaml file path\n",
    "tokens = load_tokens(TOKEN_NAMES_FP)\n",
    "for key in tokens.keys():\n",
    "    print(f\"{key}: {len(tokens[key])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get simple dataset:\n",
    "- positive set: (+++, 1 major + a helper), (++, 1 major), (+, 3 helper)\n",
    "- negative set: (---, 3 unhelper), (--, 1 helper + 2 unhelper), (-, 2 helper + 1 unhelper)\n",
    "\n",
    "\n",
    "**NOTES**<br>\n",
    "n_ppp_adverse = 2000 # 1 adverse event + 1 helper event <br>\n",
    "n_pp_adverse = 2000 # 1 adverse event <br>\n",
    "n_p_adverse = 2000 # 3 helper events <br><br>\n",
    "n_nnn_adverse = 2000 # 3 unhelper events <br>\n",
    "n_nn_adverse = 2000 # 1 helper + 2 unhelper <br>\n",
    "n_n_adverse = 2000 # 2 helper + 1 unhelper <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_dict = {\n",
    "    \"n_ppp_adverse\": TRAIN_NROWS,\n",
    "    \"n_pp_adverse\": TRAIN_NROWS,\n",
    "    \"n_p_adverse\": TRAIN_NROWS,\n",
    "    \"n_nnn_adverse\": TRAIN_NROWS,\n",
    "    \"n_nn_adverse\": TRAIN_NROWS,\n",
    "    \"n_n_adverse\": TRAIN_NROWS,\n",
    "}\n",
    "\n",
    "val_count_dict = {\n",
    "    \"n_ppp_adverse\": VAL_NROWS,\n",
    "    \"n_pp_adverse\": VAL_NROWS,\n",
    "    \"n_p_adverse\": VAL_NROWS,\n",
    "    \"n_nnn_adverse\": VAL_NROWS,\n",
    "    \"n_nn_adverse\": VAL_NROWS,\n",
    "    \"n_n_adverse\": VAL_NROWS,\n",
    "}\n",
    "\n",
    "test_count_dict = {\n",
    "    \"n_ppp_adverse\": TEST_NROWS,\n",
    "    \"n_pp_adverse\": TEST_NROWS,\n",
    "    \"n_p_adverse\": TEST_NROWS,\n",
    "    \"n_nnn_adverse\": TEST_NROWS,\n",
    "    \"n_nn_adverse\": TEST_NROWS,\n",
    "    \"n_n_adverse\": TEST_NROWS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: (18000, 303)\n",
      "ratio:\n",
      "1    0.502667\n",
      "0    0.497333\n",
      "Name: label, dtype: float64\n",
      "\n",
      "dataset: (6000, 303)\n",
      "ratio:\n",
      "1    0.507167\n",
      "0    0.492833\n",
      "Name: label, dtype: float64\n",
      "\n",
      "dataset: (6000, 303)\n",
      "ratio:\n",
      "1    0.503667\n",
      "0    0.496333\n",
      "Name: label, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_simple_data = get_simple_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=train_count_dict,\n",
    "    tokens=tokens,\n",
    ")\n",
    "\n",
    "val_simple_data = get_simple_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=val_count_dict,\n",
    "    tokens=tokens,\n",
    ")\n",
    "\n",
    "test_simple_data = get_simple_dataset(\n",
    "    seq_len=SEQ_LEN,\n",
    "    uid_len=UID_LEN,\n",
    "    uid_colname=UID_COLNAME,\n",
    "    count_dict=test_count_dict,\n",
    "    tokens=tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(train_simple_data, TRAIN_FP)\n",
    "save_csv(val_simple_data, VAL_FP)\n",
    "save_csv(test_simple_data, TEST_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2808</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>1</td>\n",
       "      <td>ERVCYRZXH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2065</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>0</td>\n",
       "      <td>BT2Z13OBJF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>0</td>\n",
       "      <td>2ATVRLS02I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2202</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>0</td>\n",
       "      <td>0WF3PEIBZS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1800</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>0</td>\n",
       "      <td>8MBXJX6H1M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    299    298    297    296    295    294    293    292    291  ...  \\\n",
       "0   2808  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1   2065  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2     25  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3   2202  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4   1800  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "               7               6               5                  4  \\\n",
       "0  dental_exam_N  ankle_sprain_N     cold_sore_N         backache_N   \n",
       "1  quad_injury_N    cut_finger_N   dental_exam_N         ACL_tear_N   \n",
       "2     ACL_tear_N      headache_N   dental_exam_N        cold_sore_N   \n",
       "3   cut_finger_N    cut_finger_N        myopia_N  annual_physical_N   \n",
       "4  quad_injury_N     foot_pain_N  ingrown_nail_N      dental_exam_N   \n",
       "\n",
       "                  3               2                  1                 0  \\\n",
       "0          myopia_N  ankle_sprain_N         headache_N       foot_pain_N   \n",
       "1  peanut_allergy_N    cut_finger_N       cut_finger_N    ingrown_nail_N   \n",
       "2      cut_finger_N     foot_pain_N       cut_finger_N     quad_injury_N   \n",
       "3    ingrown_nail_N   dental_exam_N  annual_physical_N      cut_finger_N   \n",
       "4    ankle_sprain_N   quad_injury_N        cold_sore_N  peanut_allergy_N   \n",
       "\n",
       "  label  patient_id  \n",
       "0     1  ERVCYRZXH1  \n",
       "1     0  BT2Z13OBJF  \n",
       "2     0  2ATVRLS02I  \n",
       "3     0  0WF3PEIBZS  \n",
       "4     0  8MBXJX6H1M  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN_FP)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
