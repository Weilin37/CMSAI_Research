{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output dataset and create vocabulary for C.Diff Adverse Event\n",
    "\n",
    "**Author: Tesfagabir Meharizghi *(Adopted from Lin Lee Notebook)* <br> Last updated: 01/06/21**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook to convert 365 version to 1000 version, and save CSV and vocabulary for  CDiff (event_id=d_00845) Adverse Events**\n",
    "- training data & vocabulary\n",
    "- valid data\n",
    "- test data\n",
    "- up to 1000 events, from full 365 day dataset\n",
    "\n",
    "**Required:**\n",
    "- input files: train.csv, val.csv, test.csv (unflattened)\n",
    "    - Run [this ipynb](01_data_prepare_CDiff_d00845.ipynb) to generate these data splits\n",
    "- outputs: csv files in flattened 1000 format (could be changed), and vocabulary\n",
    "\n",
    "**Nomenclature:**\n",
    "- d30: **30** days\n",
    "- vpos: vocabulary positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torchtext\n",
    "\n",
    "#!pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from more_itertools import unique_everseen\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input filepaths for training, test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "n_events = 1000\n",
    "n_rows = 1e9\n",
    "ndays = 365  # number of days to keep\n",
    "\n",
    "main_dir = \"/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/\"\n",
    "input_dir = os.path.join(main_dir, \"split\")\n",
    "output_dir = os.path.join(main_dir, f\"{n_events}_{ndays}days\")\n",
    "in_fnames = [\"train.csv\", \"val.csv\", \"test.csv\"]\n",
    "out_fnames = [\"train.csv\", \"val.csv\", \"test.csv\"]\n",
    "input_fps = [os.path.join(input_dir, fname) for fname in in_fnames]\n",
    "output_fps = [os.path.join(output_dir, fname) for fname in out_fnames]\n",
    "vocab_fp = os.path.join(output_dir, \"vocab\".format(n_events, ndays))\n",
    "\n",
    "if not os.path.join(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Options\n",
    "x_lst = [\n",
    "    str(x) for x in range(ndays, -1, -1)\n",
    "]  # total days in datasets, usually 365. Check all for death events\n",
    "x_flat_lst = [\n",
    "    str(x) for x in range(n_events - 1, -1, -1)\n",
    "]  # up to 1000 events in flattened list\n",
    "y_target = \"d_00845\"\n",
    "uid = \"patient_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x, n_events=1000):\n",
    "    \"\"\"Flatten the 365 dataset into N long events\"\"\"\n",
    "\n",
    "    def get_days(x):\n",
    "        \"\"\"Calculate number of days between events\"\"\"\n",
    "        new_lst = []\n",
    "        counter = 1\n",
    "        counting = False\n",
    "        for event in x:\n",
    "            if event is np.nan or (type(event) == float and math.isnan(event)):\n",
    "                if not counting:\n",
    "                    counting = True\n",
    "                counter += 1\n",
    "            else:\n",
    "\n",
    "                if counting:\n",
    "                    counting = False\n",
    "                    try:\n",
    "                        event = f\"{counter + 1}_days,\" + event\n",
    "                    except:\n",
    "                        print(type(counter), counter)\n",
    "                        print(event, type(event))\n",
    "                    new_lst.append(event)\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    event = \"1_days,\" + event\n",
    "                    new_lst.append(event)\n",
    "\n",
    "        return new_lst\n",
    "\n",
    "    # count days with no events, move admission/discharge to the end of the day, dedupe events per day\n",
    "    x = np.array(get_days(x))\n",
    "    lst = [move_ad_dis(str(day).replace(\" \", \"\").split(\",\")) for day in x.ravel(\"K\")]\n",
    "\n",
    "    # flatten, clean up corner cases\n",
    "    lst = [event for day in lst for event in day]\n",
    "    if not lst:\n",
    "        return [\"<pad>\"] * (n_events - len(lst)) + lst\n",
    "\n",
    "    if \"_days\" in lst[0]:\n",
    "        lst = lst[1:]\n",
    "\n",
    "    if len(lst) >= n_events:\n",
    "        return lst[-n_events:]\n",
    "\n",
    "    return [\"<pad>\"] * (n_events - len(lst)) + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_ad_dis(events_in_day):\n",
    "    \"\"\"Move target_event and patient_id to the end of the list, dedupe events\"\"\"\n",
    "    if not isinstance(events_in_day, list):\n",
    "        return events_in_day\n",
    "\n",
    "    events_in_day = list(unique_everseen(events_in_day))\n",
    "    has_admission = False\n",
    "    has_discharge = False\n",
    "\n",
    "    if \"admission\" in events_in_day:\n",
    "        has_admission = True\n",
    "        events_in_day.remove(\"admission\")\n",
    "\n",
    "    if \"discharge\" in events_in_day:\n",
    "        has_discharge = True\n",
    "        events_in_day.remove(\"discharge\")\n",
    "\n",
    "    if has_admission:\n",
    "        events_in_day.append(\"admission\")\n",
    "\n",
    "    if has_discharge:\n",
    "        events_in_day.append(\"discharge\")\n",
    "\n",
    "    return events_in_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_df(raw_df, x_lst, copy_lst, n_events):\n",
    "    \"\"\"\n",
    "    Function to flatten dataframe into 1000 long sequence.\n",
    "\n",
    "    Calls function flatten, which in turn calls move_ad_dis\n",
    "    \"\"\"\n",
    "    columns = [str(x) for x in range(n_events - 1, -1, -1)]\n",
    "    flat_df = pd.DataFrame(\n",
    "        raw_df[x_lst].apply(flatten, args=(n_events,), axis=1).tolist(),\n",
    "        columns=columns,\n",
    "    )\n",
    "\n",
    "    for colname in copy_lst:\n",
    "        flat_df[colname] = raw_df[colname].tolist()\n",
    "\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_dataset(\n",
    "    data_fp,\n",
    "    x_lst,\n",
    "    x_flat_lst,\n",
    "    y_target,\n",
    "    uid,\n",
    "    vocab_fp=None,\n",
    "    output_fp=None,\n",
    "    min_freq=500,\n",
    "    n_events=1000,\n",
    "    return_csv=False,\n",
    "    nrows=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to create flattened dataset: Reads in raw data,\n",
    "    removes death events, and flattens and saves to output CSV.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    data_fp (str) : input filepath, csv\n",
    "    x_lst (list) : list of column names (days) to use for flattening\n",
    "    x_flat_lst (list) : list of column names to use for writing out the\n",
    "                        flattened file (1000 events)\n",
    "    y_target (str) : column name of target\n",
    "    uid (str) : column name of unique identifier\n",
    "    vocab_fp (str) : path to write out vocab\n",
    "        default None (will not generate vocabulary)\n",
    "    output_fp (str) : path to write out flattened CSV file\n",
    "        default None (will not save)\n",
    "    min_freq (int) : minimum frequency associated with vocabulary generation\n",
    "    return_csv (bool) : default False, returns flattened dataframe if True\n",
    "    nrows (int) : default 0 to read and process all, otherwise\n",
    "                  will read in nrows in CSV only (for testing)\n",
    "\n",
    "    Returns:\n",
    "    ---------\n",
    "    default None unless return_csv is True, then return\n",
    "        dataframe containing flattened data\n",
    "    \"\"\"\n",
    "\n",
    "    # read in raw dataset, remove deaths\n",
    "    raw_df = read_data(\n",
    "        data_fp=data_fp, check=True, y_target=y_target, uid=uid, test=nrows\n",
    "    )\n",
    "    raw_df = remove_death(raw_df, y_target, x_lst)\n",
    "\n",
    "    raw_df = get_flat_df(\n",
    "        raw_df,\n",
    "        x_lst=x_lst,\n",
    "        copy_lst=[y_target, \"patient_id\"],\n",
    "        n_events=n_events,\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.dirname(output_fp)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if output_fp is not None and isinstance(output_fp, str):\n",
    "        raw_df.to_csv(output_fp, index=False)\n",
    "\n",
    "    # build vocabulary\n",
    "    if vocab_fp is not None:\n",
    "        print(\"Vocab generation required\")\n",
    "        vocab = build_vocab(raw_df, x_flat_lst, min_freq=min_freq, pos_labs_vocab=False)\n",
    "\n",
    "        print(f\"Nb of tokens: {len(vocab.stoi)}\")\n",
    "        torch.save(vocab, vocab_fp)\n",
    "\n",
    "    if return_csv:\n",
    "        return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from /home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/split/train.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1522738, 368)\n",
      "\n",
      "Label ratio for d_00845\n",
      "0    0.999433\n",
      "1    0.000567\n",
      "Name: d_00845, dtype: float64\n",
      "\n",
      "patient_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 0 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1522738000,)\n",
      "exact word number:  1522738000\n",
      "Completed vocabulary: 6290 vocabs\n",
      "Nb of tokens: 6290\n"
     ]
    }
   ],
   "source": [
    "train_fp = input_fps[0]\n",
    "vocab_fp = vocab_fp\n",
    "output_fp = output_fps[0]\n",
    "return_csv = True\n",
    "\n",
    "df = create_flat_dataset(\n",
    "    data_fp=train_fp,\n",
    "    x_lst=x_lst,\n",
    "    x_flat_lst=x_flat_lst,\n",
    "    y_target=y_target,\n",
    "    uid=uid,\n",
    "    vocab_fp=vocab_fp,\n",
    "    output_fp=output_fp,\n",
    "    min_freq=500,\n",
    "    n_events=n_events,\n",
    "    return_csv=return_csv,\n",
    "    nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>999</th>\n",
       "      <th>998</th>\n",
       "      <th>997</th>\n",
       "      <th>996</th>\n",
       "      <th>995</th>\n",
       "      <th>994</th>\n",
       "      <th>993</th>\n",
       "      <th>992</th>\n",
       "      <th>991</th>\n",
       "      <th>990</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>d_00845</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>8_days</td>\n",
       "      <td>d_s3320</td>\n",
       "      <td>discharge</td>\n",
       "      <td>1_days</td>\n",
       "      <td>h_1CHK1</td>\n",
       "      <td>h_G0154</td>\n",
       "      <td>2_days</td>\n",
       "      <td>h_G0154</td>\n",
       "      <td>0</td>\n",
       "      <td>IXD7U0Z74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_92014</td>\n",
       "      <td>h_92015</td>\n",
       "      <td>172_days</td>\n",
       "      <td>d_sV066</td>\n",
       "      <td>h_90658</td>\n",
       "      <td>h_90732</td>\n",
       "      <td>h_G0008</td>\n",
       "      <td>h_G0009</td>\n",
       "      <td>0</td>\n",
       "      <td>4TJJ3BGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_85027</td>\n",
       "      <td>h_99308</td>\n",
       "      <td>h_P9604</td>\n",
       "      <td>3_days</td>\n",
       "      <td>d_s72981</td>\n",
       "      <td>h_73130</td>\n",
       "      <td>h_Q0092</td>\n",
       "      <td>h_R0070</td>\n",
       "      <td>0</td>\n",
       "      <td>QIW5R08DN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_s2724</td>\n",
       "      <td>d_sV0481</td>\n",
       "      <td>h_90658</td>\n",
       "      <td>h_99213</td>\n",
       "      <td>h_G0008</td>\n",
       "      <td>9_days</td>\n",
       "      <td>d_s37033</td>\n",
       "      <td>h_92014</td>\n",
       "      <td>0</td>\n",
       "      <td>WDPIV5G4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>36_days</td>\n",
       "      <td>d_sV762</td>\n",
       "      <td>h_81000</td>\n",
       "      <td>h_G0101</td>\n",
       "      <td>h_Q0091</td>\n",
       "      <td>19_days</td>\n",
       "      <td>d_s4660</td>\n",
       "      <td>h_99213</td>\n",
       "      <td>0</td>\n",
       "      <td>JI24AUR24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     999    998    997    996    995    994    993    992    991    990  ...  \\\n",
       "0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "         7         6          5        4         3        2         1  \\\n",
       "0   8_days   d_s3320  discharge   1_days   h_1CHK1  h_G0154    2_days   \n",
       "1  h_92014   h_92015   172_days  d_sV066   h_90658  h_90732   h_G0008   \n",
       "2  h_85027   h_99308    h_P9604   3_days  d_s72981  h_73130   h_Q0092   \n",
       "3  d_s2724  d_sV0481    h_90658  h_99213   h_G0008   9_days  d_s37033   \n",
       "4  36_days   d_sV762    h_81000  h_G0101   h_Q0091  19_days   d_s4660   \n",
       "\n",
       "         0 d_00845 patient_id  \n",
       "0  h_G0154       0  IXD7U0Z74  \n",
       "1  h_G0009       0  4TJJ3BGPT  \n",
       "2  h_R0070       0  QIW5R08DN  \n",
       "3  h_92014       0  WDPIV5G4M  \n",
       "4  h_99213       0  JI24AUR24  \n",
       "\n",
       "[5 rows x 1002 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from /home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/split/val.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (190342, 368)\n",
      "\n",
      "Label ratio for d_00845\n",
      "0    0.999475\n",
      "1    0.000525\n",
      "Name: d_00845, dtype: float64\n",
      "\n",
      "patient_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 0 rows contain the word death\n"
     ]
    }
   ],
   "source": [
    "val_fp = input_fps[1]\n",
    "output_fp = output_fps[1]\n",
    "\n",
    "create_flat_dataset(\n",
    "    data_fp=val_fp,\n",
    "    x_lst=x_lst,\n",
    "    x_flat_lst=x_flat_lst,\n",
    "    y_target=y_target,\n",
    "    uid=uid,\n",
    "    vocab_fp=None,\n",
    "    output_fp=output_fp,\n",
    "    min_freq=500,\n",
    "    n_events=n_events,\n",
    "    return_csv=False,\n",
    "    nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from /home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/split/test.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (190343, 368)\n",
      "\n",
      "Label ratio for d_00845\n",
      "0    0.999391\n",
      "1    0.000609\n",
      "Name: d_00845, dtype: float64\n",
      "\n",
      "patient_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 0 rows contain the word death\n"
     ]
    }
   ],
   "source": [
    "test_fp = input_fps[2]\n",
    "output_fp = output_fps[2]\n",
    "\n",
    "create_flat_dataset(\n",
    "    data_fp=test_fp,\n",
    "    x_lst=x_lst,\n",
    "    x_flat_lst=x_flat_lst,\n",
    "    y_target=y_target,\n",
    "    uid=uid,\n",
    "    vocab_fp=None,\n",
    "    output_fp=output_fp,\n",
    "    min_freq=500,\n",
    "    n_events=n_events,\n",
    "    return_csv=False,\n",
    "    nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
