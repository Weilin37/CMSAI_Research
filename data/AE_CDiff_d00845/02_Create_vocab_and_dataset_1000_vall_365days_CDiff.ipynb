{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output dataset and create vocabulary for C.Diff Adverse Event\n",
    "\n",
    "**Author: Tesfagabir Meharizghi *(Adopted from Lin Lee Notebook)* <br> Last updated: 01/06/21**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook to convert 365 version to 1000 version, and save CSV and vocabulary for  CDiff (event_id=d_00845) Adverse Events**\n",
    "- training data & vocabulary\n",
    "- valid data\n",
    "- test data\n",
    "- up to 1000 events, from full 365 day dataset\n",
    "\n",
    "**Required:**\n",
    "- input files: train.csv, val.csv, test.csv (unflattened)\n",
    "    - Run [this ipynb](01_data_prepare_CDiff_d00845.ipynb) to generate these data splits\n",
    "- outputs: csv files in flattened 1000 format (could be changed), and vocabulary\n",
    "\n",
    "**Nomenclature:**\n",
    "- d30: **30** days\n",
    "- vpos: vocabulary positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torchtext\n",
    "\n",
    "#!pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from more_itertools import unique_everseen\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input filepaths for training, test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "n_events = 1000\n",
    "n_rows = 1e9\n",
    "ndays = 365  # number of days to keep\n",
    "\n",
    "main_dir = \"/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/\"\n",
    "input_dir = os.path.join(main_dir, \"split\")\n",
    "output_dir = os.path.join(main_dir, f\"{n_events}_{ndays}days\")\n",
    "in_fnames = [\"train.csv\", \"valid.csv\", \"test.csv\"]\n",
    "out_fnames = [\"train.csv\", \"val.csv\", \"test.csv\"]\n",
    "input_fps = [os.path.join(input_dir, fname) for fname in in_fnames]\n",
    "output_fps = [os.path.join(output_dir, fname) for fname in out_fnames]\n",
    "vocab_fp = os.path.join(output_dir, \"vocab\".format(n_events, ndays))\n",
    "\n",
    "if not os.path.join(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Options\n",
    "x_lst = [\n",
    "    str(x) for x in range(ndays, -1, -1)\n",
    "]  # total days in datasets, usually 365. Check all for death events\n",
    "x_flat_lst = [\n",
    "    str(x) for x in range(n_events - 1, -1, -1)\n",
    "]  # up to 1000 events in flattened list\n",
    "y_target = \"d_00845\"\n",
    "uid = \"patient_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x, n_events=1000):\n",
    "    \"\"\"Flatten the 365 dataset into N long events\"\"\"\n",
    "\n",
    "    def get_days(x):\n",
    "        \"\"\"Calculate number of days between events\"\"\"\n",
    "        new_lst = []\n",
    "        counter = 1\n",
    "        counting = False\n",
    "        for event in x:\n",
    "            if event is np.nan or (type(event) == float and math.isnan(event)):\n",
    "                if not counting:\n",
    "                    counting = True\n",
    "                counter += 1\n",
    "            else:\n",
    "\n",
    "                if counting:\n",
    "                    counting = False\n",
    "                    try:\n",
    "                        event = f\"{counter + 1}_days,\" + event\n",
    "                    except:\n",
    "                        print(type(counter), counter)\n",
    "                        print(event, type(event))\n",
    "                    new_lst.append(event)\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    event = \"1_days,\" + event\n",
    "                    new_lst.append(event)\n",
    "\n",
    "        return new_lst\n",
    "\n",
    "    # count days with no events, move admission/discharge to the end of the day, dedupe events per day\n",
    "    x = np.array(get_days(x))\n",
    "    lst = [move_ad_dis(str(day).replace(\" \", \"\").split(\",\")) for day in x.ravel(\"K\")]\n",
    "\n",
    "    # flatten, clean up corner cases\n",
    "    lst = [event for day in lst for event in day]\n",
    "    if not lst:\n",
    "        return [\"<pad>\"] * (n_events - len(lst)) + lst\n",
    "\n",
    "    if \"_days\" in lst[0]:\n",
    "        lst = lst[1:]\n",
    "\n",
    "    if len(lst) >= n_events:\n",
    "        return lst[-n_events:]\n",
    "\n",
    "    return [\"<pad>\"] * (n_events - len(lst)) + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_ad_dis(events_in_day):\n",
    "    \"\"\"Move target_event and patient_id to the end of the list, dedupe events\"\"\"\n",
    "    if not isinstance(events_in_day, list):\n",
    "        return events_in_day\n",
    "\n",
    "    events_in_day = list(unique_everseen(events_in_day))\n",
    "    has_admission = False\n",
    "    has_discharge = False\n",
    "\n",
    "    if \"admission\" in events_in_day:\n",
    "        has_admission = True\n",
    "        events_in_day.remove(\"admission\")\n",
    "\n",
    "    if \"discharge\" in events_in_day:\n",
    "        has_discharge = True\n",
    "        events_in_day.remove(\"discharge\")\n",
    "\n",
    "    if has_admission:\n",
    "        events_in_day.append(\"admission\")\n",
    "\n",
    "    if has_discharge:\n",
    "        events_in_day.append(\"discharge\")\n",
    "\n",
    "    return events_in_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_df(raw_df, x_lst, copy_lst, n_events):\n",
    "    \"\"\"\n",
    "    Function to flatten dataframe into 1000 long sequence.\n",
    "\n",
    "    Calls function flatten, which in turn calls move_ad_dis\n",
    "    \"\"\"\n",
    "    columns = [str(x) for x in range(n_events - 1, -1, -1)]\n",
    "    flat_df = pd.DataFrame(\n",
    "        raw_df[x_lst].apply(flatten, args=(n_events,), axis=1).tolist(),\n",
    "        columns=columns,\n",
    "    )\n",
    "\n",
    "    for colname in copy_lst:\n",
    "        flat_df[colname] = raw_df[colname].tolist()\n",
    "\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_dataset(\n",
    "    data_fp,\n",
    "    x_lst,\n",
    "    x_flat_lst,\n",
    "    y_target,\n",
    "    uid,\n",
    "    vocab_fp=None,\n",
    "    output_fp=None,\n",
    "    min_freq=500,\n",
    "    n_events=1000,\n",
    "    return_csv=False,\n",
    "    nrows=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to create flattened dataset: Reads in raw data,\n",
    "    removes death events, and flattens and saves to output CSV.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    data_fp (str) : input filepath, csv\n",
    "    x_lst (list) : list of column names (days) to use for flattening\n",
    "    x_flat_lst (list) : list of column names to use for writing out the\n",
    "                        flattened file (1000 events)\n",
    "    y_target (str) : column name of target\n",
    "    uid (str) : column name of unique identifier\n",
    "    vocab_fp (str) : path to write out vocab\n",
    "        default None (will not generate vocabulary)\n",
    "    output_fp (str) : path to write out flattened CSV file\n",
    "        default None (will not save)\n",
    "    min_freq (int) : minimum frequency associated with vocabulary generation\n",
    "    return_csv (bool) : default False, returns flattened dataframe if True\n",
    "    nrows (int) : default 0 to read and process all, otherwise\n",
    "                  will read in nrows in CSV only (for testing)\n",
    "\n",
    "    Returns:\n",
    "    ---------\n",
    "    default None unless return_csv is True, then return\n",
    "        dataframe containing flattened data\n",
    "    \"\"\"\n",
    "\n",
    "    # read in raw dataset, remove deaths\n",
    "    raw_df = read_data(\n",
    "        data_fp=data_fp, check=True, y_target=y_target, uid=uid, test=nrows\n",
    "    )\n",
    "    raw_df = remove_death(raw_df, y_target, x_lst)\n",
    "\n",
    "    raw_df = get_flat_df(\n",
    "        raw_df,\n",
    "        x_lst=x_lst,\n",
    "        copy_lst=[y_target, \"patient_id\"],\n",
    "        n_events=n_events,\n",
    "    )\n",
    "\n",
    "    output_dir = os.path.dirname(output_fp)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if output_fp is not None and isinstance(output_fp, str):\n",
    "        raw_df.to_csv(output_fp, index=False)\n",
    "\n",
    "    # build vocabulary\n",
    "    if vocab_fp is not None:\n",
    "        print(\"Vocab generation required\")\n",
    "        vocab = build_vocab(raw_df, x_flat_lst, min_freq=min_freq, pos_labs_vocab=False)\n",
    "\n",
    "        print(f\"Nb of tokens: {len(vocab.stoi)}\")\n",
    "        torch.save(vocab, vocab_fp)\n",
    "\n",
    "    if return_csv:\n",
    "        return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from /home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE_CDiff_d00845/split/train.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1522738, 368)\n",
      "\n",
      "Label ratio for d_00845\n",
      "0    0.999433\n",
      "1    0.000567\n",
      "Name: d_00845, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 0 rows contain the word death\n"
     ]
    }
   ],
   "source": [
    "train_fp = input_fps[0]\n",
    "vocab_fp = vocab_fp\n",
    "output_fp = output_fps[0]\n",
    "return_csv = True\n",
    "\n",
    "df = create_flat_dataset(\n",
    "        data_fp=train_fp,\n",
    "        x_lst=x_lst,\n",
    "        x_flat_lst=x_flat_lst,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        vocab_fp=vocab_fp,\n",
    "        output_fp=output_fp,\n",
    "        min_freq=500,\n",
    "        n_events=n_events,\n",
    "        return_csv=return_csv,\n",
    "        nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Val and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fp = input_fps[1]\n",
    "output_fp = output_fps[1]\n",
    "\n",
    "create_flat_dataset(\n",
    "            data_fp=val_fp,\n",
    "            x_lst=x_lst,\n",
    "            x_flat_lst=x_flat_lst,\n",
    "            y_target=y_target,\n",
    "            uid=uid,\n",
    "            vocab_fp=None,\n",
    "            output_fp=output_fp,\n",
    "            min_freq=500,\n",
    "            n_events=n_events,\n",
    "            return_csv=False,\n",
    "            nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = input_fps[2]\n",
    "output_fp = output_fps[2]\n",
    "\n",
    "create_flat_dataset(\n",
    "            data_fp=test_fp,\n",
    "            x_lst=x_lst,\n",
    "            x_flat_lst=x_flat_lst,\n",
    "            y_target=y_target,\n",
    "            uid=uid,\n",
    "            vocab_fp=None,\n",
    "            output_fp=output_fp,\n",
    "            min_freq=500,\n",
    "            n_events=n_events,\n",
    "            return_csv=False,\n",
    "            nrows=n_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
