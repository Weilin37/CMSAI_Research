{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Files to migrate and anonymize:\n",
    "\n",
    "Action Items:\n",
    "* Created Submitted folder (notify the team) (Tes)\n",
    "* Create global patients mapping and use for trainting and test phase data(Tes)\n",
    "* Team will move the model artificates(all)\n",
    "* Create a new folder REwithDate(Tes)\n",
    "* Syncup the common data preprocessing pipeline and generate the correct dataset(XYZ/LL)\n",
    "* Creating the anonymized data of mappings(Tes)\n",
    "* Cleanup local copies of raw data(all)\n",
    "* Replace all the patient ids with the anonymized ids for all data (all)\n",
    "\n",
    "\n",
    "* readmissions_2011\n",
    "* readmissions_2012\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/REWithDates/Data/Raw/365TestPhase/readmission_targets_with_date_testdata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1111571, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit_dt</th>\n",
       "      <th>disch_dt</th>\n",
       "      <th>events</th>\n",
       "      <th>unplanned</th>\n",
       "      <th>Days_since_last_discharge</th>\n",
       "      <th>first</th>\n",
       "      <th>unplanned_readmission</th>\n",
       "      <th>discharge_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>unplanned_readmission_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>2012-05-26</td>\n",
       "      <td>['d_04189', 'd_311', 'd_99859']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>499999951_20120526</td>\n",
       "      <td>499999951</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>2011-02-03</td>\n",
       "      <td>['d_V1251', 'd_71535', 'd_78900']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000005_20110203</td>\n",
       "      <td>100000005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-05-03</td>\n",
       "      <td>2011-05-05</td>\n",
       "      <td>['d_45341', 'd_72400', 'd_4019']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000203_20110505</td>\n",
       "      <td>100000203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>2012-01-14</td>\n",
       "      <td>['d_42789', 'd_41071', 'd_41401']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000221_20120114</td>\n",
       "      <td>100000221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>2011-01-25</td>\n",
       "      <td>['d_8052', 'd_5990', 'd_42831']</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>100000315_20110125</td>\n",
       "      <td>100000315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit_dt    disch_dt                             events  unplanned  \\\n",
       "0  2012-05-23  2012-05-26    ['d_04189', 'd_311', 'd_99859']       True   \n",
       "1  2011-01-31  2011-02-03  ['d_V1251', 'd_71535', 'd_78900']       True   \n",
       "2  2011-05-03  2011-05-05   ['d_45341', 'd_72400', 'd_4019']       True   \n",
       "3  2012-01-05  2012-01-14  ['d_42789', 'd_41071', 'd_41401']       True   \n",
       "4  2011-01-17  2011-01-25    ['d_8052', 'd_5990', 'd_42831']       True   \n",
       "\n",
       "   Days_since_last_discharge  first  unplanned_readmission  \\\n",
       "0                        NaN   True                  False   \n",
       "1                        NaN   True                  False   \n",
       "2                        NaN   True                  False   \n",
       "3                        NaN   True                  False   \n",
       "4                        NaN   True                  False   \n",
       "\n",
       "         discharge_id  patient_id unplanned_readmission_date  \n",
       "0  499999951_20120526   499999951                        NaN  \n",
       "1  100000005_20110203   100000005                        NaN  \n",
       "2  100000203_20110505   100000203                        NaN  \n",
       "3  100000221_20120114   100000221                        NaN  \n",
       "4  100000315_20110125   100000315                        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_fnames(data_dir):\n",
    "    \"\"\"Get all csv filenames from directory.\"\"\"\n",
    "    fnames = os.listdir(data_dir)\n",
    "    fnames = [fname for fname in fnames if fname.endswith('.csv')]\n",
    "    fnames = sorted(fnames)\n",
    "    return fnames\n",
    "\n",
    "\n",
    "def get_fpaths(data_dir, data_types):\n",
    "    \"\"\"Get all csv filenames from directory.\"\"\"\n",
    "    fpaths = []\n",
    "    for data_type in data_types:\n",
    "        my_data_dir = os.path.join(data_dir, data_type)\n",
    "        fnames = os.listdir(my_data_dir)\n",
    "        fnames = [fname for fname in fnames if fname.endswith('.csv')]\n",
    "        fnames = sorted(fnames)\n",
    "        fpaths += [os.path.join(my_data_dir, fname) for fname in fnames]\n",
    "    return fpaths\n",
    "\n",
    "\n",
    "def get_unique_values(fpaths, column):\n",
    "    \"\"\"Get all unique values from all files in a given column.\"\"\"\n",
    "    print('Getting unique values for the {} column...'.format(column))\n",
    "    values = set()\n",
    "    for fpath in fpaths:\n",
    "        print('Processing {}...'.format(fpath))\n",
    "        df = pd.read_csv(fpath, low_memory=False)\n",
    "        ids = set(df[column].tolist())\n",
    "        values.update(ids)\n",
    "    print('Success!')\n",
    "    return list(values)\n",
    "\n",
    "\n",
    "def _generate_strings_v0(num_values=1000, length=16):\n",
    "    \"\"\"Generate random strings for patient ids.\"\"\"\n",
    "    def _generate_unique_string(generated_list, length):\n",
    "        \"\"\"\n",
    "        Generate unique random string for patient id\n",
    "        Source: https://stackoverflow.com/questions/2511222/efficiently-generate-a-16-character-alphanumeric-string\n",
    "        \"\"\"\n",
    "        #x = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "        x = ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "        while x in generated_list:\n",
    "            x = ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "        generated_list.append(x)\n",
    "        return generated_list\n",
    "\n",
    "    gen_list = []\n",
    "    for i in range(num_values):\n",
    "        gen_list = _generate_unique_string(gen_list, length)\n",
    "    return gen_list\n",
    "\n",
    "def _generate_strings(num_values=1000, length=16):\n",
    "    \"\"\"Generate random strings for patient ids.\"\"\"\n",
    "    def _generate_unique_string(generated_set, length):\n",
    "        \"\"\"\n",
    "        Generate unique random string for patient id\n",
    "        Source: https://stackoverflow.com/questions/2511222/efficiently-generate-a-16-character-alphanumeric-string\n",
    "        \"\"\"\n",
    "        #x = ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "        x = ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "        set_len = len(x)\n",
    "        generated_set.add(x)\n",
    "        while set_len == len(generated_set):\n",
    "            x = ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "            generated_set.add(x)\n",
    "        return generated_set\n",
    "\n",
    "    gen_set = set()\n",
    "    for i in range(num_values):\n",
    "        gen_set = _generate_unique_string(gen_set, length)\n",
    "    return list(gen_set)\n",
    "\n",
    "\n",
    "def get_all_mappings(unique_values, column, output_dir, string_length):\n",
    "    \"\"\"Get mappings from the given unique values for a column to be anonymized.\"\"\"\n",
    "    print('Getting all mappings of values with randomly generated strings...')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    num_values = len(unique_values)\n",
    "    rand_values = _generate_strings(num_values, string_length)\n",
    "    all_mappings = dict(zip(unique_values, rand_values))\n",
    "\n",
    "    output_path = os.path.join(output_dir, column+'_mappings.json')\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        raise ValueError('Error! {} already exists. Please remove it and try again.'.format(output_path))\n",
    "        \n",
    "    with open(output_path, 'w') as fp:\n",
    "        json.dump(all_mappings, fp)\n",
    "    print('Success! Mappings saved to {}!'.format(output_path))\n",
    "    return output_path\n",
    "    \n",
    "    \n",
    "def read_mappings(mappings_path):\n",
    "    \"\"\"Read the patients mappings\"\"\"\n",
    "    with open(mappings_path, 'r') as fp:\n",
    "        mappings = json.load(mappings_path)\n",
    "    return mappings\n",
    "\n",
    "\n",
    "def apply_mappings(df0, output_path, all_mappings):\n",
    "    \"\"\"Add mapping columns.\"\"\"\n",
    "    df = df0.copy(deep=True)\n",
    "    \n",
    "    def _map_values(row, mappings, column):\n",
    "        \"\"\"Get mapping for a dataframe row's specific column\"\"\"\n",
    "        value = row[column]\n",
    "        return value\n",
    "\n",
    "    for column, mappings in all_mappings.items():\n",
    "        new_values = df.apply(_map_values, axis=1, args=(mappings,column))\n",
    "        df[column] = new_values\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def anonymize_ae_dataset(data_dir, fnames, output_dir, all_mappings):\n",
    "    \"\"\"Anonymize AE dataset.\"\"\"\n",
    "    import pdb; pdb.set_trace()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for fname in fnames:\n",
    "        print('Anonymizing {}...'.format(fname))\n",
    "        data_path = os.path.join(data_dir, fname)\n",
    "        output_path = os.path.join(output_dir, fname)\n",
    "        \n",
    "        df = pd.read_csv(data_path, low_memory=False)\n",
    "        df = df.iloc[:100, :] #TODO: Remove later\n",
    "        \n",
    "        df = apply_mappings(df, output_path, all_mappings)\n",
    "    print('Dataset Sucessfully Anonymized and Saved to {}!'.format(output_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AE\n",
    "ROOT_DIR = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/'\n",
    "RAW_DIR = os.path.join(ROOT_DIR, 'Raw')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'Anonymized')\n",
    "\n",
    "DATA_TYPES = ['365NoDeath', '365TestPhase']\n",
    "ANONYMIZED_COLUMN = 'patient_id'\n",
    "MAPPINGS_PATH = os.path.join(OUTPUT_DIR, '{}_mappings.csv'.format(ANONYMIZED_COLUMN))\n",
    "\n",
    "RANDOM_STRING_LENGTH = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = get_fpaths(RAW_DIR, DATA_TYPES)\n",
    "unique_values = get_unique_values(fpaths, ANONYMIZED_COLUMN)\n",
    "mappings_path = get_all_mappings(unique_values, ANONYMIZED_COLUMN, OUTPUT_DIR, RANDOM_STRING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE\n",
    "ROOT_DIR = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/RE/Data/'\n",
    "RAW_DIR = os.path.join(ROOT_DIR, 'Raw')\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'Anonymized')\n",
    "\n",
    "DATA_TYPES = ['365', '365TestPhase']\n",
    "MAPPINGS_PATH = os.path.join(OUTPUT_DIR, '{}_mappings.csv'.format(ANONYMIZED_COLUMN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = get_fpaths(RAW_DIR, DATA_TYPES)\n",
    "unique_values = get_unique_values(fpaths, ANONYMIZED_COLUMN)\n",
    "mappings_path = get_all_mappings(unique_values, ANONYMIZED_COLUMN, OUTPUT_DIR, RANDOM_STRING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the mappings\n",
    "#mappings = read_mappings(MAPPINGS_PATH)\n",
    "\n",
    "#Anonymize each AE data file\n",
    "#anonymize_ae_dataset(RAW_DIR_AE_365, fnames, OUTPUT_DIR_AE_365, mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-python3",
   "language": "python",
   "name": "venv-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
