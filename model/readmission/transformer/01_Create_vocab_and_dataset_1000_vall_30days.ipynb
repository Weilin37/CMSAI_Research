{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output dataset and create vocabulary for readmissions (all folds)\n",
    "\n",
    "**Author: Lin Lee Cheong <br> Last updated: 11/9/20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook to convert 365 version to 1000 version, and save CSV and vocabulary for  readmissions for all 5 folds**\n",
    "- training data & vocabulary\n",
    "- test data\n",
    "- up to 1000 events, from full 365 day dataset\n",
    "\n",
    "**Required:**\n",
    "- input file: raw_train_data.csv, raw_test_data.csv\n",
    "- outputs: csv files in 1000 format, and vocabulary\n",
    "\n",
    "**Nomenclature:**\n",
    "- d30: **60** days\n",
    "- s30: max 30 sequence a day\n",
    "- vpos: vocabulary positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import get_cuda\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input filepaths for training, test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "num_folds = 5\n",
    "\n",
    "main_dir = \"../../../data/readmission/\"\n",
    "train_dirs = [os.path.join(main_dir, f\"fold_{idx}/train/\") for idx in range(num_folds)]\n",
    "test_dirs = [os.path.join(main_dir, f\"fold_{idx}/test/\") for idx in range(num_folds)]\n",
    "vocab_dirs = [os.path.join(main_dir, f\"fold_{idx}/vocab/\") for idx in range(num_folds)]\n",
    "\n",
    "train_fps = [os.path.join(train_dir, \"raw_train_data.csv\") for train_dir in train_dirs]\n",
    "test_fps = [os.path.join(test_dir, \"raw_test_data.csv\") for test_dir in test_dirs]\n",
    "\n",
    "train_dl_fps = [os.path.join(train_dir, \"train_datalist_1000_vall_30days.pkl\") for train_dir in train_dirs]\n",
    "test_dl_fps = [os.path.join(test_dir, \"test_datalist_1000_vall_30days.pkl\") for test_dir in test_dirs]\n",
    "\n",
    "for vocab_dir in vocab_dirs:    \n",
    "    if not os.path.isdir(vocab_dir):\n",
    "        os.makedirs(vocab_dir)\n",
    "vocab_fps = [os.path.join(vocab_dir, f\"vocab_1000_vall_30days\") for vocab_dir in vocab_dirs]\n",
    "\n",
    "# Options\n",
    "ndays = 30\n",
    "x_lst = [str(x) for x in range(ndays, -1, -1)]\n",
    "x_flat_lst = [str(x) for x in range(999, -1, -1)]\n",
    "y_target = \"unplanned_readmission\"\n",
    "uid = \"discharge_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x, n_events=1000):\n",
    "    \"\"\"Flatten the 365 dataset into N long events\"\"\"\n",
    "    def get_days(x):\n",
    "        \"\"\"Calculate number of days between events\"\"\"\n",
    "        new_lst = []\n",
    "        counter = 0\n",
    "        counting = False\n",
    "        for event in x:\n",
    "            if event is np.nan:\n",
    "                if not counting:\n",
    "                    counting = True\n",
    "                counter += 1\n",
    "            if event is not np.nan:\n",
    "                if counting:\n",
    "                    counting = False\n",
    "                    new_lst.append(f\"{counter}_days\")\n",
    "                    new_lst.append(event)\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    new_lst.append(event)\n",
    "        return new_lst\n",
    "    \n",
    "    x = np.array(get_days(x))\n",
    "    lst = [str(day).replace(\" \", \"\").split(\",\") for day in x.ravel(\"K\")]\n",
    "    lst = [event for day in lst for event in day]\n",
    "    if len(lst) >= n_events:\n",
    "        return lst[-n_events:]\n",
    "\n",
    "    return [\"<pad>\"] * (n_events - len(lst)) + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_df(raw_df, x_lst, copy_lst):\n",
    "    '''\n",
    "    Function to flatten dataframe into 1000 long sequence\n",
    "    '''\n",
    "    flat_df = pd.DataFrame(raw_df[x_lst].apply(flatten, axis=1).tolist(),\n",
    "                        columns=[str(x) for x in range(999, -1, -1)])\n",
    "    for colname in copy_lst:\n",
    "        flat_df[colname] = raw_df[colname].copy(deep=True)\n",
    "    \n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_dataset(\n",
    "    data_fp,\n",
    "    x_lst,\n",
    "    x_flat_lst,\n",
    "    y_target,\n",
    "    uid,\n",
    "    train=True,\n",
    "    vocab_fp=None,\n",
    "    datalist_fp=None,\n",
    "    min_freq=10,\n",
    "    save_csv=False,\n",
    "):\n",
    "    # read in raw dataset, remove deaths\n",
    "    raw_df = read_data(data_fp=data_fp, check=True, y_target=y_target, uid=uid, test=0)\n",
    "    raw_df = remove_death(raw_df, y_target, x_lst)\n",
    "\n",
    "    raw_df = get_flat_df(\n",
    "        raw_df,\n",
    "        x_lst=x_lst,\n",
    "        copy_lst=[y_target, \"discharge_id\", \"discharge_dt\", \"patient_id\"],\n",
    "    )\n",
    "\n",
    "    if save_csv:\n",
    "        data_fp_write = data_fp.replace(\".csv\", \"_flatten_30days.csv\")\n",
    "        raw_df.to_csv(data_fp_write, index=False)\n",
    "\n",
    "    # build vocabulary and save if training dataset\n",
    "    if train:\n",
    "        print(\"Vocab generation required\")\n",
    "        vocab = build_vocab(raw_df, x_flat_lst, min_freq=min_freq, pos_labs_vocab=False)\n",
    "        \n",
    "        print(f\"Nb of tokens: {len(vocab.stoi)}\")\n",
    "        if vocab_fp is not None:\n",
    "            torch.save(vocab, vocab_fp)\n",
    "    # else:\n",
    "    #    vocab = torch.load(vocab_fp)\n",
    "\n",
    "    # build dataset and save\n",
    "    # whole_data = build_dataset(\n",
    "    #    raw_df, vocab, x_flat_lst, [y_target], day_length=1000, max_length=1\n",
    "    # )\n",
    "\n",
    "    # print('Whole data created\"')\n",
    "    # if datalist_fp is not None:\n",
    "    #    pickle.dump(whole_data, open(datalist_fp, \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44465 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1250861000,)\n",
      "exact word number:  1250861000\n",
      "Completed vocabulary: 28833 vocabs\n",
      "Nb of tokens: 28833\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_0/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_0/train/train_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44267 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1251059000,)\n",
      "exact word number:  1251059000\n",
      "Completed vocabulary: 28801 vocabs\n",
      "Nb of tokens: 28801\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_1/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_1/train/train_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44578 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1250748000,)\n",
      "exact word number:  1250748000\n",
      "Completed vocabulary: 28860 vocabs\n",
      "Nb of tokens: 28860\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_2/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_2/train/train_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295327, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855435\n",
      "True     0.144565\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44305 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1251022000,)\n",
      "exact word number:  1251022000\n",
      "Completed vocabulary: 28816 vocabs\n",
      "Nb of tokens: 28816\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_3/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_3/train/train_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295327, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855435\n",
      "True     0.144565\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44545 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (1250782000,)\n",
      "exact word number:  1250782000\n",
      "Completed vocabulary: 28892 vocabs\n",
      "Nb of tokens: 28892\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_4/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_4/train/train_datalist_1000_vall_30days.pkl\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_fp, train_dl_fp, vocab_fp) in enumerate(zip(train_fps, train_dl_fps, vocab_fps)):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "    \n",
    "    create_flat_dataset(\n",
    "        data_fp=train_fp,\n",
    "        x_lst=x_lst,\n",
    "        x_flat_lst=x_flat_lst,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        train=True,\n",
    "        vocab_fp=vocab_fp,\n",
    "        datalist_fp=train_dl_fp,\n",
    "        min_freq=1,\n",
    "        save_csv=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Completed, wrote to vocab: {vocab_fp}, \\n train data:{train_dl_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11075 rows contain the word death\n",
      "Completed, read from vocab: ../../../data/readmission/fold_0/vocab/vocab_1000_vall_30days, \n",
      " wrote to ../../../data/readmission/fold_0/test/test_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11273 rows contain the word death\n",
      "Completed, read from vocab: ../../../data/readmission/fold_1/vocab/vocab_1000_vall_30days, \n",
      " wrote to ../../../data/readmission/fold_1/test/test_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 10962 rows contain the word death\n",
      "Completed, read from vocab: ../../../data/readmission/fold_2/vocab/vocab_1000_vall_30days, \n",
      " wrote to ../../../data/readmission/fold_2/test/test_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323831, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855437\n",
      "True     0.144563\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11235 rows contain the word death\n",
      "Completed, read from vocab: ../../../data/readmission/fold_3/vocab/vocab_1000_vall_30days, \n",
      " wrote to ../../../data/readmission/fold_3/test/test_datalist_1000_vall_30days.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323831, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855437\n",
      "True     0.144563\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 10995 rows contain the word death\n",
      "Completed, read from vocab: ../../../data/readmission/fold_4/vocab/vocab_1000_vall_30days, \n",
      " wrote to ../../../data/readmission/fold_4/test/test_datalist_1000_vall_30days.pkl\n"
     ]
    }
   ],
   "source": [
    "for idx, (test_fp, test_dl_fp, vocab_fp) in enumerate(\n",
    "    zip(test_fps, test_dl_fps, vocab_fps)\n",
    "):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "    create_flat_dataset(\n",
    "        data_fp=test_fp,\n",
    "        x_lst=x_lst,\n",
    "        x_flat_lst=x_flat_lst,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        train=False,\n",
    "        vocab_fp=vocab_fp,\n",
    "        datalist_fp=train_dl_fp,\n",
    "        save_csv=True\n",
    "    )\n",
    "\n",
    "    print(f\"Completed, read from vocab: {vocab_fp}, \\n wrote to {test_dl_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
