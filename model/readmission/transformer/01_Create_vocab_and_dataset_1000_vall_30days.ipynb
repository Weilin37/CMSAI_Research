{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output dataset and create vocabulary for readmissions (all folds)\n",
    "\n",
    "**Author: Lin Lee Cheong <br> Last updated: 11/23/20** updated with fixes identified by Xiangyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook to convert 365 version to 1000 version, and save CSV and vocabulary for  readmissions for all 5 folds**\n",
    "- training data & vocabulary\n",
    "- test data\n",
    "- up to 1000 events, from full 365 day dataset\n",
    "\n",
    "**Required:**\n",
    "- input file: raw_train_data.csv, raw_test_data.csv\n",
    "- outputs: csv files in 1000 format, and vocabulary\n",
    "\n",
    "**Nomenclature:**\n",
    "- d30: **30** days\n",
    "- vpos: vocabulary positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from more_itertools import unique_everseen\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import get_cuda\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input filepaths for training, test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "num_folds = 5\n",
    "\n",
    "main_dir = \"../../../data/readmission/\"\n",
    "train_dirs = [os.path.join(main_dir, f\"fold_{idx}/train/\") for idx in range(num_folds)]\n",
    "test_dirs = [os.path.join(main_dir, f\"fold_{idx}/test/\") for idx in range(num_folds)]\n",
    "vocab_dirs = [os.path.join(main_dir, f\"fold_{idx}/vocab/\") for idx in range(num_folds)]\n",
    "\n",
    "train_fps = [os.path.join(train_dir, \"raw_train_data.csv\") for train_dir in train_dirs]\n",
    "test_fps = [os.path.join(test_dir, \"raw_test_data.csv\") for test_dir in test_dirs]\n",
    "\n",
    "out_train_fps = [os.path.join(train_dir, \"raw_train_data_1000_30days.csv\") for train_dir in train_dirs]\n",
    "out_test_fps = [os.path.join(test_dir, \"raw_test_data_1000_30days.csv\") for test_dir in test_dirs]\n",
    "\n",
    "for vocab_dir in vocab_dirs:    \n",
    "    if not os.path.isdir(vocab_dir):\n",
    "        os.makedirs(vocab_dir)\n",
    "vocab_fps = [os.path.join(vocab_dir, f\"vocab_1000_vall_30days\") for vocab_dir in vocab_dirs]\n",
    "\n",
    "# Options\n",
    "ndays = 30 # number of days to keep\n",
    "x_lst = [str(x) for x in range(ndays, -1, -1)] # total days in datasets, usually 365. Check all for death events\n",
    "x_flat_lst = [str(x) for x in range(999, -1, -1)] # up to 1000 events in flattened list\n",
    "y_target = \"unplanned_readmission\"\n",
    "uid = \"discharge_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x, n_events=1000):\n",
    "    \"\"\"Flatten the 365 dataset into N long events\"\"\"\n",
    "    def get_days(x):\n",
    "        \"\"\"Calculate number of days between events\"\"\"\n",
    "        new_lst = []\n",
    "        counter = 1\n",
    "        counting = False\n",
    "        for event in x:\n",
    "            if event is np.nan or (type(event) == float and math.isnan(event)):\n",
    "                if not counting:\n",
    "                    counting = True\n",
    "                counter += 1\n",
    "            else:\n",
    "                \n",
    "                if counting:\n",
    "                    counting = False\n",
    "                    try:\n",
    "                        event = f\"{counter + 1}_days,\" + event\n",
    "                    except:\n",
    "                        print(type(counter), counter)\n",
    "                        print(event, type(event))\n",
    "                    new_lst.append(event)\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    event = \"1_days,\" + event\n",
    "                    new_lst.append(event)\n",
    "                    \n",
    "        return new_lst\n",
    "    \n",
    "    # count days with no events, move admission/discharge to the end of the day, dedupe events per day\n",
    "    x = np.array(get_days(x))\n",
    "    lst = [move_ad_dis(str(day).replace(\" \", \"\").split(\",\")) for day in x.ravel(\"K\")]\n",
    "    \n",
    "    # flatten, clean up corner cases\n",
    "    lst = [event for day in lst for event in day]\n",
    "    if '_days' in lst[0]:\n",
    "        lst = lst[1:]\n",
    "    if len(lst) >= n_events:\n",
    "        return lst[-n_events:]\n",
    "\n",
    "    return [\"<pad>\"] * (n_events - len(lst)) + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_ad_dis(events_in_day):\n",
    "    \"\"\"Move admission and discharge to the end of the list, dedupe events\"\"\"\n",
    "    if not isinstance(events_in_day, list):\n",
    "        return events_in_day\n",
    "\n",
    "    events_in_day = list(unique_everseen(events_in_day))\n",
    "    has_admission = False\n",
    "    has_discharge = False\n",
    "\n",
    "    if \"admission\" in events_in_day:\n",
    "        has_admission = True\n",
    "        events_in_day.remove(\"admission\")\n",
    "\n",
    "    if \"discharge\" in events_in_day:\n",
    "        has_discharge = True\n",
    "        events_in_day.remove(\"discharge\")\n",
    "\n",
    "    if has_admission:\n",
    "        events_in_day.append(\"admission\")\n",
    "\n",
    "    if has_discharge:\n",
    "        events_in_day.append(\"discharge\")\n",
    "\n",
    "    return events_in_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_df(raw_df, x_lst, copy_lst):\n",
    "    \"\"\"\n",
    "    Function to flatten dataframe into 1000 long sequence.\n",
    "    \n",
    "    Calls function flatten, which in turn calls move_ad_dis\n",
    "    \"\"\"\n",
    "    flat_df = pd.DataFrame(\n",
    "        raw_df[x_lst].apply(flatten, axis=1).tolist(),\n",
    "        columns=[str(x) for x in range(999, -1, -1)],\n",
    "    )\n",
    "\n",
    "    for colname in copy_lst:\n",
    "        flat_df[colname] = raw_df[colname].copy(deep=True)\n",
    "\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_dataset(\n",
    "    data_fp,\n",
    "    x_lst,\n",
    "    x_flat_lst,\n",
    "    y_target,\n",
    "    uid,\n",
    "    vocab_fp=None,\n",
    "    output_fp=None,\n",
    "    min_freq=500,\n",
    "    return_csv=False,\n",
    "    nrows=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to create flattened dataset: Reads in raw data, \n",
    "    removes death events, and flattens and saves to output CSV.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    data_fp (str) : input filepath, csv\n",
    "    x_lst (list) : list of column names (days) to use for flattening\n",
    "    x_flat_lst (list) : list of column names to use for writing out the \n",
    "                        flattened file (1000 events)\n",
    "    y_target (str) : column name of target\n",
    "    uid (str) : column name of unique identifier\n",
    "    vocab_fp (str) : path to write out vocab\n",
    "        default None (will not generate vocabulary)\n",
    "    output_fp (str) : path to write out flattened CSV file\n",
    "        default None (will not save)\n",
    "    min_freq (int) : minimum frequency associated with vocabulary generation\n",
    "    return_csv (bool) : default False, returns flattened dataframe if True\n",
    "    nrows (int) : default 0 to read and process all, otherwise\n",
    "                  will read in nrows in CSV only (for testing)\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    default None unless return_csv is True, then return\n",
    "        dataframe containing flattened data\n",
    "    \"\"\"\n",
    "\n",
    "    # read in raw dataset, remove deaths\n",
    "    raw_df = read_data(\n",
    "        data_fp=data_fp, check=True, y_target=y_target, uid=uid, test=nrows\n",
    "    )\n",
    "    raw_df = remove_death(raw_df, y_target, x_lst)\n",
    "\n",
    "    raw_df = get_flat_df(\n",
    "        raw_df,\n",
    "        x_lst=x_lst,\n",
    "        copy_lst=[y_target, \"discharge_id\", \"discharge_dt\", \"patient_id\"],\n",
    "    )\n",
    "\n",
    "    if output_fp is not None and isinstance(output_fp, str):\n",
    "        raw_df.to_csv(output_fp, index=False)\n",
    "\n",
    "    # build vocabulary\n",
    "    if vocab_fp is not None:\n",
    "        print(\"Vocab generation required\")\n",
    "        vocab = build_vocab(raw_df, x_flat_lst, min_freq=min_freq, pos_labs_vocab=False)\n",
    "\n",
    "        print(f\"Nb of tokens: {len(vocab.stoi)}\")\n",
    "        torch.save(vocab, vocab_fp)\n",
    "\n",
    "    if return_csv:\n",
    "        return raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.846\n",
      "True     0.154\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 155 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (4845000,)\n",
      "exact word number:  4845000\n",
      "Completed vocabulary: 99 vocabs\n",
      "Nb of tokens: 99\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_0/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_0/train/raw_train_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8478\n",
      "True     0.1522\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 146 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (4854000,)\n",
      "exact word number:  4854000\n",
      "Completed vocabulary: 98 vocabs\n",
      "Nb of tokens: 98\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_1/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_1/train/raw_train_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8466\n",
      "True     0.1534\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 147 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (4853000,)\n",
      "exact word number:  4853000\n",
      "Completed vocabulary: 99 vocabs\n",
      "Nb of tokens: 99\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_2/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_2/train/raw_train_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8436\n",
      "True     0.1564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 151 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (4849000,)\n",
      "exact word number:  4849000\n",
      "Completed vocabulary: 99 vocabs\n",
      "Nb of tokens: 99\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_3/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_3/train/raw_train_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8484\n",
      "True     0.1516\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 155 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (4845000,)\n",
      "exact word number:  4845000\n",
      "Completed vocabulary: 96 vocabs\n",
      "Nb of tokens: 96\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_4/vocab/vocab_1000_vall_30days, \n",
      " train data:../../../data/readmission/fold_4/train/raw_train_data_1000_30days.csv\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_fp, output_fp, vocab_fp) in enumerate(\n",
    "    zip(train_fps, out_train_fps, vocab_fps)\n",
    "):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "\n",
    "    create_flat_dataset(\n",
    "        data_fp=train_fp,\n",
    "        x_lst=x_lst,\n",
    "        x_flat_lst=x_flat_lst,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        vocab_fp=vocab_fp,\n",
    "        output_fp=output_fp,\n",
    "        min_freq=500,\n",
    "        return_csv=False,\n",
    "        nrows=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Completed, wrote to vocab: {vocab_fp}, \\n train data:{output_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8532\n",
      "True     0.1468\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 136 rows contain the word death\n",
      "Completed, wrote to ../../../data/readmission/fold_0/test/raw_test_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8534\n",
      "True     0.1466\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 153 rows contain the word death\n",
      "Completed, wrote to ../../../data/readmission/fold_1/test/raw_test_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8534\n",
      "True     0.1466\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 172 rows contain the word death\n",
      "Completed, wrote to ../../../data/readmission/fold_2/test/raw_test_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8494\n",
      "True     0.1506\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 162 rows contain the word death\n",
      "Completed, wrote to ../../../data/readmission/fold_3/test/raw_test_data_1000_30days.csv\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (5000, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.8566\n",
      "True     0.1434\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 150 rows contain the word death\n",
      "Completed, wrote to ../../../data/readmission/fold_4/test/raw_test_data_1000_30days.csv\n"
     ]
    }
   ],
   "source": [
    "for idx, (test_fp, output_fp, vocab_fp) in enumerate(\n",
    "    zip(test_fps, out_test_fps, vocab_fps)\n",
    "):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "    create_flat_dataset(\n",
    "        data_fp=test_fp,\n",
    "        x_lst=x_lst,\n",
    "        x_flat_lst=x_flat_lst,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        vocab_fp=None,\n",
    "        output_fp=output_fp,\n",
    "        min_freq=500,\n",
    "        return_csv=False,\n",
    "        nrows=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Completed, wrote to {output_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
