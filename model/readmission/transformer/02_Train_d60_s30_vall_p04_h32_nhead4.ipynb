{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training all folds d60_s30_vpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import get_cuda\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader, read_pickled_ds\n",
    "from transformer_model import TransformerCNNModel\n",
    "from model_func import training_process, epoch_val, epoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "num_folds = 5\n",
    "\n",
    "main_dir = \"../../../data/readmission/\"\n",
    "train_dirs = [os.path.join(main_dir, f\"fold_{idx}/train/\") for idx in range(num_folds)]\n",
    "test_dirs = [os.path.join(main_dir, f\"fold_{idx}/test/\") for idx in range(num_folds)]\n",
    "vocab_dirs = [os.path.join(main_dir, f\"fold_{idx}/vocab/\") for idx in range(num_folds)]\n",
    "\n",
    "train_dl_fps = [os.path.join(train_dir, \"train_datalist_d60_s30_vall_mf10.pkl\") for train_dir in train_dirs]\n",
    "test_dl_fps = [os.path.join(test_dir, \"test_datalist_d60_s30_vall_mf10.pkl\") for test_dir in test_dirs]\n",
    "vocab_fps = [os.path.join(vocab_dir, f\"vocab_d60_s30_vall_mf10\") for vocab_dir in vocab_dirs]\n",
    "\n",
    "#model_dir = \"./models_d30_s30_vpos/\"\n",
    "# Options\n",
    "x_lst = [str(x) for x in range(365, -1, -1)]\n",
    "n_days = 60\n",
    "seq_per_day = 30\n",
    "y_target = \"unplanned_readmission\"\n",
    "uid = \"discharge_id\"\n",
    "batch_size=140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data and create data loader: single fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 8 # embedding dimension\n",
    "nhid = 32 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.4 # the dropout value\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************Fold 0**************************************************\n",
      "Processing \n",
      "         \t train: ../../../data/readmission/fold_0/train/train_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t test: ../../../data/readmission/fold_0/test/test_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t vocab: ../../../data/readmission/fold_0/vocab/vocab_d60_s30_vall_mf10 \n",
      "\n",
      "Nb tokens: 20440\n",
      "parameters: embsize:8, nhead:4, nhid:32, nlayers:1, dropout:0.4\n",
      "available device: cuda\n",
      "number of GPUS available: 4\n",
      "device: gpu\n",
      "----------Epoch 1/6------------------------------\n",
      "epoch_train_loss: 0.4054347395896912 epoch train AUC: 0.6464787897025881\n",
      "epoch_val_loss: 0.4038284572913589 epoch val AUC: 0.6637537158247352\n",
      "----------Epoch 2/6------------------------------\n",
      "epoch_train_loss: 0.40014958649990473 epoch train AUC: 0.6651080082782889\n",
      "epoch_val_loss: 0.40214353180161094 epoch val AUC: 0.6658956928194959\n",
      "----------Epoch 3/6------------------------------\n",
      "epoch_train_loss: 0.3984028331864527 epoch train AUC: 0.6709193094918585\n",
      "epoch_val_loss: 0.40154522566013073 epoch val AUC: 0.666680016026342\n",
      "----------Epoch 4/6------------------------------\n",
      "epoch_train_loss: 0.39709794464295445 epoch train AUC: 0.675017057545356\n",
      "epoch_val_loss: 0.39990842968038715 epoch val AUC: 0.6668846721231809\n",
      "----------Epoch 5/6------------------------------\n",
      "epoch_train_loss: 0.3962614064334689 epoch train AUC: 0.6774274596522636\n",
      "epoch_val_loss: 0.4016105730016528 epoch val AUC: 0.6673104966194294\n",
      "----------Epoch 6/6------------------------------\n",
      "epoch_train_loss: 0.39548691276004344 epoch train AUC: 0.6798087168103456\n",
      "epoch_val_loss: 0.4014610666164673 epoch val AUC: 0.6674980960066152\n",
      "\n",
      "******************************Fold 1**************************************************\n",
      "Processing \n",
      "         \t train: ../../../data/readmission/fold_1/train/train_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t test: ../../../data/readmission/fold_1/test/test_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t vocab: ../../../data/readmission/fold_1/vocab/vocab_d60_s30_vall_mf10 \n",
      "\n",
      "Nb tokens: 20471\n",
      "parameters: embsize:8, nhead:4, nhid:32, nlayers:1, dropout:0.4\n",
      "available device: cuda\n",
      "number of GPUS available: 4\n",
      "device: gpu\n",
      "----------Epoch 1/6------------------------------\n",
      "epoch_train_loss: 0.4056918027101213 epoch train AUC: 0.6458424479614923\n",
      "epoch_val_loss: 0.4037760563872077 epoch val AUC: 0.6653455147950036\n",
      "----------Epoch 2/6------------------------------\n",
      "epoch_train_loss: 0.4001239882973869 epoch train AUC: 0.6653151330637335\n",
      "epoch_val_loss: 0.40802420834575703 epoch val AUC: 0.6676621140011139\n",
      "----------Epoch 3/6------------------------------\n",
      "epoch_train_loss: 0.3984350684643352 epoch train AUC: 0.6708287698425076\n",
      "epoch_val_loss: 0.4001544804061628 epoch val AUC: 0.6682787675023256\n",
      "----------Epoch 4/6------------------------------\n",
      "epoch_train_loss: 0.3973524625894206 epoch train AUC: 0.6741515997133005\n",
      "epoch_val_loss: 0.40019061752381047 epoch val AUC: 0.6674917136275019\n",
      "----------Epoch 5/6------------------------------\n",
      "epoch_train_loss: 0.3963467023679745 epoch train AUC: 0.6772780212714268\n",
      "epoch_val_loss: 0.4013109060677508 epoch val AUC: 0.6670279485817139\n",
      "----------Epoch 6/6------------------------------\n",
      "epoch_train_loss: 0.39551988847373637 epoch train AUC: 0.6798211056768824\n",
      "epoch_val_loss: 0.3994327081234744 epoch val AUC: 0.6672470688028157\n",
      "\n",
      "******************************Fold 2**************************************************\n",
      "Processing \n",
      "         \t train: ../../../data/readmission/fold_2/train/train_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t test: ../../../data/readmission/fold_2/test/test_datalist_d60_s30_vall_mf10.pkl \n",
      "         \t vocab: ../../../data/readmission/fold_2/vocab/vocab_d60_s30_vall_mf10 \n",
      "\n",
      "Nb tokens: 20438\n",
      "parameters: embsize:8, nhead:4, nhid:32, nlayers:1, dropout:0.4\n",
      "available device: cuda\n",
      "number of GPUS available: 4\n",
      "device: gpu\n",
      "----------Epoch 1/6------------------------------\n",
      "epoch_train_loss: 0.40642968214462877 epoch train AUC: 0.6429188455526571\n",
      "epoch_val_loss: 0.4015609063624802 epoch val AUC: 0.6645922436033052\n",
      "----------Epoch 2/6------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_metric = {}\n",
    "for fold, (train_fp, test_fp, vocab_fp) in enumerate(\n",
    "    zip(train_dl_fps, test_dl_fps, vocab_fps)\n",
    "):\n",
    "    print(\"\\n\" + \"*\" * 30 + \"Fold {}\".format(fold) + \"*\" * 50)\n",
    "    print(\n",
    "        f\"Processing \\n \\\n",
    "        \\t train: {train_fp} \\n \\\n",
    "        \\t test: {test_fp} \\n \\\n",
    "        \\t vocab: {vocab_fp} \\n\"\n",
    "    )\n",
    "\n",
    "    whole_ids, whole_data, whole_labels, whole_mask = read_pickled_ds(\n",
    "        file_dir=train_fp, seq_length=n_days, event_length=seq_per_day\n",
    "    )\n",
    "        \n",
    "    train_dataset = BuildDataset(seq_length=60, event_length=30, \n",
    "        data_list=[whole_ids, whole_data, whole_labels, whole_mask]\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    whole_ids, whole_data, whole_labels, whole_mask = read_pickled_ds(\n",
    "        file_dir=test_fp, seq_length=n_days, event_length=seq_per_day\n",
    "    )\n",
    "    test_dataset = BuildDataset(seq_length=60, event_length=30, \n",
    "        data_list=[whole_ids, whole_data, whole_labels, whole_mask]\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "    )\n",
    "    \n",
    "    vocab = torch.load(vocab_fp)\n",
    "    ntokens = len(vocab.stoi)\n",
    "    print(f\"Nb tokens: {ntokens}\")\n",
    "    \n",
    "    model = TransformerCNNModel(\n",
    "        ntokens,\n",
    "        emsize,\n",
    "        nhead,\n",
    "        nhid,\n",
    "        nlayers,\n",
    "        n_class,\n",
    "        device=\"gpu\",\n",
    "        seq_length=n_days,\n",
    "        num_events=seq_per_day,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    train_metric[\"fold_\" + str(fold)] = training_process(\n",
    "        model=model,\n",
    "        epoch=6,\n",
    "        dataloaders={\"train\": train_dataloader, \"val\": test_dataloader},\n",
    "        save_model=None,\n",
    "        test=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc = []\n",
    "print(\"individual fold results\")\n",
    "for idx in range(num_folds):\n",
    "    print(f\"fold_{idx}: {max(train_metric[f'fold_{idx}']['val_metric'])}\")\n",
    "    overall_auc.append(max(train_metric[f'fold_{idx}']['val_metric']))\n",
    "          \n",
    "print('Average AUC: {}'.format(np.round(np.mean(overall_auc), 3)))\n",
    "print('Std AUC: {}'.format(np.round(np.std(overall_auc), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in range(num_folds):\n",
    "    plt.plot(train_metric[f'fold_{idx}']['val_metric'], color='red')\n",
    "    plt.plot(train_metric[f'fold_{idx}']['train_metric'], color='purple')\n",
    "    \n",
    "plt.grid(color='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
