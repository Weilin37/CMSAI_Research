{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readmission Modeling: Script 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read original data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://cmsai-mrk-amzn/CSVModelInputs/readmission_input_targets_365_v2.csv to data_d/readmission_input_targets_365_v2.csv\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://cmsai-mrk-amzn/CSVModelInputs/readmission_input_targets_365_v2.csv data_d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths, feature names and target names\n",
    "FP = './data/readmission_input_targets_365_v2.csv'\n",
    "x_lst = [str(x) for x in range(365, -1, -1)]\n",
    "y_lst = ['unplanned_readmission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (4,5,6,7,8,9,12,15,16,17,18,19,20,21,22,23,24,25,26,28,29,30,32,33,34,36,37,38,40,43,50,52,53,54,55,57,60,68,70,71,73,74,75,76,77,78,81,82,88,90,91,92,93,94,95,97,98,99,102,103,104,106,107,108,109,110,112,116,118,119,120,121,124,125,126,129,133,134,135,136,137,138,139,140,144,145,146,149,154,155,158,159,162,163,165,166,167,168,172,173,176,177,179,181,183,184,186,187,188,189,190,192,194,195,196,200,201,202,203,205,208,209,215,216,217,218,220,222,223,225,227,228,230,232,246,247,248,251,253,254,256,258,261,273,277,293,302,303,310,314,325) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>discharge_dt</th>\n",
       "      <th>discharge_id</th>\n",
       "      <th>365</th>\n",
       "      <th>364</th>\n",
       "      <th>363</th>\n",
       "      <th>362</th>\n",
       "      <th>361</th>\n",
       "      <th>360</th>\n",
       "      <th>359</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>unplanned_readmission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002085</td>\n",
       "      <td>20110922</td>\n",
       "      <td>100002085_20110922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_7295, d_78650, d_78652, d_78659, d_78659, h_...</td>\n",
       "      <td>...</td>\n",
       "      <td>d_486, d_5119, d_5128, d_5183, d_80709, h_7101...</td>\n",
       "      <td>d_496, d_80709, h_99232, h_99233</td>\n",
       "      <td>d_5119, d_80709, d_8600, h_00528, h_31645, h_3...</td>\n",
       "      <td>d_496, d_51889, h_71010</td>\n",
       "      <td>d_496, d_51189, d_5119, d_7931, d_80709, h_710...</td>\n",
       "      <td>d_5119, d_51919, d_7931, d_80709, h_00520, h_7...</td>\n",
       "      <td>d_5119, d_5128, d_5180, d_7931, d_80709, h_710...</td>\n",
       "      <td>d_5119, d_5183, d_80709, d_8600, d_V5399, h_32...</td>\n",
       "      <td>admission, d_496, d_72887, d_78605, d_78650, d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002829</td>\n",
       "      <td>20111013</td>\n",
       "      <td>100002829_20111013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_28521, d_58881, h_82310, h_84100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>h_90999, h_J1270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h_90999, h_J1270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h_90999, h_J1270, h_J1756</td>\n",
       "      <td>admission, d_40391, d_5856, d_5856, d_59970, d...</td>\n",
       "      <td>d_5856, d_59970, d_92303, d_9233, h_00400, h_1...</td>\n",
       "      <td>d_4019, d_5856, d_59970, discharge, h_90732, h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003379</td>\n",
       "      <td>20091207</td>\n",
       "      <td>100003379_20091207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_586, h_99231</td>\n",
       "      <td>d_99883, h_99231</td>\n",
       "      <td>d_586, d_99883, h_11042, h_99231, p_8622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_99883, h_99232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission, d_586, d_71945, d_V4989, discharge,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004211</td>\n",
       "      <td>20110102</td>\n",
       "      <td>100004211_20110102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_42731, d_42822, d_78650, h_93010, h_99214, h...</td>\n",
       "      <td>d_53081, d_78902, h_99214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>d_4019, d_42731, h_99233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_1950, d_5119, d_5738, h_71010, h_76705, h_99232</td>\n",
       "      <td>d_1539, d_V667, h_99233</td>\n",
       "      <td>d_1975, h_99233</td>\n",
       "      <td>d_1975, d_51881, h_99233</td>\n",
       "      <td>d_1975, d_42731, h_99233</td>\n",
       "      <td>d_1975, h_99233</td>\n",
       "      <td>d_1975, d_42731, d_51881, death, discharge, h_...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100008869</td>\n",
       "      <td>20101116</td>\n",
       "      <td>100008869_20101116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_29633, h_90806</td>\n",
       "      <td>d_53550, h_99213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>d_29620, h_99231</td>\n",
       "      <td>d_29620, h_99231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>admission, d_25000, d_29620, d_29623, d_4019, ...</td>\n",
       "      <td>d_41401, d_78650, h_93306, h_99222, h_99232</td>\n",
       "      <td>d_78650, h_99231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d_78650, discharge, h_99238</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  discharge_dt        discharge_id  365  \\\n",
       "0   100002085      20110922  100002085_20110922  NaN   \n",
       "1   100002829      20111013  100002829_20111013  NaN   \n",
       "2   100003379      20091207  100003379_20091207  NaN   \n",
       "3   100004211      20110102  100004211_20110102  NaN   \n",
       "4   100008869      20101116  100008869_20101116  NaN   \n",
       "\n",
       "                                  364               363  \\\n",
       "0                                 NaN               NaN   \n",
       "1  d_28521, d_58881, h_82310, h_84100               NaN   \n",
       "2                                 NaN               NaN   \n",
       "3                                 NaN               NaN   \n",
       "4                    d_29633, h_90806  d_53550, h_99213   \n",
       "\n",
       "                                                 362  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  d_42731, d_42822, d_78650, h_93010, h_99214, h...   \n",
       "4                                                NaN   \n",
       "\n",
       "                         361  360  \\\n",
       "0                        NaN  NaN   \n",
       "1                        NaN  NaN   \n",
       "2                        NaN  NaN   \n",
       "3  d_53081, d_78902, h_99214  NaN   \n",
       "4                        NaN  NaN   \n",
       "\n",
       "                                                 359  ...  \\\n",
       "0  d_7295, d_78650, d_78652, d_78659, d_78659, h_...  ...   \n",
       "1                                                NaN  ...   \n",
       "2                                                NaN  ...   \n",
       "3                                                NaN  ...   \n",
       "4                                                NaN  ...   \n",
       "\n",
       "                                                   8  \\\n",
       "0  d_486, d_5119, d_5128, d_5183, d_80709, h_7101...   \n",
       "1                                   h_90999, h_J1270   \n",
       "2                                                NaN   \n",
       "3                           d_4019, d_42731, h_99233   \n",
       "4                                   d_29620, h_99231   \n",
       "\n",
       "                                  7  \\\n",
       "0  d_496, d_80709, h_99232, h_99233   \n",
       "1                               NaN   \n",
       "2                    d_586, h_99231   \n",
       "3                               NaN   \n",
       "4                  d_29620, h_99231   \n",
       "\n",
       "                                                   6  \\\n",
       "0  d_5119, d_80709, d_8600, h_00528, h_31645, h_3...   \n",
       "1                                   h_90999, h_J1270   \n",
       "2                                   d_99883, h_99231   \n",
       "3  d_1950, d_5119, d_5738, h_71010, h_76705, h_99232   \n",
       "4                                                NaN   \n",
       "\n",
       "                                          5  \\\n",
       "0                   d_496, d_51889, h_71010   \n",
       "1                                       NaN   \n",
       "2  d_586, d_99883, h_11042, h_99231, p_8622   \n",
       "3                   d_1539, d_V667, h_99233   \n",
       "4                                       NaN   \n",
       "\n",
       "                                                   4  \\\n",
       "0  d_496, d_51189, d_5119, d_7931, d_80709, h_710...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                    d_1975, h_99233   \n",
       "4  admission, d_25000, d_29620, d_29623, d_4019, ...   \n",
       "\n",
       "                                                   3  \\\n",
       "0  d_5119, d_51919, d_7931, d_80709, h_00520, h_7...   \n",
       "1                          h_90999, h_J1270, h_J1756   \n",
       "2                                   d_99883, h_99232   \n",
       "3                           d_1975, d_51881, h_99233   \n",
       "4        d_41401, d_78650, h_93306, h_99222, h_99232   \n",
       "\n",
       "                                                   2  \\\n",
       "0  d_5119, d_5128, d_5180, d_7931, d_80709, h_710...   \n",
       "1  admission, d_40391, d_5856, d_5856, d_59970, d...   \n",
       "2                                                NaN   \n",
       "3                           d_1975, d_42731, h_99233   \n",
       "4                                   d_78650, h_99231   \n",
       "\n",
       "                                                   1  \\\n",
       "0  d_5119, d_5183, d_80709, d_8600, d_V5399, h_32...   \n",
       "1  d_5856, d_59970, d_92303, d_9233, h_00400, h_1...   \n",
       "2                                                NaN   \n",
       "3                                    d_1975, h_99233   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                   0 unplanned_readmission  \n",
       "0  admission, d_496, d_72887, d_78605, d_78650, d...                 False  \n",
       "1  d_4019, d_5856, d_59970, discharge, h_90732, h...                 False  \n",
       "2  admission, d_586, d_71945, d_V4989, discharge,...                  True  \n",
       "3  d_1975, d_42731, d_51881, death, discharge, h_...                 False  \n",
       "4                        d_78650, discharge, h_99238                 False  \n",
       "\n",
       "[5 rows x 370 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_targets_365_df = pd.read_csv(FP)\n",
    "re_targets_365_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicate IDs\n",
    "re_targets_365_df.discharge_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.855435\n",
       "True     0.144565\n",
       "Name: unplanned_readmission, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_targets_365_df.unplanned_readmission.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619157, 370)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "re_targets_365_df.drop_duplicates('discharge_id', inplace=True)\n",
    "re_targets_365_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619157, 370)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_targets_365_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619158, 370)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_targets_365_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove death events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55569\n",
      "(1563588, 370)\n"
     ]
    }
   ],
   "source": [
    "np_re = re_targets_365_df.astype(str)\n",
    "np_re['unplanned_readmission'] = re_targets_365_df[y_lst].astype(int).copy()\n",
    "indeces = set()\n",
    "for i in x_lst:\n",
    "    indeces.update(np_re[np_re[i].str.contains('death')].index)\n",
    "print(len(indeces)) #55568\n",
    "np_re = np_re[~np_re.index.isin(indeces)]\n",
    "print(np_re.shape) #(1563578, 370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55568\n",
      "(1563578, 370)\n"
     ]
    }
   ],
   "source": [
    "# previous runs\n",
    "np_re = re_targets_365_df.astype(str)\n",
    "np_re['unplanned_readmission'] = re_targets_365_df[y_lst].astype(int).copy()\n",
    "indeces = set()\n",
    "for i in x_lst:\n",
    "    indeces.update(np_re[np_re[i].str.contains('death')].index)\n",
    "print(len(indeces)) #55568\n",
    "np_re = np_re[~np_re.index.isin(indeces)]\n",
    "print(np_re.shape) #(1563578, 370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data with no more deaths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(np_re, 'data_d/np_re')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check frequencies and length of sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload if needed\n",
    "np_re = torch.load('data_d/np_re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619157, 370)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_targets_365_df.shape #(1619146, 370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619146, 370)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous runs\n",
    "re_targets_365_df.shape #(1619146, 370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86613670"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous runs\n",
    "test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np_re[x_lst[-30:]].values.ravel('K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "count = 0\n",
    "for w in words:\n",
    "    l = len(str(w).split(', '))\n",
    "    if l not in dic:\n",
    "        dic[l] = 1\n",
    "    else:\n",
    "        dic[l] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 1\n",
      "94 1\n",
      "89 1\n",
      "110 1\n",
      "80 1\n",
      "90 1\n",
      "157 1\n",
      "99 1\n",
      "111 1\n",
      "77 1\n",
      "91 1\n",
      "76 2\n",
      "121 2\n",
      "74 2\n",
      "86 2\n",
      "79 2\n",
      "105 2\n",
      "97 2\n",
      "85 2\n",
      "103 2\n",
      "98 3\n",
      "73 4\n",
      "78 4\n",
      "75 6\n",
      "70 7\n",
      "69 8\n",
      "71 8\n",
      "63 9\n",
      "72 9\n",
      "65 10\n",
      "67 10\n",
      "66 11\n",
      "68 12\n",
      "64 14\n",
      "61 22\n",
      "60 24\n",
      "62 24\n",
      "58 36\n",
      "59 40\n",
      "56 44\n",
      "57 49\n",
      "55 79\n",
      "54 84\n",
      "53 97\n",
      "52 124\n",
      "50 139\n",
      "51 143\n",
      "49 164\n",
      "48 196\n",
      "47 226\n",
      "46 300\n",
      "45 330\n",
      "44 400\n",
      "43 458\n",
      "42 548\n",
      "41 718\n",
      "40 804\n",
      "39 966\n",
      "38 1102\n",
      "37 1341\n",
      "36 1757\n",
      "35 2055\n",
      "34 2482\n",
      "33 3109\n",
      "32 3745\n",
      "31 4466\n",
      "30 5829\n",
      "29 7091\n",
      "28 9077\n",
      "27 11777\n",
      "26 14906\n",
      "25 18858\n",
      "24 24380\n",
      "23 31080\n",
      "22 39874\n",
      "21 51007\n",
      "20 65557\n",
      "19 82750\n",
      "18 103265\n",
      "17 128467\n",
      "16 157280\n",
      "15 188086\n",
      "14 222909\n",
      "13 256422\n",
      "12 311053\n",
      "11 350046\n",
      "10 432805\n",
      "9 483982\n",
      "8 630503\n",
      "7 757443\n",
      "6 1086147\n",
      "5 1260608\n",
      "4 1964510\n",
      "3 1971199\n",
      "2 3614046\n",
      "1 32600467\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(dic.items(), key=lambda x: x[1]):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46907640"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 1\n",
      "94 1\n",
      "89 1\n",
      "110 1\n",
      "80 1\n",
      "90 1\n",
      "157 1\n",
      "99 1\n",
      "111 1\n",
      "77 1\n",
      "91 1\n",
      "76 2\n",
      "121 2\n",
      "74 2\n",
      "86 2\n",
      "79 2\n",
      "105 2\n",
      "97 2\n",
      "85 2\n",
      "103 2\n",
      "98 3\n",
      "73 4\n",
      "78 4\n",
      "75 6\n",
      "70 7\n",
      "69 8\n",
      "71 8\n",
      "63 9\n",
      "72 9\n",
      "65 10\n",
      "67 10\n",
      "66 11\n",
      "68 12\n",
      "64 14\n",
      "61 22\n",
      "60 24\n",
      "62 24\n",
      "58 36\n",
      "59 40\n",
      "56 44\n",
      "57 49\n",
      "55 79\n",
      "54 84\n",
      "53 97\n",
      "52 124\n",
      "50 138\n",
      "51 144\n",
      "49 164\n",
      "48 196\n",
      "47 225\n",
      "46 299\n",
      "45 330\n",
      "44 400\n",
      "43 458\n",
      "42 549\n",
      "41 718\n",
      "40 803\n",
      "39 967\n",
      "38 1101\n",
      "37 1342\n",
      "36 1757\n",
      "35 2057\n",
      "34 2479\n",
      "33 3109\n",
      "32 3747\n",
      "31 4466\n",
      "30 5832\n",
      "29 7091\n",
      "28 9081\n",
      "27 11772\n",
      "26 14914\n",
      "25 18853\n",
      "24 24402\n",
      "23 31092\n",
      "22 39875\n",
      "21 51061\n",
      "20 65635\n",
      "19 82857\n",
      "18 103430\n",
      "17 128789\n",
      "16 157707\n",
      "15 188687\n",
      "14 223796\n",
      "13 256927\n",
      "12 311452\n",
      "11 349924\n",
      "10 432975\n",
      "9 483015\n",
      "8 630505\n",
      "7 756317\n",
      "6 1086027\n",
      "5 1259530\n",
      "4 1964329\n",
      "3 1971248\n",
      "2 3613787\n",
      "1 32600214\n"
     ]
    }
   ],
   "source": [
    "# previous runs\n",
    "for k, v in sorted(dic.items(), key=lambda x: x[1]):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_utils\n",
    "from collections import Counter\n",
    "import numbers\n",
    "import time\n",
    "import torch\n",
    "import numbers\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import text_classification\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "import metrics\n",
    "import numpy as np\n",
    "\n",
    "def build_vocab(df, feat_colnames, min_freq=1, specials=['<pad>', '<unk>'], specials_first=True):\n",
    "    '''\n",
    "    Create a vocabulary to be used with Script2. This maps all events to an index, including <pad> and <unk> and\n",
    "    nan, which represents padding of sentences, unknown events, and no events respectively\n",
    "    '''\n",
    "    def build_counter(df, feat_colnames):\n",
    "        counter = Counter()\n",
    "        words = df[feat_colnames].values.ravel('K')\n",
    "        print(\"start word number: \", words.shape)\n",
    "        new_words = []\n",
    "        for x in words:\n",
    "            x = str(x)\n",
    "            x = x.replace(\"d_s\", \"d_\")\n",
    "            new_words.extend(x.split(', '))\n",
    "        print(\"exact word number: \" , len(new_words))\n",
    "        counter.update(new_words)\n",
    "        if not isinstance(min_freq, numbers.Number):\n",
    "            raise ValueError(f'Something wrong with {min_freq}')\n",
    "        return counter\n",
    "    \n",
    "\n",
    "    pos_df = df[df['unplanned_readmission'] == True]\n",
    "    pos_counter = build_counter(pos_df, feat_colnames)\n",
    "    \n",
    "    print(pos_counter['nan'])\n",
    "    \n",
    "    vocab = Vocab(pos_counter, min_freq=min_freq, specials=specials, specials_first=specials_first)\n",
    "\n",
    "    print('Completed vocabulary')\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start word number:  (7021890,)\n",
      "exact word number:  19767950\n",
      "4363818\n",
      "Completed vocabulary\n",
      "22528\n"
     ]
    }
   ],
   "source": [
    "pos_vocab = build_vocab(np_re, x_lst[-30:])\n",
    "print(len(pos_vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start word number:  (7022430,)\n",
      "exact word number:  19771134\n",
      "4364148\n",
      "Completed vocabulary\n",
      "22529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nstart word number:  (7022430,)\\nexact word number:  19771134\\n4364148\\nCompleted vocabulary\\n29319\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous runs\n",
    "pos_vocab = build_vocab(np_re, x_lst[-30:])\n",
    "print(len(pos_vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check \n",
    "pos_vocab['nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary for next script\n",
    "torch.save(pos_vocab, 'data_d/pos_vocab_last30_non3digit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test validation splits with stratified targets with 365 dataset\n",
    "\n",
    "Use 20% of data for testing, and the remaining (90%:10%) for training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np_re[x_lst], np_re[y_lst],\n",
    "    test_size=0.2,\n",
    "    random_state=9999,\n",
    "    stratify=np_re[y_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250871, 366)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=9999,\n",
    "    stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125783, 366) (125088, 366) (312718, 366)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('./data/X_train.csv')\n",
    "X_val.to_csv('./data/X_val.csv')\n",
    "X_test.to_csv('./data/X_test.csv')\n",
    "\n",
    "y_train.to_csv('./data/y_train.csv')\n",
    "y_val.to_csv('./data/y_val.csv')\n",
    "y_test.to_csv('./data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write to S3 for storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/y_test.csv to s3://cmsai-mrk-amzn/xianzeng/y_test.csv\n",
      "upload: data/X_test.csv to s3://cmsai-mrk-amzn/xianzeng/X_test.csv  \n",
      "upload: data/y_val.csv to s3://cmsai-mrk-amzn/xianzeng/y_val.csv \n",
      "upload: data/X_val.csv to s3://cmsai-mrk-amzn/xianzeng/X_val.csv    \n",
      "upload: data/y_train.csv to s3://cmsai-mrk-amzn/xianzeng/y_train.csv\n",
      "upload: data/X_train.csv to s3://cmsai-mrk-amzn/xianzeng/X_train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/y_test.csv s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/X_test.csv s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/y_val.csv s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/X_val.csv s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/y_train.csv s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/X_train.csv s3://cmsai-mrk-amzn/xianzeng/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test validation splits with stratified targets with 30 day (max 30 events per day) dataset\n",
    "\n",
    "Equivalent mostly to the 1000 events dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_re = torch.load('data/np_re_non3digit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_re = np_re[['patient_id', 'discharge_id'] + x_lst[-30:] + y_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    np_re,\n",
    "    test_size=0.2,\n",
    "    random_state=9999,\n",
    "    stratify=np_re[y_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=9999,\n",
    "    stratify=train_df[y_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125783, 33) (125088, 33) (312718, 33)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "import torch\n",
    "pos_vocab = torch.load('data/pos_vocab_last30_non3digit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check an event\n",
    "pos_vocab.stoi['nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<unk>\n",
      "nan\n",
      "h_99232\n",
      "h_99233\n",
      "h_71010\n",
      "admission\n",
      "discharge\n",
      "h_93010\n",
      "h_99231\n",
      "h_99223\n",
      "h_A0425\n",
      "d_4280\n",
      "d_486\n",
      "h_99285\n",
      "d_42731\n",
      "d_5856\n",
      "h_71020\n",
      "d_5849\n",
      "h_99222\n",
      "d_496\n",
      "d_78650\n",
      "d_78605\n",
      "d_4019\n",
      "d_51881\n",
      "d_5990\n",
      "h_99238\n",
      "h_99213\n",
      "h_99214\n",
      "d_25000\n",
      "h_36415\n"
     ]
    }
   ],
   "source": [
    "# quick check on vocabulary\n",
    "count = 0\n",
    "for key in pos_vocab.stoi.keys():\n",
    "    count +=1\n",
    "    print(key)\n",
    "    if count > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset and save for script 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "def build_dataset(df, vocab, feat_colnames, label_colnames, day_length=30, max_length=50, mode='eval', keep_length=6):\n",
    "    '''\n",
    "    Subsets the entire dataset into a dataset to be used later.\n",
    "    - Specific vocabulary\n",
    "    - By number of days (whole dataset is 365)\n",
    "    \n",
    "    Returns a list of data and attributes needed by Script 2: patientid_dischargeid key, sequence of events, targets, and \n",
    "    mask (identifying padded regions)\n",
    "    '''\n",
    "    # create dataset\n",
    "    start_time = time.time()\n",
    "    print(\"used days: \", feat_colnames[-day_length], feat_colnames[-1])\n",
    "    \n",
    "    data = df[feat_colnames[-day_length:]].to_numpy()\n",
    "    \n",
    "    sequence = []\n",
    "    count = 0\n",
    "    pad_mask = []\n",
    "    print(\"total size before: \", data.shape)\n",
    "    \n",
    "    # convert all events into indexes from vocab\n",
    "    for i in range(len(data)):\n",
    "        sentence = []\n",
    "        mask = []\n",
    "        \n",
    "        for j in range(len(data[i])):\n",
    "            words = str(data[i][j]).replace('d_s', 'd_').split(', ')\n",
    "            words = sorted([vocab.stoi[w] if w in vocab.stoi else vocab.stoi['<unk>'] for w in words])\n",
    "            \n",
    "            if len(words) > max_length:\n",
    "                words = words[:max_length]\n",
    "            \n",
    "            words = words + [vocab.stoi['<pad>']] * (max_length - len(words))\n",
    "            sentence.append(words)\n",
    "            mask.append([0.0 if w == vocab.stoi['<pad>'] else 1.0 for w in words])\n",
    "            \n",
    "        sequence.append(sentence)\n",
    "        pad_mask.append(mask)\n",
    "        \n",
    "    finish_time = time.time()\n",
    "    print(\"time: \", finish_time - start_time)\n",
    "    print('New dataset created')\n",
    "    print(\"random samples: \", count)\n",
    "    print(\"sequence length: \", len(sequence))\n",
    "    \n",
    "    _, indices = np.unique(sequence, axis=0, return_index=True)\n",
    "    indices = np.sort(indices)\n",
    "    labels = df[label_colnames].to_numpy()[indices]\n",
    "    discharge_ids = df['discharge_id'].to_numpy()[indices]\n",
    "    pad_mask = np.array(pad_mask)[indices]\n",
    "    \n",
    "    try:\n",
    "        sequence = np.array(sequence)[indices]\n",
    "        print(\"unique sequence length: \", len(sequence))\n",
    "        assert labels.shape[0] == sequence.shape[0]\n",
    "    except:\n",
    "        print(\"more processing for sequence\")\n",
    "        pass\n",
    "    \n",
    "    return [discharge_ids, sequence, labels, pad_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used days:  29 0\n",
      "total size before:  (125088, 30)\n",
      "time:  42.632025718688965\n",
      "New dataset created\n",
      "random samples:  0\n",
      "sequence length:  125088\n",
      "unique sequence length:  125024\n"
     ]
    }
   ],
   "source": [
    "#val_dataset = build_dataset(val_df, relative_pos_vocab, x_lst, y_lst, day_length=30, max_length=50)\n",
    "val_dataset = build_dataset(val_df, pos_vocab, x_lst, y_lst, day_length=30, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(val_dataset, open(\"data/val_dataset_last30_non3digit_latest.pkl\", 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used days:  29 0\n",
      "total size before:  (312718, 30)\n",
      "time:  138.9652864933014\n",
      "New dataset created\n",
      "random samples:  0\n",
      "sequence length:  312718\n",
      "unique sequence length:  312542\n"
     ]
    }
   ],
   "source": [
    "#test_dataset = build_dataset(test_df, relative_pos_vocab, x_lst, y_lst, day_length=30, max_length=50)\n",
    "test_dataset = build_dataset(test_df, pos_vocab, x_lst, y_lst, day_length=30, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_dataset, open(\"data/test_dataset_last30_non3digit_latest.pkl\", 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used days:  29 0\n",
      "total size before:  (1125783, 30)\n",
      "time:  357.6096074581146\n",
      "New dataset created\n",
      "random samples:  0\n",
      "sequence length:  1125783\n",
      "unique sequence length:  1124923\n"
     ]
    }
   ],
   "source": [
    "train_dataset = None\n",
    "#train_dataset = build_dataset(train_df, relative_pos_vocab, x_lst, y_lst, day_length=30, max_length=50, mode='train', keep_length=10)\n",
    "train_dataset = build_dataset(train_df, pos_vocab, x_lst, y_lst, day_length=30, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_dataset, open(\"data/train_dataset_last30_non3digit_latest.pkl\", 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used days:  29 0\n",
      "total size before:  (1563588, 30)\n",
      "time:  516.6843750476837\n",
      "New dataset created\n",
      "random samples:  0\n",
      "sequence length:  1563588\n",
      "unique sequence length:  1562234\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = build_dataset(np_re, pos_vocab, x_lst, y_lst, day_length=30, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used days:  29 0\n",
      "total size before:  (1563578, 30)\n",
      "time:  481.286744594574\n",
      "New dataset created\n",
      "random samples:  0\n",
      "sequence length:  1563578\n",
      "unique sequence length:  1562223\n"
     ]
    }
   ],
   "source": [
    "# previous run\n",
    "whole_dataset = build_dataset(np_re, pos_vocab, x_lst, y_lst, day_length=30, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(whole_dataset, open(\"data_d/np_re_last30_non3digit_latest.pkl\", 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double check outputs manually to make sure mapping was correct between original and after indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "[[   2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  52   79  194  233  414  627  762  856 1092 1249 1490 7910    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  52   79  194    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(whole_dataset[3][0][:5])\n",
    "print(whole_dataset[1][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id                                                       100000559\n",
       "discharge_id                                            100000559_20111006\n",
       "29                                                                     nan\n",
       "28                       d_V5869, d_V5883, h_80186, h_82150, h_82247, h...\n",
       "27                                                                     nan\n",
       "26                                               h_90999, h_J1270, h_Q4081\n",
       "25                                                                     nan\n",
       "24                                                                     nan\n",
       "23                             h_90999, h_G0008, h_J1270, h_Q2037, h_Q4081\n",
       "22                                                                     nan\n",
       "21                       d_V5861, h_85610, h_90999, h_J1270, h_J1756, h...\n",
       "20                                                                     nan\n",
       "19                                               h_90999, h_J1270, h_Q4081\n",
       "18                                                                     nan\n",
       "17                                                                     nan\n",
       "16                                                                     nan\n",
       "15                                                                     nan\n",
       "14                                      h_90999, h_J1270, h_J1756, h_Q4081\n",
       "13                                                                     nan\n",
       "12                                               h_90999, h_J1270, h_Q4081\n",
       "11                                                                     nan\n",
       "10                                                                     nan\n",
       "9                                                h_90999, h_J1270, h_Q4081\n",
       "8                                                                      nan\n",
       "7                        admission, d_40391, d_5768, d_5856, d_71945, d...\n",
       "6                        d_5856, d_71947, d_73390, d_8056, h_73610, h_9...\n",
       "5                        d_5856, d_58881, d_80660, h_90935, h_90961, h_...\n",
       "4                                                                      nan\n",
       "3                                                          d_8056, h_99232\n",
       "2                        d_27541, d_5856, d_58881, d_80660, h_90935, h_...\n",
       "1                                                                      nan\n",
       "0                                     admission, d_5856, d_8056, discharge\n",
       "unplanned_readmission                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_re.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h_Q4081'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab.itos[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
