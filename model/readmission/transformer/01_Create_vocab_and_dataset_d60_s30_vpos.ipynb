{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabulary and dataset for readmissions (all folds)\n",
    "\n",
    "**Author: Lin Lee Cheong <br> Last updated: 11/1\n",
    "/20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook to create vocabulary and dataset for readmissions for all 5 folds**\n",
    "- training data & vocabulary\n",
    "- test data\n",
    "\n",
    "**Required:**\n",
    "- input file: raw_train_data.csv, raw_test_data.csv\n",
    "- outputs: pickle file of datalist (whole_ids, whole_data, whole_labels, whole_mask)\n",
    "\n",
    "**Nomenclature:**\n",
    "- d30: **60** days\n",
    "- s30: max 30 sequence a day\n",
    "- vpos: vocabulary positive only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import get_cuda\n",
    "from data_proc import read_data, remove_death, build_vocab\n",
    "from dataset_func import build_dataset, BuildDataset, get_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input filepaths for training, test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "num_folds = 5\n",
    "\n",
    "main_dir = \"../../../data/readmission/\"\n",
    "train_dirs = [os.path.join(main_dir, f\"fold_{idx}/train/\") for idx in range(num_folds)]\n",
    "test_dirs = [os.path.join(main_dir, f\"fold_{idx}/test/\") for idx in range(num_folds)]\n",
    "vocab_dirs = [os.path.join(main_dir, f\"fold_{idx}/vocab/\") for idx in range(num_folds)]\n",
    "\n",
    "train_fps = [os.path.join(train_dir, \"raw_train_data.csv\") for train_dir in train_dirs]\n",
    "test_fps = [os.path.join(test_dir, \"raw_test_data.csv\") for test_dir in test_dirs]\n",
    "\n",
    "train_dl_fps = [os.path.join(train_dir, \"train_datalist_d60_s30_vpos.pkl\") for train_dir in train_dirs]\n",
    "test_dl_fps = [os.path.join(test_dir, \"test_datalist_d60_s30_vpos.pkl\") for test_dir in test_dirs]\n",
    "\n",
    "for vocab_dir in vocab_dirs:    \n",
    "    if not os.path.isdir(vocab_dir):\n",
    "        os.makedirs(vocab_dir)\n",
    "vocab_fps = [os.path.join(vocab_dir, f\"vocab_d60_s30_vpos\") for vocab_dir in vocab_dirs]\n",
    "\n",
    "# Options\n",
    "x_lst = [str(x) for x in range(365, -1, -1)]\n",
    "n_days = 60\n",
    "seq_per_day = 30\n",
    "y_target = \"unplanned_readmission\"\n",
    "uid = \"discharge_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing and DataLoader creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_fp, x_lst, day_length, seq_length, y_target, uid, train=True, vocab_fp=None, datalist_fp=None):\n",
    "    # read in raw dataset, remove deaths\n",
    "    raw_df = read_data(\n",
    "        data_fp=data_fp,\n",
    "        check=True,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        test=0\n",
    "    )\n",
    "    raw_df = remove_death(raw_df, y_target, x_lst)    \n",
    "\n",
    "    # build vocabulary and save if training dataset\n",
    "    if train:\n",
    "        print('Vocab generation required')\n",
    "        vocab = build_vocab(raw_df, x_lst[-day_length:])\n",
    "        if vocab_fp is not None:\n",
    "            torch.save(vocab, vocab_fp)\n",
    "    else:\n",
    "        vocab = torch.load(vocab_fp)\n",
    "        \n",
    "    # build dataset and save\n",
    "    whole_data = build_dataset(\n",
    "        raw_df, vocab, x_lst, [y_target], day_length=day_length, max_length=seq_length\n",
    "    )\n",
    "\n",
    "    if datalist_fp is not None:\n",
    "        pickle.dump(whole_data, open(datalist_fp, 'wb'), protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44492 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (11235060,)\n",
      "exact word number:  25062090\n",
      "Completed vocabulary\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (1250834, 60)\n",
      "New dataset created\n",
      "Sequence length:  1250834\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_0/vocab/vocab_d60_s30_vpos, \n",
      " train data:../../../data/readmission/fold_0/train/train_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44286 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (11235000,)\n",
      "exact word number:  25032459\n",
      "Completed vocabulary\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (1251040, 60)\n",
      "New dataset created\n",
      "Sequence length:  1251040\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_1/vocab/vocab_d60_s30_vpos, \n",
      " train data:../../../data/readmission/fold_1/train/train_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295326, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855436\n",
      "True     0.144564\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44601 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (11235060,)\n",
      "exact word number:  25033202\n",
      "Completed vocabulary\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (1250725, 60)\n",
      "New dataset created\n",
      "Sequence length:  1250725\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_2/vocab/vocab_d60_s30_vpos, \n",
      " train data:../../../data/readmission/fold_2/train/train_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295327, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855435\n",
      "True     0.144565\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 1\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44327 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (11235060,)\n",
      "exact word number:  25055026\n",
      "Completed vocabulary\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (1251000, 60)\n",
      "New dataset created\n",
      "Sequence length:  1251000\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_3/vocab/vocab_d60_s30_vpos, \n",
      " train data:../../../data/readmission/fold_3/train/train_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/train/raw_train_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (1295327, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855435\n",
      "True     0.144565\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 44570 rows contain the word death\n",
      "Vocab generation required\n",
      "\n",
      "====================Build vocabulary====================\n",
      "\n",
      "start word number:  (11234940,)\n",
      "exact word number:  25054419\n",
      "Completed vocabulary\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (1250757, 60)\n",
      "New dataset created\n",
      "Sequence length:  1250757\n",
      "Completed, wrote to vocab: ../../../data/readmission/fold_4/vocab/vocab_d60_s30_vpos, \n",
      " train data:../../../data/readmission/fold_4/train/train_datalist_d60_s30_vpos.pkl\n"
     ]
    }
   ],
   "source": [
    "for idx, (train_fp, train_dl_fp, vocab_fp) in enumerate(zip(train_fps, train_dl_fps, vocab_fps)):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "    \n",
    "    create_dataset(\n",
    "        data_fp=train_fp,\n",
    "        x_lst=x_lst,\n",
    "        day_length=n_days,\n",
    "        seq_length=seq_per_day,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        train=True,\n",
    "        vocab_fp=vocab_fp,\n",
    "        datalist_fp=train_dl_fp,\n",
    "    )\n",
    "    \n",
    "    print(f\"Completed, wrote to vocab: {vocab_fp}, \\n train data:{train_dl_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 0\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_0/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11077 rows contain the word death\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (312755, 60)\n",
      "New dataset created\n",
      "Sequence length:  312755\n",
      "Completed, read from vocab: ../../../data/readmission/fold_0/vocab/vocab_d60_s30_vpos, \n",
      " wrote to ../../../data/readmission/fold_0/test/test_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 1\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_1/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11283 rows contain the word death\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (312549, 60)\n",
      "New dataset created\n",
      "Sequence length:  312549\n",
      "Completed, read from vocab: ../../../data/readmission/fold_1/vocab/vocab_d60_s30_vpos, \n",
      " wrote to ../../../data/readmission/fold_1/test/test_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 2\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_2/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323832, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855434\n",
      "True     0.144566\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 10968 rows contain the word death\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (312864, 60)\n",
      "New dataset created\n",
      "Sequence length:  312864\n",
      "Completed, read from vocab: ../../../data/readmission/fold_2/vocab/vocab_d60_s30_vpos, \n",
      " wrote to ../../../data/readmission/fold_2/test/test_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 3\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_3/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323831, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855437\n",
      "True     0.144563\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 11242 rows contain the word death\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (312589, 60)\n",
      "New dataset created\n",
      "Sequence length:  312589\n",
      "Completed, read from vocab: ../../../data/readmission/fold_3/vocab/vocab_d60_s30_vpos, \n",
      " wrote to ../../../data/readmission/fold_3/test/test_datalist_d60_s30_vpos.pkl\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "Processing fold 4\n",
      "****************************************************************************************************\n",
      "Read data from ../../../data/readmission/fold_4/test/raw_test_data.csv\n",
      "\n",
      "====================Checking data====================\n",
      "\n",
      "Data size: (323831, 370)\n",
      "\n",
      "Label ratio for unplanned_readmission\n",
      "False    0.855437\n",
      "True     0.144563\n",
      "Name: unplanned_readmission, dtype: float64\n",
      "\n",
      "Discharge_id duplicates: 0\n",
      "\n",
      "====================Removing bad word data====================\n",
      "\n",
      "Removing bad words: 10999 rows contain the word death\n",
      "====================New dataset created====================\n",
      "\n",
      "Used days:  59 0\n",
      "Total size before building dataset:  (312832, 60)\n",
      "New dataset created\n",
      "Sequence length:  312832\n",
      "Completed, read from vocab: ../../../data/readmission/fold_4/vocab/vocab_d60_s30_vpos, \n",
      " wrote to ../../../data/readmission/fold_4/test/test_datalist_d60_s30_vpos.pkl\n"
     ]
    }
   ],
   "source": [
    "for idx, (test_fp, test_dl_fp, vocab_fp) in enumerate(\n",
    "    zip(test_fps, test_dl_fps, vocab_fps)\n",
    "):\n",
    "    print(\"\\n\\n\" + \"*\" * 100)\n",
    "    print(f\"Processing fold {idx}\\n\" + \"*\" * 100)\n",
    "    create_dataset(\n",
    "        data_fp=test_fp,\n",
    "        x_lst=x_lst,\n",
    "        day_length=n_days,\n",
    "        seq_length=seq_per_day,\n",
    "        y_target=y_target,\n",
    "        uid=uid,\n",
    "        train=False,\n",
    "        vocab_fp=vocab_fp,\n",
    "        datalist_fp=test_dl_fp,\n",
    "    )\n",
    "\n",
    "    print(f\"Completed, read from vocab: {vocab_fp}, \\n wrote to {test_dl_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
