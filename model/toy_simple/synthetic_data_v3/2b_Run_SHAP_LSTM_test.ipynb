{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SHAP on all test results for best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import newaxis as na\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n",
    "from lrp_att_model import *\n",
    "import shap_jacc_utils as sj_utils\n",
    "\n",
    "import rbo\n",
    "\n",
    "MODEL_NAME = \"lstm-att-lrp\"\n",
    "\n",
    "NROWS = 1e9\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SEQ_LEN = 30\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = f\"../../../data/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/train.csv\"\n",
    "VALID_DATA_PATH = f\"../../../data/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/val.csv\"\n",
    "TEST_DATA_PATH = f\"../../../data/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/test.csv\"\n",
    "SELECTED_EXAMPLES_PATH = f\"../../data/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/visualized_test_patients.txt\"\n",
    "VOCAB_PATH = f\"../../../data/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/vocab.pkl\"\n",
    "\n",
    "MODEL_SAVE_PATH_PATTERN = (\n",
    "    f\"./output/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/{MODEL_NAME}/model_weights/model_{'{}'}.pkl\"\n",
    ")\n",
    "SHAP_SAVE_DIR_PATTERN = f\"./output/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/{MODEL_NAME}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split\n",
    "\n",
    "OUTPUT_RESULTS_PATH = (\n",
    "    f\"output/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/{MODEL_NAME}/train_results/results.csv\"\n",
    ")\n",
    "PARAMS_PATH = (\n",
    "    f\"output/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/{MODEL_NAME}/train_results/model_params.json\"\n",
    ")\n",
    "\n",
    "N_EPOCHS = 6\n",
    "\n",
    "TARGET_COLNAME = \"label\"\n",
    "UID_COLNAME = \"patient_id\"\n",
    "TARGET_VALUE = \"1\"\n",
    "\n",
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "def get_rbo(x, y, uid, p=0.7):\n",
    "    x_idx = np.argsort(x)[::-1]\n",
    "    y_idx = np.argsort(y)[::-1]\n",
    "\n",
    "    return rbo.RankingSimilarity(\n",
    "        [uid[idx] for idx in x_idx], [uid[idx] for idx in y_idx]\n",
    "    ).rbo(p=p)\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x):\n",
    "    if \"_N\" in x:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class AttNoHtLSTM(SimpleLSTM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        hidden_dim,\n",
    "        vocab,\n",
    "        device,\n",
    "        nlayers=1,\n",
    "        bidi=True,\n",
    "        use_gpu=True,\n",
    "        pad_idx=0,\n",
    "        dropout=None,\n",
    "        init_type=\"zero\",\n",
    "        linear_bias=True,\n",
    "    ):\n",
    "        super(AttNoHtLSTM, self).__init__(\n",
    "            emb_dim=emb_dim, hidden_dim=hidden_dim, vocab=vocab, device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.emb_layer = nn.Embedding(self.input_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidi = bidi\n",
    "        self.nlayers = nlayers\n",
    "        self.linear_bias = linear_bias\n",
    "\n",
    "        \"\"\"\n",
    "        self.attn_layer = (\n",
    "            nn.Linear(hidden_dim *2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        if dropout is None:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "        self.pred_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias)\n",
    "            if bidi\n",
    "            else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "\n",
    "        self.dpt = nn.Dropout(dropout)\n",
    "\n",
    "        \"\"\"\n",
    "        self.context_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, tokens, ret_attn=False):\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            embedded = self.dpt(self.emb_layer(tokens))\n",
    "        else:\n",
    "            embedded = self.emb_layer(tokens)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = True\n",
    "            self.c0.requires_grad = True\n",
    "            hidden = (\n",
    "                self.h0.repeat(1, tokens.shape[0], 1),\n",
    "                self.c0.repeat(1, tokens.shape[0], 1),\n",
    "            )\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(tokens.shape[0])\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        text_lengths = torch.sum(tokens != self.pad_idx, dim=1).to(\"cpu\")\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "\n",
    "        packed_output, (final_hidden, cell) = self.lstm(packed_embedded, hidden)\n",
    "\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=tokens.shape[1]\n",
    "        )\n",
    "\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "\n",
    "        # Switch to multiplicative attention\n",
    "        mask_feats = np.array(tokens.cpu().numpy() == 0)\n",
    "        mask_feats = -1000 * mask_feats.astype(np.int)\n",
    "\n",
    "        mask_feats = torch.Tensor(mask_feats).to(self.device)\n",
    "\n",
    "        attn_weights_int = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            (tokens.shape[1]) ** 0.5\n",
    "        )\n",
    "        attn_weights = nn.functional.softmax(attn_weights_int + mask_feats, -1)\n",
    "\n",
    "        context = torch.bmm(output.transpose(1, 2), attn_weights.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )\n",
    "\n",
    "        concat_out = context\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            pred = self.pred_layer(self.dpt(concat_out))\n",
    "        else:\n",
    "            pred = self.pred_layer(concat_out)\n",
    "\n",
    "        if ret_attn:\n",
    "            return (\n",
    "                pred.detach().cpu().numpy(),\n",
    "                attn_weights.detach().cpu().numpy(),\n",
    "                context.detach().cpu().numpy(),\n",
    "                attn_weights_int.detach().cpu().numpy(),\n",
    "                out.detach().cpu().numpy(),\n",
    "                output.detach().cpu().numpy(),\n",
    "            )\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward_shap(self, token_ids, mask, full_id_matrix=False):\n",
    "        token_ids = token_ids if token_ids.is_cuda else token_ids.to(self.device)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = False\n",
    "            self.c0.requires_grad = False\n",
    "\n",
    "            hidden = (self.h0.repeat(1, 1, 1), self.c0.repeat(1, 1, 1))\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(1)\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        token_ids[sum(mask) :, :] = 0\n",
    "        embedded = torch.matmul(token_ids, self.emb_layer.weight).unsqueeze(0)\n",
    "\n",
    "        embedded = embedded[:, : sum(mask), :]\n",
    "\n",
    "        output, _ = self.lstm(embedded, hidden)\n",
    "\n",
    "        # output = output.permute(1, 0, 2)  # [batch, text_length, hidden_dim]\n",
    "        # print(f'Output dimensions: {output.shape}')\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        # print(f'Stacked hidden dimensions: {stacked_hidden.shape}')\n",
    "        # print(f'mask weight dimensions: {mask_feats.shape}')\n",
    "        # attention = self.context_layer(output).squeeze(-1)\n",
    "        # att_weights = nn.functional.softmax(attention, dim=-1)\n",
    "        # context = torch.bmm(att_weights.unsqueeze(1), output).squeeze(1)\n",
    "        attn_weights = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            sum(mask) ** 0.5\n",
    "        )\n",
    "\n",
    "        soft_attn_weights = nn.functional.softmax(attn_weights, 1)\n",
    "\n",
    "        context = torch.bmm(\n",
    "            output.transpose(1, 2), soft_attn_weights.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # concat_out = torch.cat((context, out), dim=1)\n",
    "        concat_out = context\n",
    "        pred = self.pred_layer(concat_out)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class LSTM_LRP_MultiLayer:\n",
    "    def __init__(self, pymodel):\n",
    "        super(LSTM_LRP_MultiLayer, self).__init__()\n",
    "\n",
    "        self.init_model(pymodel)\n",
    "\n",
    "    def init_model(self, pymodel):\n",
    "\n",
    "        self.device = pymodel.device\n",
    "        self.use_gpu = pymodel.use_gpu\n",
    "        self.bidi = pymodel.bidi\n",
    "\n",
    "        self.emb_dim = pymodel.emb_dim\n",
    "        self.vocab = pymodel.vocab\n",
    "        self.input_dim = len(self.vocab)\n",
    "        self.pad_idx = pymodel.pad_idx\n",
    "        self.hidden_dim = pymodel.hidden_dim\n",
    "\n",
    "        self.emb = pymodel.emb_layer.weight.detach().numpy()\n",
    "\n",
    "        param_list = list(pymodel.lstm.named_parameters())\n",
    "        param_dict = {}\n",
    "        for param_tuple in param_list:\n",
    "            param_dict[param_tuple[0]] = param_tuple[-1].detach().numpy()\n",
    "\n",
    "        # rearrange, pytorch uses ifgo format, need to move to icfo/igfo format\n",
    "        idx_list = (\n",
    "            list(range(0, self.hidden_dim))\n",
    "            + list(range(self.hidden_dim * 2, self.hidden_dim * 3))\n",
    "            + list(range(self.hidden_dim, self.hidden_dim * 2))\n",
    "            + list(range(self.hidden_dim * 3, self.hidden_dim * 4))\n",
    "        )\n",
    "        self.nlayers = pymodel.nlayers\n",
    "\n",
    "        # i (input), g (candidate), f (forget), o (output) order\n",
    "        # (4 * hidden_dim, emb_dim)\n",
    "        self.Wxh_Left = {}\n",
    "        self.bxh_Left = {}\n",
    "        self.Whh_Left = {}\n",
    "        self.bhh_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wxh_Right = {}\n",
    "            self.bxh_Right = {}\n",
    "            self.Whh_Right = {}\n",
    "            self.bhh_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.Wxh_Left[layer] = param_dict[f\"weight_ih_l{layer}\"][idx_list]\n",
    "            self.bxh_Left[layer] = param_dict[f\"bias_ih_l{layer}\"][idx_list]  # shape 4d\n",
    "            self.Whh_Left[layer] = param_dict[f\"weight_hh_l{layer}\"][\n",
    "                idx_list\n",
    "            ]  # shape 4d*d\n",
    "            self.bhh_Left[layer] = param_dict[f\"bias_hh_l{layer}\"][idx_list]  # shape 4d\n",
    "\n",
    "            if self.bidi:\n",
    "                # LSTM right encoder\n",
    "                self.Wxh_Right[layer] = param_dict[f\"weight_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bxh_Right[layer] = param_dict[f\"bias_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.Whh_Right[layer] = param_dict[f\"weight_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bhh_Right[layer] = param_dict[f\"bias_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "\n",
    "        # START ADDED: CONTEXT LAYER INIT\n",
    "        # linear output layer: shape C * 4d\n",
    "        # 0-d: fwd & context\n",
    "        # d-2d: rev & context\n",
    "        # 2d-3d: fwd & final hidden\n",
    "        # 3d-4d: rev & final hidden\n",
    "        Why = pymodel.pred_layer.weight.detach().numpy()\n",
    "\n",
    "        self.Why_Left = Why[:, 2 * self.hidden_dim : 3 * self.hidden_dim]  # shape C*d\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Why_Right = Why[:, 3 * self.hidden_dim :]  # shape C*d\n",
    "\n",
    "        self.Wcy_Left = Why[:, : self.hidden_dim]\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wcy_Right = Why[:, self.hidden_dim : 2 * self.hidden_dim]\n",
    "        # END ADDED: CONTEXT LAYER INIT\n",
    "\n",
    "    def set_input(self, tokens):\n",
    "        T = len(tokens)  # sequence length\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)  # hidden layer dimension\n",
    "        e = self.emb.shape[1]  # word embedding dimension\n",
    "\n",
    "        self.w = tokens\n",
    "        self.x = {}\n",
    "        self.x_rev = {}\n",
    "        x = np.zeros((T, e))\n",
    "        x[:, :] = self.emb[tokens, :]\n",
    "        self.x[0] = x\n",
    "        self.x_rev[0] = x[::-1, :].copy()\n",
    "        self.h_Left = {}\n",
    "        self.c_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.h_Right = {}\n",
    "            self.c_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.h_Left[layer] = np.zeros((T + 1, d))\n",
    "            self.c_Left[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "            if self.bidi:\n",
    "                self.h_Right[layer] = np.zeros((T + 1, d))\n",
    "                self.c_Right[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "        self.att_score = None\n",
    "\n",
    "    def forward_gate(self, layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir):\n",
    "\n",
    "        if gate_dir == \"left\":\n",
    "            self.gates_xh_Left[layer][t] = np.dot(\n",
    "                self.Wxh_Left[layer], self.x[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Left[layer][t] = np.dot(\n",
    "                self.Whh_Left[layer], self.h_Left[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Left[layer][t] = (\n",
    "                self.gates_xh_Left[layer][t]\n",
    "                + self.gates_hh_Left[layer][t]\n",
    "                + self.bxh_Left[layer]\n",
    "                + self.bhh_Left[layer]\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Left[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Left[layer][t] = (\n",
    "                self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1]\n",
    "                + self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Left[layer][t] = self.gates_Left[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Left[layer][t]\n",
    "            )\n",
    "\n",
    "        if gate_dir == \"right\":\n",
    "            self.gates_xh_Right[layer][t] = np.dot(\n",
    "                self.Wxh_Right[layer], self.x_rev[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Right[layer][t] = np.dot(\n",
    "                self.Whh_Right[layer], self.h_Right[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Right[layer][t] = (\n",
    "                self.gates_xh_Right[layer][t]\n",
    "                + self.gates_hh_Right[layer][t]\n",
    "                + self.bxh_Right[layer]\n",
    "                + self.bhh_Right[layer]\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Right[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Right[layer][t] = (\n",
    "                self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1]\n",
    "                + self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Right[layer][t] = self.gates_Right[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Right[layer][t]\n",
    "            )\n",
    "\n",
    "    def forward_lrp(self):\n",
    "        \"\"\"\n",
    "        Standard forward pass.\n",
    "        Compute the hidden layer values (assuming input x/x_rev was previously set)\n",
    "        \"\"\"\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "\n",
    "        # gate indices (assuming the gate ordering in the LSTM weights is i,g,f,o):\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        self.gates_xh_Left = {}\n",
    "        self.gates_hh_Left = {}\n",
    "        self.gates_pre_Left = {}\n",
    "        self.gates_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.gates_xh_Right = {}\n",
    "            self.gates_hh_Right = {}\n",
    "            self.gates_pre_Right = {}\n",
    "            self.gates_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.gates_xh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_hh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_pre_Left[layer] = np.zeros((T, 4 * d))  # gates pre-activation\n",
    "            self.gates_Left[layer] = np.zeros((T, 4 * d))  # gates activation\n",
    "\n",
    "            if self.bidi:\n",
    "                self.gates_xh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_hh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_pre_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_Right[layer] = np.zeros((T, 4 * d))\n",
    "\n",
    "        # START ADDED: INITIALIZE CONTEXT LAYERS\n",
    "        self.ctxt_Left = np.zeros((1, d))\n",
    "        self.ctxt_Right = np.zeros((1, d))\n",
    "        self.att_wgt_Left = np.zeros((T, 1))\n",
    "        self.att_wgt_Right = np.zeros((T, 1))\n",
    "        self.att_score = np.zeros((T, 1))\n",
    "\n",
    "        # END ADDED: INITIALIZE CONTEXT LAYERS\n",
    "\n",
    "        # START EDIT: cycle through first layer first\n",
    "        layer = 0\n",
    "        for t in range(T):\n",
    "            self.forward_gate(\n",
    "                layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "            )\n",
    "            if self.bidi:\n",
    "                self.forward_gate(\n",
    "                    layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                )\n",
    "\n",
    "        # go through all the rest of the layers\n",
    "        if self.nlayers > 1:\n",
    "            ## TODO: fix init t-1 (zero time step) Zeroes!!\n",
    "            self.x[layer + 1] = (\n",
    "                np.concatenate(\n",
    "                    (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                )\n",
    "                if self.bidi\n",
    "                else self.h_Left[layer][:T]\n",
    "            )\n",
    "\n",
    "            self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "            for layer in range(1, self.nlayers):\n",
    "                for t in range(T):\n",
    "                    self.forward_gate(\n",
    "                        layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "                    )\n",
    "                    if self.bidi:\n",
    "                        self.forward_gate(\n",
    "                            layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                        )\n",
    "\n",
    "                    self.x[layer + 1] = np.concatenate(\n",
    "                        (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                    )\n",
    "                    self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "        # calculate attention layer & context layer\n",
    "        top_layer = self.nlayers - 1\n",
    "        self.att_wgt_Left = np.dot(\n",
    "            self.h_Left[top_layer][:T, :], self.h_Left[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_wgt_Right = np.dot(\n",
    "            self.h_Right[top_layer][:T, :], self.h_Right[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_score = self.stable_softmax(\n",
    "            (self.att_wgt_Left + self.att_wgt_Right) / (T ** 0.5)\n",
    "        )\n",
    "\n",
    "        self.ctxt_Left = (self.att_score[:, na] * self.h_Left[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "        self.ctxt_Right = (self.att_score[:, na] * self.h_Right[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # CALCULATE WITH CONTEXT & OUT, NOT JUST HIDDEN\n",
    "        # self.y_Left = np.dot(self.Why_Left, self.h_Left[top_layer][T - 1])\n",
    "        self.y_Left = np.dot(self.Wcy_Left, self.ctxt_Left)\n",
    "\n",
    "        # self.y_Right = np.dot(self.Why_Right, self.h_Right[top_layer][T - 1])\n",
    "        self.y_Right = np.dot(self.Wcy_Right, self.ctxt_Right)\n",
    "\n",
    "        self.s = self.y_Left + self.y_Right\n",
    "\n",
    "        return self.s.copy()  # prediction scores\n",
    "\n",
    "    def stable_softmax(self, x):\n",
    "        z = x - np.max(x)\n",
    "        num = np.exp(z)\n",
    "        denom = np.sum(num)\n",
    "        softmax_vals = num / denom\n",
    "\n",
    "        return softmax_vals\n",
    "\n",
    "    def lrp_left_gate(\n",
    "        self,\n",
    "        Rc_Left,\n",
    "        Rh_Left,\n",
    "        Rg_Left,\n",
    "        Rx,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "\n",
    "        # import IPython\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        Rc_Left[layer][t] += Rh_Left[layer][t]\n",
    "        Rc_Left[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rg_Left[layer][t] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx[layer][t] += lrp_linear(\n",
    "            self.x[layer][t],\n",
    "            self.Wxh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Left[layer][t - 1] += lrp_linear(\n",
    "            self.h_Left[layer][t - 1],\n",
    "            self.Whh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Left, Rh_Left, Rg_Left, Rx\n",
    "\n",
    "    def lrp_right_gate(\n",
    "        self,\n",
    "        Rc_Right,\n",
    "        Rh_Right,\n",
    "        Rg_Right,\n",
    "        Rx_rev,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "        Rc_Right[layer][t] += Rh_Right[layer][t]\n",
    "        Rc_Right[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        Rg_Right[layer][t] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx_rev[layer][t] += lrp_linear(\n",
    "            self.x_rev[layer][t],\n",
    "            self.Wxh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Right[layer][t - 1] += lrp_linear(\n",
    "            self.h_Right[layer][t - 1],\n",
    "            self.Whh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Right, Rh_Right, Rg_Right, Rx_rev\n",
    "\n",
    "    def lrp(self, w, LRP_class, eps=0.001, bias_factor=0.0):\n",
    "        \"\"\"\n",
    "        Layer-wise Relevance Propagation (LRP) backward pass.\n",
    "        Compute the hidden layer relevances by performing LRP for the target class LRP_class\n",
    "        (according to the papers:\n",
    "            - https://doi.org/10.1371/journal.pone.0130140\n",
    "            - https://doi.org/10.18653/v1/W17-5221 )\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        self.set_input(w)\n",
    "        self.forward_lrp()\n",
    "\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "        e = self.emb.shape[1]\n",
    "        C = self.Why_Left.shape[0]  # number of classes\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        Rx = {}\n",
    "        Rx_rev = {}\n",
    "        Rx_all = {}\n",
    "\n",
    "        Rh_Left = {}\n",
    "        Rc_Left = {}\n",
    "        Rg_Left = {}  # gate g only\n",
    "\n",
    "        if self.bidi:\n",
    "            Rh_Right = {}\n",
    "            Rc_Right = {}\n",
    "            Rg_Right = {}  # gate g only\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            Rx[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_rev[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_all[layer] = np.zeros(self.x[layer].shape)\n",
    "\n",
    "            Rh_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rc_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rg_Left[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "            if self.bidi:\n",
    "                Rh_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rc_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rg_Right[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "        Rctxt_Left = np.zeros((1, d))\n",
    "        Rctxt_Right = np.zeros((1, d))\n",
    "\n",
    "        Rout_mask = np.zeros((C))\n",
    "        Rout_mask[LRP_class] = 1.0\n",
    "\n",
    "        # process top most layer first\n",
    "        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
    "        layer = self.nlayers - 1\n",
    "        \"\"\"\n",
    "        Rh_Left[layer][T - 1] = lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            self.Why_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] = lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                self.Why_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "        \"\"\"\n",
    "        # ADD CONTEXT CALCULATIONS TO CONTEXT LAYER\n",
    "        Rctxt_Left = lrp_linear(\n",
    "            self.ctxt_Left,\n",
    "            self.Wcy_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rctxt_Right = lrp_linear(\n",
    "                self.ctxt_Right,\n",
    "                self.Wcy_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        # CONTRIBUTION FROM ATTN LAYER\n",
    "        Rh_Left[layer][T - 1] += lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            np.identity((d)),\n",
    "            np.zeros((d)),\n",
    "            self.ctxt_Left,\n",
    "            self.att_score[T - 1] * Rctxt_Left,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] += lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Right,\n",
    "                self.att_score[T - 1] * Rctxt_Right,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        ee = e if self.nlayers == 1 else 2 * d\n",
    "        for t in reversed(range(T)):\n",
    "\n",
    "            Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                Rc_Left,\n",
    "                Rh_Left,\n",
    "                Rg_Left,\n",
    "                Rx,\n",
    "                layer,\n",
    "                t,\n",
    "                d,\n",
    "                ee,\n",
    "                idx,\n",
    "                idx_f,\n",
    "                idx_i,\n",
    "                idx_g,\n",
    "                idx_o,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "            )\n",
    "\n",
    "            # ATTN Relevance scores\n",
    "            Rh_Left[layer][t - 1] += lrp_linear(\n",
    "                self.h_Left[layer][t - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Left,\n",
    "                self.att_score[t - 1] * Rctxt_Left,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "            if self.bidi:\n",
    "                Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                    Rc_Right,\n",
    "                    Rh_Right,\n",
    "                    Rg_Right,\n",
    "                    Rx_rev,\n",
    "                    layer,\n",
    "                    t,\n",
    "                    d,\n",
    "                    ee,\n",
    "                    idx,\n",
    "                    idx_f,\n",
    "                    idx_i,\n",
    "                    idx_g,\n",
    "                    idx_o,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                )\n",
    "                # ATTN Relevance scores for top-most layer\n",
    "                Rh_Right[layer][t - 1] += lrp_linear(\n",
    "                    self.h_Right[layer][t - 1],\n",
    "                    np.identity((d)),\n",
    "                    np.zeros((d)),\n",
    "                    self.ctxt_Right,\n",
    "                    self.att_score[t - 1] * Rctxt_Right,\n",
    "                    4 * d,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                    debug=False,\n",
    "                )\n",
    "\n",
    "        # propagate through remaining layers\n",
    "        if self.nlayers > 1:\n",
    "            remaining_layers = list(range(0, self.nlayers - 1))[::-1]\n",
    "            # print(f\"remaining layers: {remaining_layers}\")\n",
    "\n",
    "            # no more attn layer flow back\n",
    "            for layer in remaining_layers:\n",
    "\n",
    "                # Sum up all the relevances for each of the inputs in sequence\n",
    "                Rx_all[layer + 1] = Rx[layer + 1] + Rx_rev[layer + 1][::-1, :]\n",
    "\n",
    "                ee = e if layer == 0 else 2 * d\n",
    "                for t in reversed(range(T)):\n",
    "                    # Rh_Left[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Left[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Left[layer][t], #self.x[layer+1][t, :d],\n",
    "                    #    Rx_all[layer+1][t, :d],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Left[layer][t] += Rx_all[layer + 1][t, :d]\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                        Rc_Left,\n",
    "                        Rh_Left,\n",
    "                        Rg_Left,\n",
    "                        Rx,\n",
    "                        layer,\n",
    "                        t,\n",
    "                        d,\n",
    "                        ee,\n",
    "                        idx,\n",
    "                        idx_f,\n",
    "                        idx_i,\n",
    "                        idx_g,\n",
    "                        idx_o,\n",
    "                        eps,\n",
    "                        bias_factor,\n",
    "                    )\n",
    "\n",
    "                    ### RIGHT +++++++++\n",
    "                    # Rh_Right[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Right[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Right[layer][t], #self.x_rev[layer+1][::-1, :][t, d:],\n",
    "                    #    Rx_all[layer+1][t, d:],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Right[layer][t] += Rx_all[layer + 1][::-1, :][t, d:]\n",
    "                    if self.bidi:\n",
    "                        Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                            Rc_Right,\n",
    "                            Rh_Right,\n",
    "                            Rg_Right,\n",
    "                            Rx_rev,\n",
    "                            layer,\n",
    "                            t,\n",
    "                            d,\n",
    "                            ee,\n",
    "                            idx,\n",
    "                            idx_f,\n",
    "                            idx_i,\n",
    "                            idx_g,\n",
    "                            idx_o,\n",
    "                            eps,\n",
    "                            bias_factor,\n",
    "                        )\n",
    "\n",
    "        # record\n",
    "        self.Rx_all = Rx_all\n",
    "        self.Rx = Rx\n",
    "        self.Rx_rev = Rx_rev\n",
    "        self.Rh_Left = Rh_Left\n",
    "        self.Rh_Right = Rh_Right\n",
    "        self.Rc_Left = Rc_Left\n",
    "        self.Rc_Right = Rc_Right\n",
    "        self.Rg_Right = Rg_Right\n",
    "        self.d = d\n",
    "        self.ee = ee\n",
    "        self.Rctxt_Left = Rctxt_Left\n",
    "        self.Rctxt_Right = Rctxt_Right\n",
    "\n",
    "        return (\n",
    "            Rx[0],\n",
    "            Rx_rev[0][::-1, :],\n",
    "            Rh_Left[0][-1].sum()\n",
    "            + Rc_Left[0][-1].sum()\n",
    "            + Rh_Right[0][-1].sum()\n",
    "            + Rc_Right[0][-1].sum(),\n",
    "        )\n",
    "\n",
    "    def get_attn_values(self):\n",
    "        return self.att_score\n",
    "\n",
    "\n",
    "def get_sim(idx_model, idx_gt):\n",
    "    return len(set(idx_model).intersection(set(idx_gt))) / len(idx_gt)\n",
    "\n",
    "\n",
    "def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):\n",
    "    \"\"\"\n",
    "    LRP for a linear layer with input dim D and output dim M.\n",
    "    Args:\n",
    "    - hin:            forward pass input, of shape (D,)\n",
    "    - w:              connection weights, of shape (D, M)\n",
    "    - b:              biases, of shape (M,)\n",
    "    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)\n",
    "    - Rout:           relevance at layer output, of shape (M,)\n",
    "    - bias_nb_units:  total number of connected lower-layer units (onto which the bias/stabilizer contribution is redistributed for sanity check)\n",
    "    - eps:            stabilizer (small positive number)\n",
    "    - bias_factor:    set to 1.0 to check global relevance conservation, otherwise use 0.0 to ignore bias/stabilizer redistribution (recommended)\n",
    "    Returns:\n",
    "    - Rin:            relevance at layer input, of shape (D,)\n",
    "    \"\"\"\n",
    "    sign_out = np.where(hout[na, :] >= 0, 1.0, -1.0)  # shape (1, M)\n",
    "\n",
    "    # numer    = (w * hin[:,na]) + ( (bias_factor*b[na,:]*1.) * 1./bias_nb_units )\n",
    "    numer = (w * hin[:, na]) + (\n",
    "        bias_factor * (b[na, :] * 1.0 + eps * sign_out * 1.0) / bias_nb_units\n",
    "    )  # shape (D, M)\n",
    "\n",
    "    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact\n",
    "    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check\n",
    "    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)\n",
    "\n",
    "    denom = hout[na, :] + (eps * sign_out * 1.0)  # shape (1, M)\n",
    "\n",
    "    message = (numer / denom) * Rout[na, :]  # shape (D, M)\n",
    "\n",
    "    Rin = message.sum(axis=1)  # shape (D,)\n",
    "\n",
    "    if debug:\n",
    "        print(\"local diff: \", Rout.sum() - Rin.sum())\n",
    "    # Note:\n",
    "    # - local  layer   relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)\n",
    "    # - global network relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections\n",
    "    # -> can be used for sanity check\n",
    "\n",
    "    return Rin\n",
    "\n",
    "\n",
    "def get_sub_valid_data(n_val_eval, batch_size, valid_dataloader):\n",
    "    \"\"\"Get subset of validation dataset to run SHAP/LRP on\"\"\"\n",
    "\n",
    "    n_loads = int(np.ceil(n_val_eval / batch_size))\n",
    "    counter = 0\n",
    "\n",
    "    for ids, labels, idxed_text in valid_dataloader:\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 1:\n",
    "            sub_val_ids, sub_val_labels, sub_val_idxed_text = ids, labels, idxed_text\n",
    "        else:\n",
    "            sub_val_ids = sub_val_ids + ids\n",
    "            sub_val_labels = torch.cat([sub_val_labels, labels])\n",
    "            sub_val_idxed_text = torch.cat([sub_val_idxed_text, idxed_text])\n",
    "\n",
    "        if counter == n_loads:\n",
    "            break\n",
    "\n",
    "    sub_val_ids = sub_val_ids[:n_val_eval]\n",
    "    sub_val_labels = sub_val_labels[:n_val_eval]\n",
    "    sub_val_idxed_text = sub_val_idxed_text[:n_val_eval]\n",
    "\n",
    "    return (sub_val_ids, sub_val_labels, sub_val_idxed_text)\n",
    "\n",
    "\n",
    "def glfass_single(cpu_model, background, test, seq_len, device):\n",
    "    \"\"\"\n",
    "    Single-thread function for Get Lstm Features And Shap Scores\n",
    "    Called by get_lstm_features_and_shap_scores_mp\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = cpu_model.to(device)\n",
    "\n",
    "    try:\n",
    "\n",
    "        background_ids, background_labels, background_idxes = background\n",
    "        bg_data, bg_masks = model.get_all_ids_masks(background_idxes, seq_len)\n",
    "\n",
    "        explainer = deep_id_pytorch.CustomPyTorchDeepIDExplainer(\n",
    "            model, bg_data, bg_masks, gpu_memory_efficient=True\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        test_ids, test_labels, test_idxes = test\n",
    "        test_data, test_masks = model.get_all_ids_masks(test_idxes, seq_len)\n",
    "\n",
    "        #         import pdb\n",
    "\n",
    "        #         pdb.set_trace()\n",
    "\n",
    "        lstm_shap_values = explainer.shap_values(\n",
    "            test_data, test_masks, model_device=device\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        #test_ids = test_ids.detach().cpu()\n",
    "        #test_labels = test_labels.detach().cpu()\n",
    "        #test_idxes = test_idxes.detach().cpu()\n",
    "        #lstm_shap_values = lstm_shap_values.detach().cpu()\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        raise Exception\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "    end_time = time.time()\n",
    "    mins, secs = epoch_time(start_time, end_time)\n",
    "    # print(f\"{device}: test_ids={len(test_ids)}, test_labels={len(test_labels)}, test_idxes={len(test_idxes)}\")\n",
    "    # print(f\"Completed on {device} taking {mins}:{secs}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return (\n",
    "        copy.deepcopy(test_ids), \n",
    "        copy.deepcopy(test_labels), \n",
    "        copy.deepcopy(test_idxes), \n",
    "        copy.deepcopy(lstm_shap_values))\n",
    "\n",
    "\n",
    "def mycallback(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def myerrorcallback(exception):\n",
    "    print(exception)\n",
    "    return exception\n",
    "\n",
    "\n",
    "def get_lstm_features_and_shap_scores_mp(\n",
    "    model,\n",
    "    tr_dataloader,\n",
    "    test,  # don't use dataloader to fix dataset (test_ids, test_labels, test_idxes)\n",
    "    seq_len,\n",
    "    shap_path,\n",
    "    save_output=True,\n",
    "    n_background=None,\n",
    "    background_negative_only=False,\n",
    "    test_positive_only=False,\n",
    "    is_test_random=False,\n",
    "    output_explainer=False,\n",
    "    multigpu_lst=None,  # cuda:1, cuda:2 ...\n",
    "):\n",
    "    \"\"\"Get all features and shape importance scores for each example in te_dataloader.\"\"\"\n",
    "\n",
    "    # Get background dataset\n",
    "    background = sj_utils.get_lstm_background(\n",
    "        tr_dataloader, n_background=n_background, negative_only=background_negative_only\n",
    "    )\n",
    "\n",
    "    # split up test datasets\n",
    "\n",
    "    n_gpu = len(multigpu_lst)\n",
    "    gpu_model_tuple = []\n",
    "    for gpu in multigpu_lst:\n",
    "        model = copy.deepcopy(model)\n",
    "        model.device = gpu\n",
    "        model = model.to(gpu)\n",
    "        gpu_model_tuple.append((gpu, model))\n",
    "\n",
    "    # test = sj_utils.get_lstm_data(\n",
    "    #    te_dataloader,\n",
    "    #    n_test,\n",
    "    #    positive_only=test_positive_only,\n",
    "    #    is_random=is_test_random,\n",
    "    # )\n",
    "    test_ids, test_labels, test_idxes = test\n",
    "\n",
    "    test_labels_lst, test_idxes_lst, test_ids_lst = [], [], []\n",
    "    n_per_gpu = int(np.ceil(len(test_ids) / n_gpu))\n",
    "    for idx in range(n_gpu):\n",
    "        if idx == (n_gpu - 1):\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu :])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu :])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu :])\n",
    "        else:\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "\n",
    "    # multiprocess one core one gpu\n",
    "    # print(f'Starting multiprocess for {n_gpu} cores')\n",
    "    try:\n",
    "        from multiprocessing.dummy import Pool as dThreadPool\n",
    "\n",
    "        pool = dThreadPool(n_gpu)\n",
    "        # pool = torch.multiprocessing.Pool(n_gpu)  # one feeding each gpu\n",
    "        func_call_lst = []\n",
    "        for cur_test_id, cur_test_label, cur_test_idxes, (gpu, model) in zip(\n",
    "            test_ids_lst, test_labels_lst, test_idxes_lst, gpu_model_tuple\n",
    "        ):\n",
    "            # print(f\"\\nlength of tests={len(cur_test_id)}\")\n",
    "            # print(f\"gpu: {n_gpu}\")\n",
    "            # print(f\"model: {model.device}\")\n",
    "\n",
    "            func_call = pool.apply_async(\n",
    "                glfass_single,\n",
    "                (\n",
    "                    model.cpu(),\n",
    "                    background,\n",
    "                    (cur_test_id, cur_test_label, cur_test_idxes),\n",
    "                    seq_len,\n",
    "                    gpu,\n",
    "                ),\n",
    "                callback=mycallback,\n",
    "                error_callback=myerrorcallback,\n",
    "            )\n",
    "            func_call_lst.append(func_call)\n",
    "\n",
    "        # print('Starting to wait')\n",
    "        for func_call in func_call_lst:\n",
    "            func_call.wait()\n",
    "\n",
    "        # print('Collecting results')\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        test_ids, test_labels, test_idxes, lstm_shap_values = None, None, None, None\n",
    "        for func_call in func_call_lst:\n",
    "            init_results = func_call.get()\n",
    "\n",
    "            # first one\n",
    "            if test_ids is None:\n",
    "                test_ids, test_labels, test_idxes, lstm_shap_values = init_results\n",
    "                test_ids = list(test_ids)\n",
    "            else:\n",
    "                test_ids = test_ids + list(init_results[0])\n",
    "                test_labels = torch.cat([test_labels, init_results[1]], dim=0)\n",
    "                test_idxes = torch.cat([test_idxes, init_results[2]], dim=0)\n",
    "                lstm_shap_values = np.concatenate(\n",
    "                    [lstm_shap_values, init_results[3]], axis=0\n",
    "                )\n",
    "\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # raise Exception\n",
    "    #         import IPython.core.debugger\n",
    "\n",
    "    #         dbg = IPython.core.debugger.Pdb()\n",
    "    #         dbg.set_trace()\n",
    "\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pool.terminate()\n",
    "        # print('Multiprocessing pool closed')\n",
    "\n",
    "    # print('collating per patient results')\n",
    "    try:\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "        test = (test_ids, test_labels, test_idxes)\n",
    "        features = []\n",
    "        scores = []\n",
    "        patients = []\n",
    "        total = len(test[0])\n",
    "        import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for idx in range(total):\n",
    "            df_shap, patient_id = sj_utils.get_per_patient_shap(\n",
    "                lstm_shap_values, test, model.vocab, idx\n",
    "            )\n",
    "            events = df_shap[\"events\"].values.tolist()\n",
    "            vals = df_shap[\"shap_vals\"].values.tolist()\n",
    "\n",
    "            pad = \"<pad>\"\n",
    "            if pad in events:\n",
    "                pad_indx = events.index(pad)\n",
    "                events = events[:pad_indx]\n",
    "                vals = vals[:pad_indx]\n",
    "\n",
    "            features.append(events)\n",
    "            scores.append(vals[:])\n",
    "            patients.append(patient_id)\n",
    "\n",
    "        shap_values = (features, scores, patients)\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        raise Exception\n",
    "\n",
    "    if save_output:\n",
    "        if not os.path.isdir(os.path.split(shap_path)[0]):\n",
    "            os.makedirs(os.path.split(shap_path)[0])\n",
    "        save_pickle(shap_values, shap_path)\n",
    "\n",
    "    if output_explainer:\n",
    "        return shap_values, explainer.expected_value\n",
    "\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "MODEL_PARAMS = {\n",
    "    # Dataset/vocab related\n",
    "    \"min_freq\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_eval_val\": 200,\n",
    "    \"num_eval_test\": 200,\n",
    "    # Model related parameters\n",
    "    \"embedding_dim\": 8,\n",
    "    \"hidden_dim\": 16,\n",
    "    \"nlayers\": 2,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"linear_bias\": False,\n",
    "    \"init_type\": \"zero\",  # zero/learned\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"scheduler_step\": 3,\n",
    "    \"clip\": False,\n",
    "    \"rev\": False,\n",
    "    # SHAP-related parameters\n",
    "    \"n_background\": 300,  # Number of background examples\n",
    "    \"background_negative_only\": True,  # If negative examples are used as background\n",
    "    \"background_positive_only\": False,\n",
    "    \"test_positive_only\": False,\n",
    "    \"is_test_random\": False,\n",
    "    \"n_valid_examples\": 64,  # Number of validation examples to be used during shap computation\n",
    "    \"n_test_examples\": 64,  # Number of the final test examples to be used in shap computation #TODO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 47\n",
      "Building dataset from ../../../data/time_diff_toy_dataset_v3/event_based/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/time_diff_toy_dataset_v3/event_based/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/time_diff_toy_dataset_v3/event_based/30/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "with open(VOCAB_PATH, \"rb\") as fp:\n",
    "    vocab = pickle.load(fp)\n",
    "print(f\"vocab len: {len(vocab)}\")  # vocab + padding + unknown\n",
    "\n",
    "train_dataset, _ = build_lstm_dataset(\n",
    "    TRAIN_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "    VALID_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "    TEST_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], \n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], \n",
    "    shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], \n",
    "    shuffle=False, num_workers=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./output/time_diff_toy_dataset_v3/event_based/30/lstm-att-lrp/model_weights/model_05.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_best = AttNoHtLSTM(\n",
    "    MODEL_PARAMS[\"embedding_dim\"],\n",
    "    MODEL_PARAMS[\"hidden_dim\"],\n",
    "    vocab,\n",
    "    model_device,\n",
    "    bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "    nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "    dropout=MODEL_PARAMS[\"dropout\"],\n",
    "    init_type=MODEL_PARAMS[\"init_type\"],\n",
    "    linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_best.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_data(dataloader, num):\n",
    "    \"\"\"Get more than one iteration of data\"\"\"\n",
    "    col_pid, col_lab, col_txt = None, None, None\n",
    "    col_num = 0\n",
    "    for pid, lab, txt in dataloader:\n",
    "        if col_pid is None:\n",
    "            col_pid, col_lab, col_txt = pid, lab, txt\n",
    "        else:\n",
    "            col_pid = tuple(list(col_pid) + list(pid))\n",
    "            col_lab = torch.cat((col_lab, lab), dim=0)\n",
    "            col_txt = torch.cat((col_txt, txt), dim=0)\n",
    "        col_num = len(col_pid)\n",
    "        if col_num > num:\n",
    "            break\n",
    "            \n",
    "    return col_pid[:num], col_lab[:num], col_txt[:num]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_ids, test_labels, test_idxed_text = get_eval_data(\n",
    "    test_dataloader, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch number to match outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttNoHtLSTM(\n",
       "  (emb_layer): Embedding(47, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=False)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_best = {}\n",
    "test_results_best[best_epoch] = {}\n",
    "# calculate relevancy and SHAP\n",
    "lstm_model_best.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP running conditions: define your lower and upper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lim_per_epoch = 200\n",
    "upper_lim = 7000\n",
    "start_idx = 0\n",
    "batch = list(range(num_lim_per_epoch, upper_lim + 1, num_lim_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000, 4200, 4400, 4600, 4800, 5000, 5200, 5400, 5600, 5800, 6000, 6200, 6400, 6600, 6800, 7000]\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_batch(start_idx, end_idx):\n",
    "    start = time.time()\n",
    "    (\n",
    "        test_features,\n",
    "        test_scores,\n",
    "        test_patients,\n",
    "    ) = get_lstm_features_and_shap_scores_mp(\n",
    "        lstm_model_best,\n",
    "        train_dataloader,\n",
    "        (test_patient_ids[start_idx:end_idx], \n",
    "         test_labels[start_idx:end_idx], \n",
    "         test_idxed_text[start_idx:end_idx]),\n",
    "        SEQ_LEN,\n",
    "        \"\",\n",
    "        save_output=False,\n",
    "        n_background=MODEL_PARAMS[\"n_background\"],\n",
    "        background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "        test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "        is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        multigpu_lst=[\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "    )\n",
    "\n",
    "    start_idx = end_idx\n",
    "    end = time.time()\n",
    "    mins, secs = epoch_time(start, end)\n",
    "    print(f\"{end_idx} --> {mins}min: {secs}sec\")\n",
    "    \n",
    "    return (copy.deepcopy(test_features),\n",
    "            copy.deepcopy(test_scores),\n",
    "            copy.deepcopy(test_patients))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual run, this is the long one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 --> 4min: 29sec\n",
      "400 --> 4min: 35sec\n",
      "600 --> 4min: 31sec\n",
      "800 --> 4min: 31sec\n",
      "1000 --> 4min: 32sec\n",
      "1200 --> 4min: 30sec\n",
      "1400 --> 4min: 31sec\n",
      "1600 --> 4min: 28sec\n",
      "1800 --> 4min: 32sec\n",
      "2000 --> 4min: 30sec\n",
      "2200 --> 4min: 35sec\n",
      "2400 --> 4min: 34sec\n",
      "2600 --> 4min: 34sec\n",
      "2800 --> 4min: 33sec\n",
      "3000 --> 4min: 34sec\n",
      "3200 --> 4min: 32sec\n",
      "3400 --> 4min: 35sec\n",
      "3600 --> 4min: 37sec\n",
      "3800 --> 4min: 28sec\n",
      "4000 --> 4min: 33sec\n",
      "4200 --> 4min: 32sec\n",
      "4400 --> 4min: 33sec\n",
      "4600 --> 4min: 33sec\n",
      "4800 --> 4min: 33sec\n",
      "5000 --> 4min: 32sec\n",
      "5200 --> 4min: 32sec\n",
      "5400 --> 4min: 33sec\n",
      "5600 --> 4min: 34sec\n",
      "5800 --> 4min: 34sec\n",
      "6000 --> 4min: 34sec\n",
      "6200 --> 4min: 33sec\n",
      "6400 --> 4min: 37sec\n",
      "6600 --> 4min: 30sec\n",
      "6800 --> 4min: 37sec\n",
      "7000 --> 4min: 35sec\n"
     ]
    }
   ],
   "source": [
    "start_idx = 0\n",
    "all_test_features = []\n",
    "all_test_scores = []\n",
    "all_test_patients = []\n",
    "for end_idx in batch:\n",
    "    (\n",
    "        test_features,\n",
    "        test_scores,\n",
    "        test_patients,\n",
    "    ) = run_a_batch(start_idx, end_idx)\n",
    "    start_idx = end_idx\n",
    "    \n",
    "    all_test_features = all_test_features + test_features\n",
    "    all_test_scores = all_test_scores + test_scores\n",
    "    all_test_patients = all_test_patients + test_patients\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check got all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "for idx, pid in enumerate(all_test_patients):\n",
    "    if pid not in test_results_best[best_epoch].keys():\n",
    "        test_results_best[best_epoch][pid] = {}\n",
    "        \n",
    "    if \"imp\" not in test_results_best[best_epoch][pid].keys():\n",
    "        df = pd.DataFrame()\n",
    "        df['token'] = all_test_features[idx]\n",
    "        df['seq_idx'] = [x for x in range(len(all_test_features[idx]))]\n",
    "    else:\n",
    "        df = test_results_best[best_epoch][pid][\"imp\"]    \n",
    "    df[\"shap_scores\"] = all_test_scores[idx]\n",
    "    \n",
    "    test_results_best[best_epoch][pid][\"imp\"] = df.copy()\n",
    "    \n",
    "    if idx % 500 == 0:\n",
    "        print(idx)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_path = f\"output/time_diff_toy_dataset_v3/event_based/{SEQ_LEN}/{MODEL_NAME}/test_results_shap_5.pkl\"\n",
    "with open(test_results_path, 'wb') as fp:\n",
    "    pickle.dump(test_results_best, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
