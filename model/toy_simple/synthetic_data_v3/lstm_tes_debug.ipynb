{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP, LRP & attention scores with Attention-LSTM\n",
    "Author: Lin Lee Cheong <br>\n",
    "Modified by: Tesfagabir Meharizghi\n",
    "Date created: 1/13/2021 <br>\n",
    "Date updated: 2/10/2021 <br>\n",
    "\n",
    "**Data:** <br>\n",
    "Using the final version of 30 sequence length dataset (sequence-based) generated by Tes<br>\n",
    "Train, validation (for model training), test (for performance etc), and example (4 output)\n",
    "<br>\n",
    "\n",
    "\n",
    "**Steps:** <br>\n",
    "1. Read in datasets [DONE]\n",
    "2. LSTM model training \n",
    "    - TODO: check probab outputs\n",
    "    - save epoch train, val, loss, etc [DONE]\n",
    "    - calculated SHAP & relevance scores for val and test sets [DONE]\n",
    "    - calculate rbo, tau for val and test sets [DONE]\n",
    "    - plot rbo, tau\n",
    "3. Extract SHAP, attention and relevance scores for a TEST set\n",
    "    - calculate SHAP, relevance scores, performance (AUC, test loss)[DONE]\n",
    "    - calculate rbo, tau [DONE]\n",
    "3. Extract SHAP and relevance scores for example set of 4\n",
    "    - plot epoch evolution [DONE]\n",
    "    - add attention [DONE]\n",
    "4. Save output in dict format[DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import newaxis as na\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n",
    "#from lrp_att_model import *\n",
    "import shap_jacc_utils as sj_utils\n",
    "\n",
    "import rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm-att-lrp\"\n",
    "\n",
    "NROWS = 1e9\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SEQ_LEN = 30\n",
    "DATA_TYPE = 'seq_based'\n",
    "\n",
    "XGB_BEST_SHAP_PATH = f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/xgb/shap/test_shap_19.pkl\"\n",
    "\n",
    "TRAIN_DATA_PATH = f\"../../../data/toy_dataset/data/final_final/{DATA_TYPE}/{SEQ_LEN}/train.csv\"\n",
    "VALID_DATA_PATH = f\"../../../data/toy_dataset/data/final_final/{DATA_TYPE}/{SEQ_LEN}/val.csv\"\n",
    "TEST_DATA_PATH = f\"../../../data/toy_dataset/data/final_final/{DATA_TYPE}/{SEQ_LEN}/test.csv\"\n",
    "SELECTED_EXAMPLES_PATH = f\"../../../data/toy_dataset/data/final_final/{DATA_TYPE}/{SEQ_LEN}/visualized_test_patients.txt\"\n",
    "VOCAB_PATH = f\"../../../data/toy_dataset/data/final_final/{DATA_TYPE}/{SEQ_LEN}/vocab.pkl\"\n",
    "\n",
    "MODEL_SAVE_PATH_PATTERN = (\n",
    "    f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/model_weights/model_{'{}'}.pkl\"\n",
    ")\n",
    "SHAP_SAVE_DIR_PATTERN = f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split\n",
    "\n",
    "OUTPUT_RESULTS_PATH = (\n",
    "    f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/train_results/results.csv\"\n",
    ")\n",
    "PARAMS_PATH = (\n",
    "    f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/train_results/model_params.json\"\n",
    ")\n",
    "\n",
    "PLOT_SAVE_DIR = (\n",
    "    f\"./output-debug/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/plots/\"\n",
    ")\n",
    "\n",
    "N_EPOCHS = 6\n",
    "\n",
    "TARGET_COLNAME = \"label\"\n",
    "UID_COLNAME = \"patient_id\"\n",
    "TARGET_VALUE = \"1\"\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "MODEL_PARAMS = {\n",
    "    # Dataset/vocab related\n",
    "    \"min_freq\": 1,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_eval_val\": 200,\n",
    "    \"num_eval_test\": 200,\n",
    "    # Model related parameters\n",
    "    \"embedding_dim\": 8,\n",
    "    \"hidden_dim\": 16,\n",
    "    \"nlayers\": 2,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"linear_bias\": False,\n",
    "    \"init_type\": \"zero\",  # zero/learned\n",
    "    \"learning_rate\": 0.01,#0.01,\n",
    "    \"scheduler_step\": 3,\n",
    "    \"clip\": False,\n",
    "    \"rev\": False,\n",
    "    # SHAP-related parameters\n",
    "    \"n_background\": 300,  # Number of background examples\n",
    "    \"background_negative_only\": True,  # If negative examples are used as background\n",
    "    \"background_positive_only\": False,\n",
    "    \"test_positive_only\": False,\n",
    "    \"is_test_random\": False,\n",
    "    \"n_valid_examples\": 128,  # Number of validation examples to be used during shap computation\n",
    "    \"n_test_examples\": 128,  # Number of the final test examples to be used in shap computation #TODO\n",
    "}\n",
    "\n",
    "# For seq-len=300\n",
    "# # Model Parameters\n",
    "# MODEL_PARAMS = {\n",
    "#     # Dataset/vocab related\n",
    "#     \"min_freq\": 1,\n",
    "#     \"batch_size\": 64\n",
    "#     # Model related parameters\n",
    "#     \"embedding_dim\": 8,\n",
    "#     \"hidden_dim\": 128,\n",
    "#     \"nlayers\": 2,\n",
    "#     \"bidirectional\": True,\n",
    "#     \"dropout\": 0.2,\n",
    "#     \"linear_bias\": False,\n",
    "#     \"init_type\": \"learned\",  # zero/learned\n",
    "#     \"learning_rate\": 0.01,\n",
    "#     \"scheduler_step\": 3,\n",
    "#     \"clip\": True,\n",
    "#     \"rev\": False,\n",
    "#     # SHAP-related parameters\n",
    "#     \"n_background\": 300,  # Number of background examples\n",
    "#     \"background_negative_only\": True,  # If negative examples are used as background\n",
    "#     \"background_positive_only\": False,\n",
    "#     \"test_positive_only\": False,\n",
    "#     \"is_test_random\": False,\n",
    "#     \"n_valid_examples\": 64,  # Number of validation examples to be used during shap computation\n",
    "#     \"n_test_examples\": 128,  # Number of the final test examples to be used in shap computation #TODO\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "def get_rbo(x, y, uid, p=0.7):\n",
    "    x_idx = np.argsort(x)[::-1]\n",
    "    y_idx = np.argsort(y)[::-1]\n",
    "\n",
    "    return rbo.RankingSimilarity(\n",
    "        [uid[idx] for idx in x_idx], [uid[idx] for idx in y_idx]\n",
    "    ).rbo(p=p)\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x):\n",
    "    if \"_N\" in x:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class AttNoHtLSTM(SimpleLSTM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        hidden_dim,\n",
    "        vocab,\n",
    "        device,\n",
    "        nlayers=1,\n",
    "        bidi=True,\n",
    "        use_gpu=True,\n",
    "        pad_idx=0,\n",
    "        dropout=None,\n",
    "        init_type=\"zero\",\n",
    "        linear_bias=True,\n",
    "    ):\n",
    "        super(AttNoHtLSTM, self).__init__(\n",
    "            emb_dim=emb_dim, hidden_dim=hidden_dim, vocab=vocab, device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.emb_layer = nn.Embedding(self.input_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidi = bidi\n",
    "        self.nlayers = nlayers\n",
    "        self.linear_bias = linear_bias\n",
    "\n",
    "        \"\"\"\n",
    "        self.attn_layer = (\n",
    "            nn.Linear(hidden_dim *2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        if dropout is None:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "        self.pred_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias)\n",
    "            if bidi\n",
    "            else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "\n",
    "        self.dpt = nn.Dropout(dropout)\n",
    "\n",
    "        \"\"\"\n",
    "        self.context_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, tokens, ret_attn=False):\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            embedded = self.dpt(self.emb_layer(tokens))\n",
    "        else:\n",
    "            embedded = self.emb_layer(tokens)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = True\n",
    "            self.c0.requires_grad = True\n",
    "            hidden = (\n",
    "                self.h0.repeat(1, tokens.shape[0], 1),\n",
    "                self.c0.repeat(1, tokens.shape[0], 1),\n",
    "            )\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(tokens.shape[0])\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        text_lengths = torch.sum(tokens != self.pad_idx, dim=1).to(\"cpu\")\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "\n",
    "        packed_output, (final_hidden, cell) = self.lstm(packed_embedded, hidden)\n",
    "\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=tokens.shape[1]\n",
    "        )\n",
    "\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "\n",
    "        # Switch to multiplicative attention\n",
    "        mask_feats = np.array(tokens.cpu().numpy() == 0)\n",
    "        mask_feats = -1000 * mask_feats.astype(np.int)\n",
    "\n",
    "        mask_feats = torch.Tensor(mask_feats).to(self.device)\n",
    "\n",
    "        attn_weights_int = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            (tokens.shape[1]) ** 0.5\n",
    "        )\n",
    "        attn_weights = nn.functional.softmax(attn_weights_int + mask_feats, -1)\n",
    "\n",
    "        context = torch.bmm(output.transpose(1, 2), attn_weights.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )\n",
    "\n",
    "        concat_out = context\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            pred = self.pred_layer(self.dpt(concat_out))\n",
    "        else:\n",
    "            pred = self.pred_layer(concat_out)\n",
    "\n",
    "        if ret_attn:\n",
    "            return (\n",
    "                pred.detach().cpu().numpy(),\n",
    "                attn_weights.detach().cpu().numpy(),\n",
    "                context.detach().cpu().numpy(),\n",
    "                attn_weights_int.detach().cpu().numpy(),\n",
    "                out.detach().cpu().numpy(),\n",
    "                output.detach().cpu().numpy(),\n",
    "            )\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward_shap(self, token_ids, mask, full_id_matrix=False):\n",
    "        token_ids = token_ids if token_ids.is_cuda else token_ids.to(self.device)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = False\n",
    "            self.c0.requires_grad = False\n",
    "\n",
    "            hidden = (self.h0.repeat(1, 1, 1), self.c0.repeat(1, 1, 1))\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(1)\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        token_ids[sum(mask) :, :] = 0\n",
    "        embedded = torch.matmul(token_ids, self.emb_layer.weight).unsqueeze(0)\n",
    "\n",
    "        embedded = embedded[:, : sum(mask), :]\n",
    "\n",
    "        output, _ = self.lstm(embedded, hidden)\n",
    "\n",
    "        # output = output.permute(1, 0, 2)  # [batch, text_length, hidden_dim]\n",
    "        # print(f'Output dimensions: {output.shape}')\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        # print(f'Stacked hidden dimensions: {stacked_hidden.shape}')\n",
    "        # print(f'mask weight dimensions: {mask_feats.shape}')\n",
    "        # attention = self.context_layer(output).squeeze(-1)\n",
    "        # att_weights = nn.functional.softmax(attention, dim=-1)\n",
    "        # context = torch.bmm(att_weights.unsqueeze(1), output).squeeze(1)\n",
    "        attn_weights = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            sum(mask) ** 0.5\n",
    "        )\n",
    "\n",
    "        soft_attn_weights = nn.functional.softmax(attn_weights, 1)\n",
    "\n",
    "        context = torch.bmm(\n",
    "            output.transpose(1, 2), soft_attn_weights.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # concat_out = torch.cat((context, out), dim=1)\n",
    "        concat_out = context\n",
    "        pred = self.pred_layer(concat_out)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class LSTM_LRP_MultiLayer:\n",
    "    def __init__(self, pymodel):\n",
    "        super(LSTM_LRP_MultiLayer, self).__init__()\n",
    "\n",
    "        self.init_model(pymodel)\n",
    "\n",
    "    def init_model(self, pymodel):\n",
    "\n",
    "        self.device = pymodel.device\n",
    "        self.use_gpu = pymodel.use_gpu\n",
    "        self.bidi = pymodel.bidi\n",
    "\n",
    "        self.emb_dim = pymodel.emb_dim\n",
    "        self.vocab = pymodel.vocab\n",
    "        self.input_dim = len(self.vocab)\n",
    "        self.pad_idx = pymodel.pad_idx\n",
    "        self.hidden_dim = pymodel.hidden_dim\n",
    "\n",
    "        self.emb = pymodel.emb_layer.weight.detach().numpy()\n",
    "\n",
    "        param_list = list(pymodel.lstm.named_parameters())\n",
    "        param_dict = {}\n",
    "        for param_tuple in param_list:\n",
    "            param_dict[param_tuple[0]] = param_tuple[-1].detach().numpy()\n",
    "\n",
    "        # rearrange, pytorch uses ifgo format, need to move to icfo/igfo format\n",
    "        idx_list = (\n",
    "            list(range(0, self.hidden_dim))\n",
    "            + list(range(self.hidden_dim * 2, self.hidden_dim * 3))\n",
    "            + list(range(self.hidden_dim, self.hidden_dim * 2))\n",
    "            + list(range(self.hidden_dim * 3, self.hidden_dim * 4))\n",
    "        )\n",
    "        self.nlayers = pymodel.nlayers\n",
    "\n",
    "        # i (input), g (candidate), f (forget), o (output) order\n",
    "        # (4 * hidden_dim, emb_dim)\n",
    "        self.Wxh_Left = {}\n",
    "        self.bxh_Left = {}\n",
    "        self.Whh_Left = {}\n",
    "        self.bhh_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wxh_Right = {}\n",
    "            self.bxh_Right = {}\n",
    "            self.Whh_Right = {}\n",
    "            self.bhh_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.Wxh_Left[layer] = param_dict[f\"weight_ih_l{layer}\"][idx_list]\n",
    "            self.bxh_Left[layer] = param_dict[f\"bias_ih_l{layer}\"][idx_list]  # shape 4d\n",
    "            self.Whh_Left[layer] = param_dict[f\"weight_hh_l{layer}\"][\n",
    "                idx_list\n",
    "            ]  # shape 4d*d\n",
    "            self.bhh_Left[layer] = param_dict[f\"bias_hh_l{layer}\"][idx_list]  # shape 4d\n",
    "\n",
    "            if self.bidi:\n",
    "                # LSTM right encoder\n",
    "                self.Wxh_Right[layer] = param_dict[f\"weight_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bxh_Right[layer] = param_dict[f\"bias_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.Whh_Right[layer] = param_dict[f\"weight_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bhh_Right[layer] = param_dict[f\"bias_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "\n",
    "        # START ADDED: CONTEXT LAYER INIT\n",
    "        # linear output layer: shape C * 4d\n",
    "        # 0-d: fwd & context\n",
    "        # d-2d: rev & context\n",
    "        # 2d-3d: fwd & final hidden\n",
    "        # 3d-4d: rev & final hidden\n",
    "        Why = pymodel.pred_layer.weight.detach().numpy()\n",
    "\n",
    "        self.Why_Left = Why[:, 2 * self.hidden_dim : 3 * self.hidden_dim]  # shape C*d\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Why_Right = Why[:, 3 * self.hidden_dim :]  # shape C*d\n",
    "\n",
    "        self.Wcy_Left = Why[:, : self.hidden_dim]\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wcy_Right = Why[:, self.hidden_dim : 2 * self.hidden_dim]\n",
    "        # END ADDED: CONTEXT LAYER INIT\n",
    "\n",
    "    def set_input(self, tokens):\n",
    "        T = len(tokens)  # sequence length\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)  # hidden layer dimension\n",
    "        e = self.emb.shape[1]  # word embedding dimension\n",
    "\n",
    "        self.w = tokens\n",
    "        self.x = {}\n",
    "        self.x_rev = {}\n",
    "        x = np.zeros((T, e))\n",
    "        x[:, :] = self.emb[tokens, :]\n",
    "        self.x[0] = x\n",
    "        self.x_rev[0] = x[::-1, :].copy()\n",
    "        self.h_Left = {}\n",
    "        self.c_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.h_Right = {}\n",
    "            self.c_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.h_Left[layer] = np.zeros((T + 1, d))\n",
    "            self.c_Left[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "            if self.bidi:\n",
    "                self.h_Right[layer] = np.zeros((T + 1, d))\n",
    "                self.c_Right[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "        self.att_score = None\n",
    "\n",
    "    def forward_gate(self, layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir):\n",
    "\n",
    "        if gate_dir == \"left\":\n",
    "            self.gates_xh_Left[layer][t] = np.dot(\n",
    "                self.Wxh_Left[layer], self.x[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Left[layer][t] = np.dot(\n",
    "                self.Whh_Left[layer], self.h_Left[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Left[layer][t] = (\n",
    "                self.gates_xh_Left[layer][t]\n",
    "                + self.gates_hh_Left[layer][t]\n",
    "                + self.bxh_Left[layer]\n",
    "                + self.bhh_Left[layer]\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Left[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Left[layer][t] = (\n",
    "                self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1]\n",
    "                + self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Left[layer][t] = self.gates_Left[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Left[layer][t]\n",
    "            )\n",
    "\n",
    "        if gate_dir == \"right\":\n",
    "            self.gates_xh_Right[layer][t] = np.dot(\n",
    "                self.Wxh_Right[layer], self.x_rev[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Right[layer][t] = np.dot(\n",
    "                self.Whh_Right[layer], self.h_Right[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Right[layer][t] = (\n",
    "                self.gates_xh_Right[layer][t]\n",
    "                + self.gates_hh_Right[layer][t]\n",
    "                + self.bxh_Right[layer]\n",
    "                + self.bhh_Right[layer]\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Right[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Right[layer][t] = (\n",
    "                self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1]\n",
    "                + self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Right[layer][t] = self.gates_Right[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Right[layer][t]\n",
    "            )\n",
    "\n",
    "    def forward_lrp(self):\n",
    "        \"\"\"\n",
    "        Standard forward pass.\n",
    "        Compute the hidden layer values (assuming input x/x_rev was previously set)\n",
    "        \"\"\"\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "\n",
    "        # gate indices (assuming the gate ordering in the LSTM weights is i,g,f,o):\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        self.gates_xh_Left = {}\n",
    "        self.gates_hh_Left = {}\n",
    "        self.gates_pre_Left = {}\n",
    "        self.gates_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.gates_xh_Right = {}\n",
    "            self.gates_hh_Right = {}\n",
    "            self.gates_pre_Right = {}\n",
    "            self.gates_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.gates_xh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_hh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_pre_Left[layer] = np.zeros((T, 4 * d))  # gates pre-activation\n",
    "            self.gates_Left[layer] = np.zeros((T, 4 * d))  # gates activation\n",
    "\n",
    "            if self.bidi:\n",
    "                self.gates_xh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_hh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_pre_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_Right[layer] = np.zeros((T, 4 * d))\n",
    "\n",
    "        # START ADDED: INITIALIZE CONTEXT LAYERS\n",
    "        self.ctxt_Left = np.zeros((1, d))\n",
    "        self.ctxt_Right = np.zeros((1, d))\n",
    "        self.att_wgt_Left = np.zeros((T, 1))\n",
    "        self.att_wgt_Right = np.zeros((T, 1))\n",
    "        self.att_score = np.zeros((T, 1))\n",
    "\n",
    "        # END ADDED: INITIALIZE CONTEXT LAYERS\n",
    "\n",
    "        # START EDIT: cycle through first layer first\n",
    "        layer = 0\n",
    "        for t in range(T):\n",
    "            self.forward_gate(\n",
    "                layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "            )\n",
    "            if self.bidi:\n",
    "                self.forward_gate(\n",
    "                    layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                )\n",
    "\n",
    "        # go through all the rest of the layers\n",
    "        if self.nlayers > 1:\n",
    "            ## TODO: fix init t-1 (zero time step) Zeroes!!\n",
    "            self.x[layer + 1] = (\n",
    "                np.concatenate(\n",
    "                    (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                )\n",
    "                if self.bidi\n",
    "                else self.h_Left[layer][:T]\n",
    "            )\n",
    "\n",
    "            self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "            for layer in range(1, self.nlayers):\n",
    "                for t in range(T):\n",
    "                    self.forward_gate(\n",
    "                        layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "                    )\n",
    "                    if self.bidi:\n",
    "                        self.forward_gate(\n",
    "                            layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                        )\n",
    "\n",
    "                    self.x[layer + 1] = np.concatenate(\n",
    "                        (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                    )\n",
    "                    self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "        # calculate attention layer & context layer\n",
    "        top_layer = self.nlayers - 1\n",
    "        self.att_wgt_Left = np.dot(\n",
    "            self.h_Left[top_layer][:T, :], self.h_Left[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_wgt_Right = np.dot(\n",
    "            self.h_Right[top_layer][:T, :], self.h_Right[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_score = self.stable_softmax(\n",
    "            (self.att_wgt_Left + self.att_wgt_Right) / (T ** 0.5)\n",
    "        )\n",
    "\n",
    "        self.ctxt_Left = (self.att_score[:, na] * self.h_Left[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "        self.ctxt_Right = (self.att_score[:, na] * self.h_Right[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # CALCULATE WITH CONTEXT & OUT, NOT JUST HIDDEN\n",
    "        # self.y_Left = np.dot(self.Why_Left, self.h_Left[top_layer][T - 1])\n",
    "        self.y_Left = np.dot(self.Wcy_Left, self.ctxt_Left)\n",
    "\n",
    "        # self.y_Right = np.dot(self.Why_Right, self.h_Right[top_layer][T - 1])\n",
    "        self.y_Right = np.dot(self.Wcy_Right, self.ctxt_Right)\n",
    "\n",
    "        self.s = self.y_Left + self.y_Right\n",
    "\n",
    "        return self.s.copy()  # prediction scores\n",
    "\n",
    "    def stable_softmax(self, x):\n",
    "        z = x - np.max(x)\n",
    "        num = np.exp(z)\n",
    "        denom = np.sum(num)\n",
    "        softmax_vals = num / denom\n",
    "\n",
    "        return softmax_vals\n",
    "\n",
    "    def lrp_left_gate(\n",
    "        self,\n",
    "        Rc_Left,\n",
    "        Rh_Left,\n",
    "        Rg_Left,\n",
    "        Rx,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "\n",
    "        # import IPython\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        Rc_Left[layer][t] += Rh_Left[layer][t]\n",
    "        Rc_Left[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rg_Left[layer][t] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx[layer][t] += lrp_linear(\n",
    "            self.x[layer][t],\n",
    "            self.Wxh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Left[layer][t - 1] += lrp_linear(\n",
    "            self.h_Left[layer][t - 1],\n",
    "            self.Whh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Left, Rh_Left, Rg_Left, Rx\n",
    "\n",
    "    def lrp_right_gate(\n",
    "        self,\n",
    "        Rc_Right,\n",
    "        Rh_Right,\n",
    "        Rg_Right,\n",
    "        Rx_rev,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "        Rc_Right[layer][t] += Rh_Right[layer][t]\n",
    "        Rc_Right[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        Rg_Right[layer][t] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx_rev[layer][t] += lrp_linear(\n",
    "            self.x_rev[layer][t],\n",
    "            self.Wxh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Right[layer][t - 1] += lrp_linear(\n",
    "            self.h_Right[layer][t - 1],\n",
    "            self.Whh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Right, Rh_Right, Rg_Right, Rx_rev\n",
    "\n",
    "    def lrp(self, w, LRP_class, eps=0.001, bias_factor=0.0):\n",
    "        \"\"\"\n",
    "        Layer-wise Relevance Propagation (LRP) backward pass.\n",
    "        Compute the hidden layer relevances by performing LRP for the target class LRP_class\n",
    "        (according to the papers:\n",
    "            - https://doi.org/10.1371/journal.pone.0130140\n",
    "            - https://doi.org/10.18653/v1/W17-5221 )\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        self.set_input(w)\n",
    "        self.forward_lrp()\n",
    "\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "        e = self.emb.shape[1]\n",
    "        C = self.Why_Left.shape[0]  # number of classes\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        Rx = {}\n",
    "        Rx_rev = {}\n",
    "        Rx_all = {}\n",
    "\n",
    "        Rh_Left = {}\n",
    "        Rc_Left = {}\n",
    "        Rg_Left = {}  # gate g only\n",
    "\n",
    "        if self.bidi:\n",
    "            Rh_Right = {}\n",
    "            Rc_Right = {}\n",
    "            Rg_Right = {}  # gate g only\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            Rx[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_rev[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_all[layer] = np.zeros(self.x[layer].shape)\n",
    "\n",
    "            Rh_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rc_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rg_Left[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "            if self.bidi:\n",
    "                Rh_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rc_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rg_Right[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "        Rctxt_Left = np.zeros((1, d))\n",
    "        Rctxt_Right = np.zeros((1, d))\n",
    "\n",
    "        Rout_mask = np.zeros((C))\n",
    "        Rout_mask[LRP_class] = 1.0\n",
    "\n",
    "        # process top most layer first\n",
    "        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
    "        layer = self.nlayers - 1\n",
    "        \"\"\"\n",
    "        Rh_Left[layer][T - 1] = lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            self.Why_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] = lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                self.Why_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "        \"\"\"\n",
    "        # ADD CONTEXT CALCULATIONS TO CONTEXT LAYER\n",
    "        Rctxt_Left = lrp_linear(\n",
    "            self.ctxt_Left,\n",
    "            self.Wcy_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rctxt_Right = lrp_linear(\n",
    "                self.ctxt_Right,\n",
    "                self.Wcy_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        # CONTRIBUTION FROM ATTN LAYER\n",
    "        Rh_Left[layer][T - 1] += lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            np.identity((d)),\n",
    "            np.zeros((d)),\n",
    "            self.ctxt_Left,\n",
    "            self.att_score[T - 1] * Rctxt_Left,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] += lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Right,\n",
    "                self.att_score[T - 1] * Rctxt_Right,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        ee = e if self.nlayers == 1 else 2 * d\n",
    "        for t in reversed(range(T)):\n",
    "\n",
    "            Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                Rc_Left,\n",
    "                Rh_Left,\n",
    "                Rg_Left,\n",
    "                Rx,\n",
    "                layer,\n",
    "                t,\n",
    "                d,\n",
    "                ee,\n",
    "                idx,\n",
    "                idx_f,\n",
    "                idx_i,\n",
    "                idx_g,\n",
    "                idx_o,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "            )\n",
    "\n",
    "            # ATTN Relevance scores\n",
    "            Rh_Left[layer][t - 1] += lrp_linear(\n",
    "                self.h_Left[layer][t - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Left,\n",
    "                self.att_score[t - 1] * Rctxt_Left,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "            if self.bidi:\n",
    "                Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                    Rc_Right,\n",
    "                    Rh_Right,\n",
    "                    Rg_Right,\n",
    "                    Rx_rev,\n",
    "                    layer,\n",
    "                    t,\n",
    "                    d,\n",
    "                    ee,\n",
    "                    idx,\n",
    "                    idx_f,\n",
    "                    idx_i,\n",
    "                    idx_g,\n",
    "                    idx_o,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                )\n",
    "                # ATTN Relevance scores for top-most layer\n",
    "                Rh_Right[layer][t - 1] += lrp_linear(\n",
    "                    self.h_Right[layer][t - 1],\n",
    "                    np.identity((d)),\n",
    "                    np.zeros((d)),\n",
    "                    self.ctxt_Right,\n",
    "                    self.att_score[t - 1] * Rctxt_Right,\n",
    "                    4 * d,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                    debug=False,\n",
    "                )\n",
    "\n",
    "        # propagate through remaining layers\n",
    "        if self.nlayers > 1:\n",
    "            remaining_layers = list(range(0, self.nlayers - 1))[::-1]\n",
    "            # print(f\"remaining layers: {remaining_layers}\")\n",
    "\n",
    "            # no more attn layer flow back\n",
    "            for layer in remaining_layers:\n",
    "\n",
    "                # Sum up all the relevances for each of the inputs in sequence\n",
    "                Rx_all[layer + 1] = Rx[layer + 1] + Rx_rev[layer + 1][::-1, :]\n",
    "\n",
    "                ee = e if layer == 0 else 2 * d\n",
    "                for t in reversed(range(T)):\n",
    "                    # Rh_Left[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Left[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Left[layer][t], #self.x[layer+1][t, :d],\n",
    "                    #    Rx_all[layer+1][t, :d],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Left[layer][t] += Rx_all[layer + 1][t, :d]\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                        Rc_Left,\n",
    "                        Rh_Left,\n",
    "                        Rg_Left,\n",
    "                        Rx,\n",
    "                        layer,\n",
    "                        t,\n",
    "                        d,\n",
    "                        ee,\n",
    "                        idx,\n",
    "                        idx_f,\n",
    "                        idx_i,\n",
    "                        idx_g,\n",
    "                        idx_o,\n",
    "                        eps,\n",
    "                        bias_factor,\n",
    "                    )\n",
    "\n",
    "                    ### RIGHT +++++++++\n",
    "                    # Rh_Right[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Right[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Right[layer][t], #self.x_rev[layer+1][::-1, :][t, d:],\n",
    "                    #    Rx_all[layer+1][t, d:],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Right[layer][t] += Rx_all[layer + 1][::-1, :][t, d:]\n",
    "                    if self.bidi:\n",
    "                        Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                            Rc_Right,\n",
    "                            Rh_Right,\n",
    "                            Rg_Right,\n",
    "                            Rx_rev,\n",
    "                            layer,\n",
    "                            t,\n",
    "                            d,\n",
    "                            ee,\n",
    "                            idx,\n",
    "                            idx_f,\n",
    "                            idx_i,\n",
    "                            idx_g,\n",
    "                            idx_o,\n",
    "                            eps,\n",
    "                            bias_factor,\n",
    "                        )\n",
    "\n",
    "        # record\n",
    "        self.Rx_all = Rx_all\n",
    "        self.Rx = Rx\n",
    "        self.Rx_rev = Rx_rev\n",
    "        self.Rh_Left = Rh_Left\n",
    "        self.Rh_Right = Rh_Right\n",
    "        self.Rc_Left = Rc_Left\n",
    "        self.Rc_Right = Rc_Right\n",
    "        self.Rg_Right = Rg_Right\n",
    "        self.d = d\n",
    "        self.ee = ee\n",
    "        self.Rctxt_Left = Rctxt_Left\n",
    "        self.Rctxt_Right = Rctxt_Right\n",
    "\n",
    "        return (\n",
    "            Rx[0],\n",
    "            Rx_rev[0][::-1, :],\n",
    "            Rh_Left[0][-1].sum()\n",
    "            + Rc_Left[0][-1].sum()\n",
    "            + Rh_Right[0][-1].sum()\n",
    "            + Rc_Right[0][-1].sum(),\n",
    "        )\n",
    "\n",
    "    def get_attn_values(self):\n",
    "        return self.att_score\n",
    "\n",
    "\n",
    "def get_sim(idx_model, idx_gt):\n",
    "    return len(set(idx_model).intersection(set(idx_gt))) / len(idx_gt)\n",
    "\n",
    "\n",
    "def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):\n",
    "    \"\"\"\n",
    "    LRP for a linear layer with input dim D and output dim M.\n",
    "    Args:\n",
    "    - hin:            forward pass input, of shape (D,)\n",
    "    - w:              connection weights, of shape (D, M)\n",
    "    - b:              biases, of shape (M,)\n",
    "    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)\n",
    "    - Rout:           relevance at layer output, of shape (M,)\n",
    "    - bias_nb_units:  total number of connected lower-layer units (onto which the bias/stabilizer contribution is redistributed for sanity check)\n",
    "    - eps:            stabilizer (small positive number)\n",
    "    - bias_factor:    set to 1.0 to check global relevance conservation, otherwise use 0.0 to ignore bias/stabilizer redistribution (recommended)\n",
    "    Returns:\n",
    "    - Rin:            relevance at layer input, of shape (D,)\n",
    "    \"\"\"\n",
    "    sign_out = np.where(hout[na, :] >= 0, 1.0, -1.0)  # shape (1, M)\n",
    "\n",
    "    # numer    = (w * hin[:,na]) + ( (bias_factor*b[na,:]*1.) * 1./bias_nb_units )\n",
    "    numer = (w * hin[:, na]) + (\n",
    "        bias_factor * (b[na, :] * 1.0 + eps * sign_out * 1.0) / bias_nb_units\n",
    "    )  # shape (D, M)\n",
    "\n",
    "    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact\n",
    "    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check\n",
    "    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)\n",
    "\n",
    "    denom = hout[na, :] + (eps * sign_out * 1.0)  # shape (1, M)\n",
    "\n",
    "    message = (numer / denom) * Rout[na, :]  # shape (D, M)\n",
    "\n",
    "    Rin = message.sum(axis=1)  # shape (D,)\n",
    "\n",
    "    if debug:\n",
    "        print(\"local diff: \", Rout.sum() - Rin.sum())\n",
    "    # Note:\n",
    "    # - local  layer   relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)\n",
    "    # - global network relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections\n",
    "    # -> can be used for sanity check\n",
    "\n",
    "    return Rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_valid_data(n_val_eval, batch_size, valid_dataloader):\n",
    "    \"\"\"Get subset of validation dataset to run SHAP/LRP on\"\"\"\n",
    "\n",
    "    n_loads = int(np.ceil(n_val_eval / batch_size))\n",
    "    counter = 0\n",
    "\n",
    "    for ids, labels, idxed_text in valid_dataloader:\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 1:\n",
    "            sub_val_ids, sub_val_labels, sub_val_idxed_text = ids, labels, idxed_text\n",
    "        else:\n",
    "            sub_val_ids = sub_val_ids + ids\n",
    "            sub_val_labels = torch.cat([sub_val_labels, labels])\n",
    "            sub_val_idxed_text = torch.cat([sub_val_idxed_text, idxed_text])\n",
    "\n",
    "        if counter == n_loads:\n",
    "            break\n",
    "\n",
    "    sub_val_ids = sub_val_ids[:n_val_eval]\n",
    "    sub_val_labels = sub_val_labels[:n_val_eval]\n",
    "    sub_val_idxed_text = sub_val_idxed_text[:n_val_eval]\n",
    "\n",
    "    return (sub_val_ids, sub_val_labels, sub_val_idxed_text)\n",
    "\n",
    "\n",
    "def glfass_single(cpu_model, background, test, seq_len, device):\n",
    "    \"\"\"\n",
    "    Single-thread function for Get Lstm Features And Shap Scores\n",
    "    Called by get_lstm_features_and_shap_scores_mp\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = cpu_model.to(device)\n",
    "\n",
    "    try:\n",
    "\n",
    "        background_ids, background_labels, background_idxes = background\n",
    "        bg_data, bg_masks = model.get_all_ids_masks(background_idxes, seq_len)\n",
    "\n",
    "        explainer = deep_id_pytorch.CustomPyTorchDeepIDExplainer(\n",
    "            model, bg_data, bg_masks, gpu_memory_efficient=True\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        test_ids, test_labels, test_idxes = test\n",
    "        test_data, test_masks = model.get_all_ids_masks(test_idxes, seq_len)\n",
    "\n",
    "        #         import pdb\n",
    "\n",
    "        #         pdb.set_trace()\n",
    "\n",
    "        lstm_shap_values = explainer.shap_values(\n",
    "            test_data, test_masks, model_device=device\n",
    "        )\n",
    "\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        raise Exception\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "    end_time = time.time()\n",
    "    mins, secs = epoch_time(start_time, end_time)\n",
    "    # print(f\"{device}: test_ids={len(test_ids)}, test_labels={len(test_labels)}, test_idxes={len(test_idxes)}\")\n",
    "    # print(f\"Completed on {device} taking {mins}:{secs}\")\n",
    "    return (test_ids, test_labels, test_idxes, lstm_shap_values)\n",
    "\n",
    "\n",
    "def mycallback(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def myerrorcallback(exception):\n",
    "    print(exception)\n",
    "    return exception\n",
    "\n",
    "\n",
    "def get_lstm_features_and_shap_scores_mp(\n",
    "    model,\n",
    "    tr_dataloader,\n",
    "    test,  # don't use dataloader to fix dataset (test_ids, test_labels, test_idxes)\n",
    "    seq_len,\n",
    "    shap_path,\n",
    "    save_output=True,\n",
    "    n_background=None,\n",
    "    background_negative_only=False,\n",
    "    test_positive_only=False,\n",
    "    is_test_random=False,\n",
    "    output_explainer=False,\n",
    "    multigpu_lst=None,  # cuda:1, cuda:2 ...\n",
    "):\n",
    "    \"\"\"Get all features and shape importance scores for each example in te_dataloader.\"\"\"\n",
    "\n",
    "    # Get background dataset\n",
    "    background = sj_utils.get_lstm_background(\n",
    "        tr_dataloader, n_background=n_background, negative_only=background_negative_only\n",
    "    )\n",
    "\n",
    "    # split up test datasets\n",
    "\n",
    "    n_gpu = len(multigpu_lst)\n",
    "    gpu_model_tuple = []\n",
    "    for gpu in multigpu_lst:\n",
    "        model = copy.deepcopy(model)\n",
    "        model.device = gpu\n",
    "        model = model.to(gpu)\n",
    "        gpu_model_tuple.append((gpu, model))\n",
    "\n",
    "    # test = sj_utils.get_lstm_data(\n",
    "    #    te_dataloader,\n",
    "    #    n_test,\n",
    "    #    positive_only=test_positive_only,\n",
    "    #    is_random=is_test_random,\n",
    "    # )\n",
    "    test_ids, test_labels, test_idxes = test\n",
    "\n",
    "    test_labels_lst, test_idxes_lst, test_ids_lst = [], [], []\n",
    "    n_per_gpu = int(np.ceil(len(test_ids) / n_gpu))\n",
    "    for idx in range(n_gpu):\n",
    "        if idx == (n_gpu - 1):\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu :])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu :])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu :])\n",
    "        else:\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "\n",
    "    # multiprocess one core one gpu\n",
    "    # print(f'Starting multiprocess for {n_gpu} cores')\n",
    "    try:\n",
    "        from multiprocessing.dummy import Pool as dThreadPool\n",
    "\n",
    "        pool = dThreadPool(n_gpu)\n",
    "        # pool = torch.multiprocessing.Pool(n_gpu)  # one feeding each gpu\n",
    "        func_call_lst = []\n",
    "        for cur_test_id, cur_test_label, cur_test_idxes, (gpu, model) in zip(\n",
    "            test_ids_lst, test_labels_lst, test_idxes_lst, gpu_model_tuple\n",
    "        ):\n",
    "            # print(f\"\\nlength of tests={len(cur_test_id)}\")\n",
    "            # print(f\"gpu: {n_gpu}\")\n",
    "            # print(f\"model: {model.device}\")\n",
    "\n",
    "            func_call = pool.apply_async(\n",
    "                glfass_single,\n",
    "                (\n",
    "                    model.cpu(),\n",
    "                    background,\n",
    "                    (cur_test_id, cur_test_label, cur_test_idxes),\n",
    "                    seq_len,\n",
    "                    gpu,\n",
    "                ),\n",
    "                callback=mycallback,\n",
    "                error_callback=myerrorcallback,\n",
    "            )\n",
    "            func_call_lst.append(func_call)\n",
    "\n",
    "        # print('Starting to wait')\n",
    "        for func_call in func_call_lst:\n",
    "            func_call.wait()\n",
    "\n",
    "        # print('Collecting results')\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        test_ids, test_labels, test_idxes, lstm_shap_values = None, None, None, None\n",
    "        for func_call in func_call_lst:\n",
    "            init_results = func_call.get()\n",
    "\n",
    "            # first one\n",
    "            if test_ids is None:\n",
    "                test_ids, test_labels, test_idxes, lstm_shap_values = init_results\n",
    "                test_ids = list(test_ids)\n",
    "            else:\n",
    "                test_ids = test_ids + list(init_results[0])\n",
    "                test_labels = torch.cat([test_labels, init_results[1]], dim=0)\n",
    "                test_idxes = torch.cat([test_idxes, init_results[2]], dim=0)\n",
    "                lstm_shap_values = np.concatenate(\n",
    "                    [lstm_shap_values, init_results[3]], axis=0\n",
    "                )\n",
    "\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # raise Exception\n",
    "    #         import IPython.core.debugger\n",
    "\n",
    "    #         dbg = IPython.core.debugger.Pdb()\n",
    "    #         dbg.set_trace()\n",
    "\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pool.terminate()\n",
    "        # print('Multiprocessing pool closed')\n",
    "\n",
    "    # print('collating per patient results')\n",
    "    try:\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "        test = (test_ids, test_labels, test_idxes)\n",
    "        features = []\n",
    "        scores = []\n",
    "        patients = []\n",
    "        total = len(test[0])\n",
    "        import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for idx in range(total):\n",
    "            df_shap, patient_id = sj_utils.get_per_patient_shap(\n",
    "                lstm_shap_values, test, model.vocab, idx\n",
    "            )\n",
    "            events = df_shap[\"events\"].values.tolist()\n",
    "            vals = df_shap[\"shap_vals\"].values.tolist()\n",
    "\n",
    "            pad = \"<pad>\"\n",
    "            if pad in events:\n",
    "                pad_indx = events.index(pad)\n",
    "                events = events[:pad_indx]\n",
    "                vals = vals[:pad_indx]\n",
    "\n",
    "            features.append(events)\n",
    "            scores.append(vals[:])\n",
    "            patients.append(patient_id)\n",
    "\n",
    "        shap_values = (features, scores, patients)\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        raise Exception\n",
    "\n",
    "    if save_output:\n",
    "        if not os.path.isdir(os.path.split(shap_path)[0]):\n",
    "            os.makedirs(os.path.split(shap_path)[0])\n",
    "        save_pickle(shap_values, shap_path)\n",
    "\n",
    "    if output_explainer:\n",
    "        return shap_values, explainer.expected_value\n",
    "\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created: ./output-debug/final_final/seq_based/30/lstm-att-lrp/plots/\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(PLOT_SAVE_DIR):\n",
    "    shutil.rmtree(PLOT_SAVE_DIR)  \n",
    "os.makedirs(PLOT_SAVE_DIR)\n",
    "print(f\"Directory Created: {PLOT_SAVE_DIR}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created: ./output-debug/final_final/seq_based/30/lstm-att-lrp/model_weights\n",
      "Directory Created: ./output-debug/final_final/seq_based/30/lstm-att-lrp/shap\n",
      "Directory Created: ./output-debug/final_final/seq_based/30/lstm-att-lrp/train_results\n",
      "Directory Created: ./output-debug/final_final/seq_based/30/lstm-att-lrp/plots/\n"
     ]
    }
   ],
   "source": [
    "# Create output directories if needed\n",
    "model_dir = os.path.dirname(MODEL_SAVE_PATH_PATTERN)\n",
    "shap_dir = os.path.dirname(SHAP_SAVE_DIR_PATTERN)\n",
    "output_dir = os.path.dirname(OUTPUT_RESULTS_PATH)\n",
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    if os.path.exists(shap_dir):\n",
    "        shutil.rmtree(shap_dir)\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    if os.path.exists(PLOT_SAVE_DIR):\n",
    "        shutil.rmtree(PLOT_SAVE_DIR)       \n",
    "    os.makedirs(model_dir)\n",
    "    os.makedirs(shap_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    os.makedirs(PLOT_SAVE_DIR)\n",
    "    print(f\"Directory Created: {model_dir}\")\n",
    "    print(f\"Directory Created: {shap_dir}\")\n",
    "    print(f\"Directory Created: {output_dir}\")\n",
    "    print(f\"Directory Created: {PLOT_SAVE_DIR}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data/toy_dataset/data/final_final/seq_based/30/visualized_test_patients.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECTED_EXAMPLES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Test Examples: ['PPNXUONY5M', 'F245SZ8WW0', '05OUFAS2FY', 'NXYMGAG4RI']\n"
     ]
    }
   ],
   "source": [
    "# Load Selected Patients for later SHAP visualization\n",
    "patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "patients = patients.values.flatten().tolist()\n",
    "print(f\"Selected Test Examples: {patients}\")\n",
    "# create the example set\n",
    "selected_patients_path = os.path.join(output_dir, \"selected_test_patients.csv\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df[test_df.patient_id.isin(patients)].to_csv(selected_patients_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = None\n",
    "# if os.path.exists(VOCAB_PATH):\n",
    "#     with open(VOCAB_PATH, \"rb\") as fp:\n",
    "#         vocab = pickle.load(fp)\n",
    "#     print(f\"vocab len: {len(vocab)}\")  # vocab + padding + unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data/toy_dataset/data/final_final/seq_based/30/vocab.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../../data/toy_dataset/data/final_final/seq_based/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/toy_dataset/data/final_final/seq_based/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/toy_dataset/data/final_final/seq_based/30/test.csv..\n",
      "Success!\n",
      "Building dataset from ./output-debug/final_final/seq_based/30/lstm-att-lrp/train_results/selected_test_patients.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "    TRAIN_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=None,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "    VALID_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "    TEST_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "example_dataset, _ = build_lstm_dataset(\n",
    "    selected_patients_path,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab._vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vocabulary Saved to ../../../data/toy_dataset/data/final_final/seq_based/30/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save vocab\n",
    "with open(VOCAB_PATH, \"wb\") as fp:\n",
    "    pickle.dump(vocab, fp)\n",
    "print(f\"Dataset Vocabulary Saved to {VOCAB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and load LRP LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # LOAD Model Parameters\n",
    "    with open(PARAMS_PATH, \"r\") as fp:\n",
    "        MODEL_PARAMS = json.load(fp)\n",
    "\n",
    "lstm_model = AttNoHtLSTM(\n",
    "    MODEL_PARAMS[\"embedding_dim\"],\n",
    "    MODEL_PARAMS[\"hidden_dim\"],\n",
    "    vocab,\n",
    "    model_device,\n",
    "    bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "    nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "    dropout=MODEL_PARAMS[\"dropout\"],\n",
    "    init_type=MODEL_PARAMS[\"init_type\"],\n",
    "    linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttNoHtLSTM(\n",
       "  (emb_layer): Embedding(47, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=False)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lstm_model.parameters(), lr=MODEL_PARAMS[\"learning_rate\"], weight_decay=0.03\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, MODEL_PARAMS[\"scheduler_step\"], gamma=0.9\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.0001, weight_decay=0.02)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 11, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm_model.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_data(dataloader, num):\n",
    "    \"\"\"Get more than one iteration of data\"\"\"\n",
    "    col_pid, col_lab, col_txt = None, None, None\n",
    "    col_num = 0\n",
    "    for pid, lab, txt in dataloader:\n",
    "        if col_pid is None:\n",
    "            col_pid, col_lab, col_txt = pid, lab, txt\n",
    "        else:\n",
    "            col_pid = tuple(list(col_pid) + list(pid))\n",
    "            col_lab = torch.cat((col_lab, lab), dim=0)\n",
    "            col_txt = torch.cat((col_txt, txt), dim=0)\n",
    "        col_num = len(col_pid)\n",
    "        if col_num > num:\n",
    "            break\n",
    "            \n",
    "    return col_pid[:num], col_lab[:num], col_txt[:num]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(95)<module>()\n",
      "-> int(token.numpy())\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  model_path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'model_path' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  model_save_path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'model_save_path' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  save_path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./output-debug/final_final/seq_based/30/lstm-att-lrp/model_weights/model_00.pkl'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  len(vocab._vocab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  vocab._vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<unk>': 1, 'backache_N': 2, 'ankle_sprain_N': 3, 'headache_N': 4, 'myopia_N': 5, 'ACL_tear_N': 6, 'cold_sore_N': 7, 'ingrown_nail_N': 8, 'congestive_heart_failure_A': 9, 'cut_finger_N': 10, 'dental_exam_N': 11, 'eye_exam_N': 12, 'annual_physical_N': 13, 'troponin_H': 14, 'hay_fever_N': 15, 'ventricular_aneurysm_A': 16, 'foot_pain_N': 17, 'peanut_allergy_N': 18, 'ARB_U': 19, 'metabolic_disorder_H': 20, 'ACE_inhibitors_U': 21, 'quad_injury_N': 22, 'coronary_artery_disease_H': 23, 'pulmonary_embolism_A': 24, 'diuretics_U': 25, 'sleep_apnea_treatment_U': 26, 'Brain_Natriuretic_Peptide_H': 27, 'catheter_ablation_U': 28, 'beta_blockers_U': 29, 'hypertension_A': 30, 'pneumonia_H': 31, 'elevated_creatinine_H': 32, 'arrhythmia_A': 33, 'pacemaker_U': 34, 'heart_valve_failure_A': 35, 'electrical_cardioversion_U': 36, 'edema_H': 37, 'alchoholism_H': 38, 'electrolyte_imbalance_H': 39, 'cardiomyopathy_A': 40, 'Chronic_Obstructive_Pulmonary_Disease_A': 41, 'sleep_apnea_H': 42, 'Acute_Myocardial_Infarction_A': 43, 'ventricular_hypertrophy_A': 44, 'Percutaneous_Coronary_Intervention_U': 45, 'cardiac_rehab_U': 46}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  one_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'one_text' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(96)<module>()\n",
      "-> for token in val_idxed_text[sel_idx]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(99)<module>()\n",
      "-> lrp_model.set_input(one_text)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  one_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 22, 10, 3, 46, 3, 2, 7, 3, 7, 22, 6, 17, 11, 10, 5, 6, 2, 17, 2, 8, 12, 26, 11]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(100)<module>()\n",
      "-> lrp_model.forward_lrp()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95  \t                int(token.numpy())\n",
      " 96  \t                for token in val_idxed_text[sel_idx]\n",
      " 97  \t                if int(token.numpy()) != 0\n",
      " 98  \t            ]\n",
      " 99  \t            lrp_model.set_input(one_text)\n",
      "100  ->\t            lrp_model.forward_lrp()\n",
      "101  \t\n",
      "102  \t            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
      "103  \t            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
      "104  \t\n",
      "105  \t            df = pd.DataFrame()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(102)<module>()\n",
      "-> Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fcf5dcfe458c>(103)<module>()\n",
      "-> R_words = np.sum(Rx + Rx_rev, axis=1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  Rx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-1.13278722e-02, -1.27554235e-03, -1.03454182e-02,\n",
      "        -5.55965928e-03, -7.24765505e-04, -1.24756839e-02,\n",
      "        -1.96285142e-02, -1.74118463e-04],\n",
      "       [ 3.60301587e-03, -8.80070009e-03, -1.30621304e-02,\n",
      "        -9.58944829e-03, -7.70964844e-03, -9.40287669e-03,\n",
      "         6.44787291e-03,  8.62244042e-04],\n",
      "       [-2.93223265e-03,  2.10013865e-04, -6.13983530e-03,\n",
      "        -1.15756701e-02, -9.18109887e-03, -2.83079090e-03,\n",
      "        -6.42079912e-03, -2.16322671e-03],\n",
      "       [-4.28666870e-03, -1.28950894e-03,  2.90009309e-03,\n",
      "        -3.78763470e-03, -2.54731950e-04, -7.14254085e-03,\n",
      "        -1.47511121e-03, -6.40425234e-04],\n",
      "       [-4.01475969e-04, -8.62899158e-03, -8.65565676e-03,\n",
      "        -1.09863527e-02,  2.80064874e-03,  5.22141610e-03,\n",
      "        -4.49924600e-03,  3.66601207e-04],\n",
      "       [-3.41862938e-02, -2.00883788e-02, -4.65421669e-02,\n",
      "        -4.28320464e-02, -2.98574329e-02, -2.14536458e-02,\n",
      "        -1.66915760e-02,  2.22623230e-04],\n",
      "       [-3.35200585e-04, -7.53443300e-03, -7.23225972e-03,\n",
      "        -9.06387876e-03,  2.33051323e-03,  4.39390525e-03,\n",
      "        -3.46162040e-03,  5.31055564e-04],\n",
      "       [ 2.39525298e-03, -3.69838151e-03, -1.10419299e-03,\n",
      "        -3.44866993e-03, -1.28387282e-02, -6.01872868e-03,\n",
      "         7.03362040e-03, -1.05758336e-03],\n",
      "       [ 1.60593115e-04,  2.76907614e-03, -9.50651032e-03,\n",
      "        -8.82145775e-03,  1.14270881e-03, -6.59003236e-03,\n",
      "        -3.31565000e-03, -2.57812401e-04],\n",
      "       [-2.62559851e-04, -5.86619212e-03, -5.63821730e-03,\n",
      "        -7.01703913e-03,  1.82188855e-03,  3.37161318e-03,\n",
      "        -2.65711106e-03,  3.85242995e-04],\n",
      "       [ 1.43333112e-04,  2.48106767e-03, -8.48484430e-03,\n",
      "        -7.87866625e-03,  1.02093008e-03, -5.85860704e-03,\n",
      "        -2.94182059e-03, -2.25626042e-04],\n",
      "       [-1.35701769e-03,  8.96396661e-05, -2.83191787e-03,\n",
      "        -4.98572825e-03, -3.89943881e-03, -1.25647688e-03,\n",
      "        -2.61372653e-03, -7.69412184e-04],\n",
      "       [ 3.38847177e-04, -4.04947093e-03, -9.54568447e-03,\n",
      "        -1.28156795e-03,  6.02603708e-04, -2.67891181e-03,\n",
      "        -2.15642090e-03, -3.87194039e-04],\n",
      "       [-3.73726938e-03, -5.36477243e-03, -1.05716767e-03,\n",
      "        -5.48699893e-03, -1.48633067e-03,  4.21585580e-03,\n",
      "        -3.40627327e-03, -4.96463301e-04],\n",
      "       [-6.55570163e-03, -4.97368792e-03,  1.55215928e-04,\n",
      "        -4.40978051e-03,  1.26511864e-03, -9.12714931e-04,\n",
      "        -3.45631176e-03,  6.92166698e-04],\n",
      "       [-1.80633750e-03, -5.37806135e-04,  1.21763663e-03,\n",
      "        -1.52758510e-03, -1.03270652e-04, -2.89654111e-03,\n",
      "        -5.47322413e-04, -2.72548730e-04],\n",
      "       [-2.46978446e-03, -3.74858508e-03, -3.82350478e-03,\n",
      "        -4.69459436e-04, -3.76891964e-03,  3.55730721e-03,\n",
      "         9.24287383e-04,  8.78900644e-04],\n",
      "       [ 2.49167622e-04, -3.10375936e-03, -7.02958555e-03,\n",
      "        -9.60350947e-04,  4.54136804e-04, -1.95552457e-03,\n",
      "        -1.58211611e-03, -2.99541263e-04],\n",
      "       [ 1.21791431e-03, -1.97854869e-03, -5.60523868e-04,\n",
      "        -1.78565989e-03, -6.72521973e-03, -2.97671258e-03,\n",
      "         3.49924746e-03, -5.12648960e-04],\n",
      "       [-2.39028873e-03, -3.62514154e-03, -6.76919683e-04,\n",
      "        -3.59941251e-03, -9.84084640e-04,  2.65567925e-03,\n",
      "        -2.18024900e-03, -3.27946112e-04],\n",
      "       [ 9.71319919e-04, -1.62202265e-03, -4.46969800e-04,\n",
      "        -1.43856028e-03, -5.45648441e-03, -2.34722372e-03,\n",
      "         2.78520315e-03, -4.23459185e-04],\n",
      "       [ 6.49779740e-04, -1.56563895e-03, -2.35595228e-03,\n",
      "        -1.65820733e-03, -1.32850902e-03, -1.56264392e-03,\n",
      "         1.01932469e-03,  1.11437494e-04],\n",
      "       [-1.99126334e-03, -8.38647196e-04,  6.82923219e-04,\n",
      "        -8.08575531e-04,  1.51920306e-03, -2.09953261e-03,\n",
      "        -3.86341976e-04, -3.38483592e-05],\n",
      "       [-4.38926312e-03, -2.86160890e-03, -6.33251875e-03,\n",
      "        -4.75905759e-03, -4.93226037e-03, -3.13947587e-03,\n",
      "        -2.66202054e-03, -3.53500057e-05],\n",
      "       [-1.36768037e-03, -1.25763791e-03,  3.26808018e-05,\n",
      "        -9.98439478e-04,  3.02661356e-04, -1.89671849e-04,\n",
      "        -7.21700904e-04,  2.50134291e-04]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  Rx.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 8)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  len(one_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  Rx[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.01132787, -0.00127554, -0.01034542, -0.00555966, -0.00072477,\n",
      "       -0.01247568, -0.01962851, -0.00017412])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fcf5dcfe458c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mRx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRx_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mR_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mRx_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-fcf5dcfe458c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mRx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRx_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mR_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mRx_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_results = {}\n",
    "test_results = {}\n",
    "\n",
    "rbo_p = 0.95\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    # Save Model Parameters\n",
    "    with open(PARAMS_PATH, \"w\") as fp:\n",
    "        json.dump(MODEL_PARAMS, fp)\n",
    "\n",
    "    train_auc_lst = []\n",
    "    train_loss_lst = []\n",
    "\n",
    "    val_auc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_lrp_sim_lst = []\n",
    "    val_shap_sim_lst = []\n",
    "    val_lrp_shap_rbo_lst = []\n",
    "    val_lrp_shap_tau_lst = []\n",
    "\n",
    "    test_auc_lst = []\n",
    "    test_loss_lst = []\n",
    "    test_lrp_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "    test_lrp_shap_rbo_lst = []\n",
    "    test_lrp_shap_tau_lst = []\n",
    "\n",
    "    val_patient_ids, val_labels, val_idxed_text = get_eval_data(\n",
    "        valid_dataloader, MODEL_PARAMS[\"num_eval_val\"])\n",
    "    \n",
    "    test_patient_ids, test_labels, test_idxed_text = get_eval_data(\n",
    "        test_dataloader, MODEL_PARAMS[\"num_eval_test\"])\n",
    "    #val_patient_ids, val_labels, val_idxed_text = next(iter(valid_dataloader))\n",
    "    #test_patient_ids, test_labels, test_idxed_text = next(iter(test_dataloader))\n",
    "\n",
    "    # patient_ids, labels, idxed_text = get_sub_valid_data(N_VALID_EXAMPLES, MODEL_PARAMS['batch_size'], valid_dataloader)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        lstm_model.train()\n",
    "        # model training & perf evaluation\n",
    "        train_loss, train_auc = epoch_train_lstm(\n",
    "            lstm_model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            clip=MODEL_PARAMS[\"clip\"],\n",
    "            device=model_device,\n",
    "        )\n",
    "        train_auc_lst.append(train_auc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "\n",
    "        valid_loss, valid_auc = epoch_val_lstm(\n",
    "            lstm_model, valid_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        val_auc_lst.append(valid_auc)\n",
    "        val_loss_lst.append(valid_loss)\n",
    "\n",
    "        test_loss, test_auc = epoch_val_lstm(\n",
    "            lstm_model, test_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        test_auc_lst.append(test_auc)\n",
    "        test_loss_lst.append(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # save model\n",
    "        save_path = MODEL_SAVE_PATH_PATTERN.format(str(epoch).zfill(2))\n",
    "        torch.save(lstm_model.state_dict(), save_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "#         print(\n",
    "#             f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "#             + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "#         )\n",
    "#         continue\n",
    "        \n",
    "        # calculate relevancy and SHAP\n",
    "        lstm_model.eval()\n",
    "        lrp_model = LSTM_LRP_MultiLayer(lstm_model.cpu())\n",
    "\n",
    "        # Save valid/test results\n",
    "        valid_results[epoch] = {}\n",
    "        test_results[epoch] = {}\n",
    "\n",
    "        for sel_idx in range(len(val_labels)):\n",
    "            import pdb; pdb.set_trace()\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in val_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if val_patient_ids[sel_idx] not in valid_results[epoch]:\n",
    "                valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"label\"] = val_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "        continue\n",
    "        for sel_idx in range(len(test_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in test_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if test_patient_ids[sel_idx] not in test_results[epoch]:\n",
    "                test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"label\"] = test_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        shap_start_time = time.time()\n",
    "        #         (\n",
    "        #             val_features,\n",
    "        #             val_scores,\n",
    "        #             val_patients,\n",
    "        #         ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "        #             lstm_model.cuda(),\n",
    "        #             train_dataloader,\n",
    "        #             valid_dataloader,\n",
    "        #             SEQ_LEN,\n",
    "        #             \"\",\n",
    "        #             save_output=False,\n",
    "        #             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "        #             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "        #             n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "        #             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "        #             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        #         )\n",
    "\n",
    "        #         (\n",
    "        #             test_features,\n",
    "        #             test_scores,\n",
    "        #             test_patients,\n",
    "        #         ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "        #             lstm_model.cuda(),\n",
    "        #             train_dataloader,\n",
    "        #             test_dataloader,\n",
    "        #             SEQ_LEN,\n",
    "        #             \"\",\n",
    "        #             save_output=False,\n",
    "        #             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "        #             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "        #             n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "        #             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "        #             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        #         )\n",
    "\n",
    "        (\n",
    "            val_features,\n",
    "            val_scores,\n",
    "            val_patients,\n",
    "        ) = get_lstm_features_and_shap_scores_mp(\n",
    "            lstm_model.cpu(),\n",
    "            train_dataloader,\n",
    "            (val_patient_ids, val_labels, val_idxed_text),\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            multigpu_lst=[\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        )\n",
    "\n",
    "        (\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_patients,\n",
    "        ) = get_lstm_features_and_shap_scores_mp(\n",
    "            lstm_model.cpu(),\n",
    "            train_dataloader,\n",
    "            (test_patient_ids, test_labels, test_idxed_text),\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            multigpu_lst=[\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        )\n",
    "\n",
    "        shap_end_time = time.time()\n",
    "        shap_mins, shap_secs = epoch_time(shap_start_time, shap_end_time)\n",
    "\n",
    "        for idx, pid in enumerate(val_patients):\n",
    "            df = valid_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(val_scores[idx])\n",
    "            df[\"shap_scores\"] = val_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        for idx, pid in enumerate(test_patients):\n",
    "            df = test_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(test_scores[idx])\n",
    "            df[\"shap_scores\"] = test_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            test_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        # calculate similarity indexes for val\n",
    "        epoch_val_lrp_shap_t_corr = []\n",
    "        epoch_val_lrp_shap_rbo = []\n",
    "        epoch_val_lrp_sim = []\n",
    "        epoch_val_shap_sim = []\n",
    "\n",
    "        for pid in valid_results[epoch].keys():\n",
    "            imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            valid_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_val_lrp_shap_t_corr.append(\n",
    "                valid_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_val_lrp_shap_rbo.append(valid_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            n_gt = len(gt_idx)\n",
    "            if n_gt > 0:\n",
    "                lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt]\n",
    "                shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "                    : n_gt\n",
    "                ]\n",
    "                lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "                shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "                epoch_val_lrp_sim.append(lrp_sim)\n",
    "                epoch_val_shap_sim.append(shap_sim)\n",
    "            else:\n",
    "                print(\"-1 is the output\")\n",
    "                lrp_sim = -1\n",
    "                shap_sim = -1\n",
    "            valid_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            valid_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(valid_results[epoch], fp)\n",
    "\n",
    "        val_lrp_shap_rbo_lst.append(np.mean(epoch_val_lrp_shap_rbo))\n",
    "        val_lrp_shap_tau_lst.append(np.mean(epoch_val_lrp_shap_t_corr))\n",
    "        val_lrp_sim_lst.append(np.mean(epoch_val_lrp_sim))\n",
    "        val_shap_sim_lst.append(np.mean(epoch_val_shap_sim))\n",
    "\n",
    "        # calculate similarity indexes for test\n",
    "        epoch_test_lrp_shap_t_corr = []\n",
    "        epoch_test_lrp_shap_rbo = []\n",
    "        epoch_test_lrp_sim = []\n",
    "        epoch_test_shap_sim = []\n",
    "\n",
    "        for pid in test_results[epoch].keys():\n",
    "            imp_df = test_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            test_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            test_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_test_lrp_shap_t_corr.append(\n",
    "                test_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_test_lrp_shap_rbo.append(test_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            n_gt = len(gt_idx)\n",
    "            if n_gt > 0:\n",
    "                lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt]\n",
    "                shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "                    : n_gt\n",
    "                ]\n",
    "                lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "                shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "                epoch_test_lrp_sim.append(lrp_sim)\n",
    "                epoch_test_shap_sim.append(shap_sim)\n",
    "            else:\n",
    "                print(\"-1 is the output\")\n",
    "                lrp_sim = -1\n",
    "                shap_sim = -1\n",
    "            test_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            test_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(test_results[epoch], fp)\n",
    "\n",
    "        test_lrp_shap_rbo_lst.append(np.mean(epoch_test_lrp_shap_rbo))\n",
    "        test_lrp_shap_tau_lst.append(np.mean(epoch_test_lrp_shap_t_corr))\n",
    "        test_lrp_sim_lst.append(np.mean(epoch_test_lrp_sim))\n",
    "        test_shap_sim_lst.append(np.mean(epoch_test_shap_sim))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | \"\n",
    "            + f\"SHAP Time: {shap_mins}m {shap_secs}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "            + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "            + f\"| Val LRP Sim: {np.mean(epoch_val_lrp_sim):.4f} | Val SHAP Sim: {np.mean(epoch_val_shap_sim):.4f}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "    df_results = pd.DataFrame()\n",
    "    df_results[\"epoch\"] = [x for x in range(N_EPOCHS)]\n",
    "    df_results[\"train_AUC\"] = train_auc_lst\n",
    "    df_results[\"train_Loss\"] = train_loss_lst\n",
    "    df_results[\"val_AUC\"] = val_auc_lst\n",
    "    df_results[\"val_Loss\"] = val_loss_lst\n",
    "    df_results[\"test_AUC\"] = test_auc_lst\n",
    "    df_results[\"test_Loss\"] = test_loss_lst\n",
    "    df_results[\"val_lrp_shap_rbo\"] = val_lrp_shap_rbo_lst\n",
    "    df_results[\"val_lrp_shap_tau\"] = val_lrp_shap_tau_lst\n",
    "    df_results[\"test_lrp_shap_rbo\"] = test_lrp_shap_rbo_lst\n",
    "    df_results[\"test_lrp_shap_tau\"] = test_lrp_shap_tau_lst\n",
    "    df_results[\"val_GT_lrp_sim\"] = val_lrp_sim_lst\n",
    "    df_results[\"val_GT_shap_sim\"] = val_shap_sim_lst\n",
    "    df_results[\"test_GT_lrp_sim\"] = test_lrp_sim_lst\n",
    "    df_results[\"test_GT_shap_sim\"] = test_shap_sim_lst\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    df_results.to_csv(OUTPUT_RESULTS_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"Loading Training results....\")\n",
    "    df_results = pd.read_csv(OUTPUT_RESULTS_PATH)\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Load valid results.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"rb\") as fp:\n",
    "            valid_results[epoch] = pickle.load(fp)\n",
    "        # Load test results.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"rb\") as fp:\n",
    "            test_results[epoch] = pickle.load(fp)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.shape)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"SHAP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_lrp_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_rbo\", \"val_lrp_shap_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs SHAP with RBO/Kendall-T on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_rbo\", \"val_lrp_shap_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"LRP/SHAP with RBO/Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sel_idx in range(len(val_labels)):\n",
    "#     one_text = [\n",
    "#         int(token.numpy())\n",
    "#         for token in val_idxed_text[sel_idx]\n",
    "#         if int(token.numpy()) != 0\n",
    "#     ]\n",
    "#     lrp_model.set_input(one_text)\n",
    "#     lrp_model.forward_lrp()\n",
    "\n",
    "#     Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "#     R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "#     df = pd.DataFrame()\n",
    "#     df[\"lrp_scores\"] = R_words\n",
    "#     df[\"idx\"] = one_text\n",
    "#     df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "#     df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "#     df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "#     if val_patient_ids[sel_idx] not in valid_results[epoch]:\n",
    "#         valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "#     valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "#     valid_results[epoch][val_patient_ids[sel_idx]][\"label\"] = val_labels[\n",
    "#         sel_idx\n",
    "#     ]\n",
    "#     valid_results[epoch][val_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "#     valid_results[epoch][val_patient_ids[sel_idx]][\"imp\"] = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "def ret_label_df(df, colname, dataset):\n",
    "    len_epoch = df.shape[0]\n",
    "    plot_df = df[['epoch', colname]].copy()\n",
    "    plot_df.columns = ['epoch', 'AUC']\n",
    "    plot_df['dataset'] = dataset\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ret_label_df(df_results.reset_index(), 'train_AUC', 'training')\n",
    "val_df = ret_label_df(df_results.reset_index(), 'val_AUC', 'validation')\n",
    "plot_df = pd.concat([train_df, val_df])\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.lineplot(\n",
    "    data=plot_df, x=\"epoch\", y=\"AUC\", hue=\"dataset\",\n",
    "    markers=True, dashes=False, style='dataset')\n",
    "ax.set_title(\"Prediction performance for Dot-Attention-LSTM model\")\n",
    "ax.set_xlabel(\"round\")\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(PLOT_SAVE_DIR, 'LSTM_auc_vs_epoch.png')\n",
    "plt.savefig(plot_path, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {'axes.grid':False}) \n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(data=df_results.reset_index(), x=\"epoch\", y=\"val_GT_shap_sim\",\n",
    "                 color='blue', alpha=0.5, edgecolor='black', linewidth=1)\n",
    "ax.set(ylim=(0.6, 1.0))\n",
    "ax.set_xlabel('round')\n",
    "ax.set_ylabel('SHAP similarity to ground truth labels', color='mediumslateblue')\n",
    "for i, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"mediumslateblue\")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(data=df_results.reset_index(), x=\"epoch\", y=\"val_AUC\", ax=ax2, \n",
    "             color='black', marker='o')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax.set_title(\"Overlap between ground truth and SHAP features on validation set\")\n",
    "ax.title.set_position([.5, 1.05])\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(PLOT_SAVE_DIR, 'LSTM_auc_vs_gt_SHAP_val.png')\n",
    "plt.savefig(plot_path, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {'axes.grid':False}) \n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(data=df_results.reset_index(), x=\"epoch\", y=\"test_GT_shap_sim\",\n",
    "                 color='blue', alpha=0.5, edgecolor='black', linewidth=1)\n",
    "ax.set(ylim=(0.6, 1.0))\n",
    "ax.set_xlabel('round')\n",
    "ax.set_ylabel('SHAP similarity to ground truth labels', color='mediumslateblue')\n",
    "for i, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"mediumslateblue\")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(data=df_results.reset_index(), x=\"epoch\", y=\"test_AUC\", ax=ax2, \n",
    "             color='black', marker='o')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax.set_title(\"Overlap between ground truth and SHAP features on test set\")\n",
    "ax.title.set_position([.5, 1.05])\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(PLOT_SAVE_DIR, 'LSTM_auc_vs_gt_SHAP_test.png')\n",
    "plt.savefig(plot_path, dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: RUN SHAP & LRP over whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best epoch first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(df, by=\"val_AUC\"):\n",
    "    \"\"\"Get best epoch based on the given dataframe and column\"\"\"\n",
    "    best_epoch = df_results[by].idxmax()\n",
    "    best_epoch = int(best_epoch)\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = get_best_epoch(df_results, by='val_AUC')\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [0, best_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "selected_patients = selected_patients.values.flatten().tolist()\n",
    "selected_patients\n",
    "# selected_patients = [\"1OD472J277\", \"XEK4OM00KJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = {}\n",
    "for pat_id in selected_patients:\n",
    "    example_results[pat_id] = {}\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if pat_id in test_results[epoch].keys():\n",
    "            example_results[pat_id][epoch] = test_results[epoch][pat_id]\n",
    "example_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_results[best_epoch].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    lrp_all_scores = []\n",
    "    shap_all_scores = []\n",
    "    attn_all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        shap_all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "        lrp_all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "        attn_all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"att_weights\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "\n",
    "    shap_global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, shap_all_scores, absolute=True)\n",
    "    lrp_global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, lrp_all_scores, absolute=True)\n",
    "\n",
    "    att_global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, attn_all_scores, absolute=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in shap_global_scores.keys():\n",
    "    rows.append([token, shap_global_scores[token], lrp_global_scores[token], att_global_scores[token]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "als_df = pd.DataFrame(rows)\n",
    "als_df.columns = ['features', 'shap_scores', 'lrp_scores', 'attention_weights']\n",
    "als_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_df.sort_values('shap_scores', ascending=True, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "axes = fig.subplots(ncols=3, sharey=True)\n",
    "axes[0].barh(\n",
    "    als_df.features, als_df.lrp_scores, color='mediumslateblue',\n",
    "    align='center', zorder=10)\n",
    "axes[0].set(title='LRP', xlabel=\"LRP scores\")\n",
    "axes[1].barh(\n",
    "    als_df.features, als_df.shap_scores, color='orange',\n",
    "    align='center')\n",
    "axes[1].set(title='SHAP', xlabel=\"SHAP scores\")\n",
    "#axes[0].invert_xaxis()\n",
    "#axes[1].set_yticks([])\n",
    "\n",
    "axes[2].barh(\n",
    "    als_df.features, als_df.attention_weights, color='deepskyblue',\n",
    "    align='center')\n",
    "axes[2].set(title='Dot-Product Attention', xlabel=\"Attention weights\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.09)\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(PLOT_SAVE_DIR, f\"LSTM_LRP_SHAP_Att_E{best_epoch}.png\")\n",
    "plt.savefig(plot_path, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'test'\n",
    "BEST_EPOCH = 5\n",
    "best_epoch = 5\n",
    "results_path = f\"./output-lrp-shap-whole/final_final/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/train_results/{DATA_SPLIT}_results_all_{BEST_EPOCH}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path, 'rb') as fp:\n",
    "    all_results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_all_shap_scores\n",
    "# n_jobs = len(all_results.keys())\n",
    "# step = int(n_jobs / 2) - 1\n",
    "# for epoch, all_scores in all_results.items():\n",
    "#     if epoch != best_epoch:\n",
    "#         continue\n",
    "#     all_features = []\n",
    "#     lrp_all_scores = []\n",
    "#     shap_all_scores = []\n",
    "#     attn_all_scores = []\n",
    "#     for pat_id, scores in all_scores.items():\n",
    "#         shap_all_scores.append(all_results[epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "#         lrp_all_scores.append(all_results[epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "#         attn_all_scores.append(all_results[epoch][pat_id][\"imp\"][\"att_weights\"].tolist())\n",
    "#         all_features.append(all_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "\n",
    "#     shap_global_scores = sj_utils.get_global_feature_importance(\n",
    "#         all_features, shap_all_scores, absolute=True)\n",
    "#     lrp_global_scores = sj_utils.get_global_feature_importance(\n",
    "#         all_features, lrp_all_scores, absolute=True)\n",
    "\n",
    "#     att_global_scores = sj_utils.get_global_feature_importance(\n",
    "#         all_features, attn_all_scores, absolute=True)\n",
    "    \n",
    "# rows = []\n",
    "# for token in shap_global_scores.keys():\n",
    "#     rows.append([token, shap_global_scores[token], lrp_global_scores[token], att_global_scores[token]])\n",
    "    \n",
    "# als_df = pd.DataFrame(rows)\n",
    "# als_df.columns = ['features', 'shap_scores', 'lrp_scores', 'attention_weights']\n",
    "\n",
    "# als_df.sort_values('lrp_scores', ascending=True, inplace=True)\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# axes = fig.subplots(ncols=3, sharey=True)\n",
    "# axes[0].barh(\n",
    "#     als_df.features, als_df.lrp_scores, color='mediumslateblue',\n",
    "#     align='center', zorder=10)\n",
    "# axes[0].set(title='LRP', xlabel=\"LRP scores\")\n",
    "# axes[1].barh(\n",
    "#     als_df.features, als_df.shap_scores, color='orange',\n",
    "#     align='center')\n",
    "# axes[1].set(title='SHAP', xlabel=\"SHAP scores\")\n",
    "# #axes[0].invert_xaxis()\n",
    "# #axes[1].set_yticks([])\n",
    "\n",
    "# axes[2].barh(\n",
    "#     als_df.features, als_df.attention_weights, color='deepskyblue',\n",
    "#     align='center')\n",
    "# axes[2].set(title='Dot-Product Attention', xlabel=\"Attention weights\")\n",
    "\n",
    "# fig.subplots_adjust(wspace=0.09)\n",
    "# plt.tight_layout()\n",
    "# # plot_path = os.path.join(PLOT_SAVE_DIR, f\"LSTM_LRP_SHAP_Att_E{best_epoch}.png\")\n",
    "# # plt.savefig(plot_path, dpi=600)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results2 = {}\n",
    "for pat_id in selected_patients:\n",
    "    example_results2[pat_id] = {}\n",
    "    for epoch in [BEST_EPOCH]:\n",
    "        if pat_id in all_results[epoch].keys():\n",
    "            example_results2[pat_id][epoch] = all_results[epoch][pat_id]\n",
    "example_results2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [best_epoch]\n",
    "for uid in example_results2.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results2[uid].keys():\n",
    "        df[epoch] = example_results2[uid][epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"token\"] = example_results2[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results2[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    # df[example_results2[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results2[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results2[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"SHAP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_results[best_epoch][pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for pid in valid_results[best_epoch].keys():\n",
    "    imp_df = valid_results[best_epoch][pid][\"imp\"]\n",
    "    \n",
    "    # gt similarity\n",
    "    gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "    n_gt = len(gt_idx)\n",
    "    if n_gt > 0:\n",
    "        att_idx = np.argsort(np.abs(imp_df.att_weights.values))[::-1][: n_gt]\n",
    "        att_sim = len(set(att_idx).intersection(gt_idx)) / n_gt\n",
    "    else:\n",
    "        print(\"-1 is the output\")\n",
    "        att_sim = -1\n",
    "    pid_data = valid_results[best_epoch][pid]\n",
    "    \n",
    "    rows.append([pid, pid_data['lrp_sim'], pid_data['shap_sim'], att_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = pd.DataFrame(rows)\n",
    "sims.columns = ['patient_id', 'lrp_sim', 'shap_sim', 'att_sim']\n",
    "sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True, figsize=(10, 5))\n",
    "sns.histplot(sims.lrp_sim, ax=ax1, binwidth=0.04, color='mediumslateblue', stat='probability')\n",
    "ax1.set_xlabel(\"similarity to ground truth\")\n",
    "ax1.set_title(\"LRP\")\n",
    "ax1.set_ylabel(\"distribution\")\n",
    "sns.histplot(sims.shap_sim, ax=ax2, binwidth=0.04, color='mediumslateblue', stat='probability')\n",
    "ax2.set_xlabel(\"similarity to ground truth\")\n",
    "ax2.set_title(\"SHAP\")\n",
    "sns.histplot(sims.att_sim, ax=ax3, binwidth=0.04, color='mediumslateblue', stat='probability')\n",
    "ax3.set_xlabel(\"similarity to ground truth\")\n",
    "ax3.set_title(\"Attention\")\n",
    "\n",
    "plt.title(\"Similarity to ground truth histogram\")\n",
    "plot_path = os.path.join(PLOT_SAVE_DIR, \"LSTM_sim_dist.png\")\n",
    "plt.savefig(plot_path, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"LRP for Epoch: \" + str(epoch))\n",
    "    sj_utils.plot_global_feature_importance(global_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"SHAP for Epoch: \" + str(epoch))\n",
    "    sj_utils.plot_global_feature_importance(global_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"SHAP SCORES for {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.axisbelow\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LSTM LRP & SHAP, LRP & SHAP & Attention scores separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    df[[\"lrp_scores\", \"shap_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    df[[\"lrp_scores\", \"shap_scores\", \"att_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP/SHAP/Attn scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot example results vertically, along with XGBoost in second portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_weights\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "    df[\"u_token\"] = [str(seq)+\"_\"+tok for tok, seq in zip(df.token, df.seq_idx)]\n",
    "    \n",
    "    label = example_results[uid][best_epoch]['label'][0]\n",
    "    pred_prob = example_results[uid][best_epoch]['pred']\n",
    "\n",
    "    figsize = (12, 10) \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    axes = fig.subplots(nrows=3, ncols=1)\n",
    "    \n",
    "    methods = ['shap_scores', 'lrp_scores', 'att_weights']\n",
    "    for row, color, method in zip(\n",
    "        axes, ['mediumslateblue', 'orange', 'deepskyblue'], methods):\n",
    "        row.bar(\n",
    "            df['u_token'], df[method], 0.8, label=method, color=color, edgecolor='black')\n",
    "        if method != methods[-1]:\n",
    "            row.set_xticks([])\n",
    "        else:\n",
    "            row.set_xticklabels(df['token'].tolist(), rotation=90)\n",
    "        row.grid(axis='y')\n",
    "        row.set_ylim((df[method].min()-0.1, df[method].max()+0.1))\n",
    "        row.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    prob = float(torch.sigmoid(torch.Tensor([pred_prob])))\n",
    "    st = fig.suptitle(f\"explainability for {uid}: label={label}, pred={prob:.4}\", \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    st.set_y(1.02)\n",
    "    fig.subplots_adjust(top=0.9) \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(PLOT_SAVE_DIR, \"LSTM_perpatient_\" + uid + f\"_label{label}.png\")\n",
    "    plt.savefig(plot_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(XGB_BEST_SHAP_PATH, 'rb') as f:\n",
    "    xgb_test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_results[uid].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    ha,wa = imga.shape[:2]\n",
    "    hb,wb = imgb.shape[:2]\n",
    "    max_width = np.max([wa, wb])\n",
    "    total_height = ha+hb\n",
    "    new_img = np.zeros(shape=(total_height, max_width, 3))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[ha:ha+hb,:wa]=imgb\n",
    "    return new_img\n",
    "\n",
    "def concat_n_images(image_path_list):\n",
    "    \"\"\"\n",
    "    Combines N color images from a list of image paths.\n",
    "    \"\"\"\n",
    "    output = None\n",
    "    for i, img_path in enumerate(image_path_list):\n",
    "        img = plt.imread(img_path)[:,:,:3]\n",
    "        if i==0:\n",
    "            output = img\n",
    "        else:\n",
    "            output = concat_images(output, img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"lstm_shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_weights\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "    df[\"u_token\"] = [str(seq)+\"_\"+tok for tok, seq in zip(df.token, df.seq_idx)]\n",
    "    \n",
    "    \n",
    "    xgb_imp = []\n",
    "    for token in df.token.tolist():\n",
    "        idx = xgb_test_results[uid]['features_xgb'].index(token)\n",
    "        xgb_imp.append(xgb_test_results[uid]['xgb_shap'][idx])\n",
    "    df['xgb_shap_scores'] = xgb_imp\n",
    "    \n",
    "    shap_df = pd.DataFrame()\n",
    "    shap_df['token'] = xgb_test_results[uid]['features_xgb']\n",
    "    shap_df['token'] = ['1_' + tok if tok in df.token.values else '0_' + tok for tok in shap_df.token]\n",
    "    shap_df['xgb_shap_scores'] = xgb_test_results[uid]['xgb_shap']\n",
    "    \n",
    "    label = example_results[uid][best_epoch]['label'][0]\n",
    "    pred_prob = example_results[uid][best_epoch]['pred']\n",
    "\n",
    "    figsize = (12, 10) \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    axes = fig.subplots(nrows=4, ncols=1)\n",
    "    \n",
    "    methods = ['lstm_shap_scores', 'lrp_scores', 'att_weights', 'xgb_shap_scores']\n",
    "    for row, color, method in zip(\n",
    "        axes, ['mediumslateblue', 'orange', 'deepskyblue', 'limegreen'], methods):\n",
    "        row.bar(\n",
    "            df['u_token'], df[method], 0.8, label=method, color=color, edgecolor='black')\n",
    "        if method != methods[-1]:\n",
    "            row.set_xticks([])\n",
    "        else:\n",
    "            row.set_xticklabels(df['token'].tolist(), rotation=90)\n",
    "        row.grid(axis='y')\n",
    "        row.set_ylim((df[method].min()-0.1, df[method].max()+0.1))\n",
    "        row.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    prob = float(torch.sigmoid(torch.Tensor([pred_prob])))\n",
    "    st = fig.suptitle(f\"explainability for {uid}: label={label}, lstm_pred={prob:.4}\", fontsize=14, \n",
    "                      fontweight='bold')\n",
    "    #st.set_y(0.93)\n",
    "    st.set_y(1.02)\n",
    "    fig.subplots_adjust(top=0.9) \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(PLOT_SAVE_DIR, \"XGB_LSTM_perpatient_\" + uid + \".png\")\n",
    "    plt.savefig(plot_path, dpi=600, bbox_inches='tight')\n",
    "        \n",
    "    \n",
    "    figsize = (12, 8) \n",
    "    fig2 = plt.figure(figsize=figsize)\n",
    "    axes2 = fig2.subplots(nrows=1, ncols=1)\n",
    "    axes2.bar(shap_df.token, shap_df.xgb_shap_scores, color='limegreen', edgecolor='black')\n",
    "    axes2.set_xticklabels(shap_df.token, rotation=90)\n",
    "    \n",
    "    plt.title(f\"explainability for XGBoost only, xgb_pred={xgb_test_results[uid]['xgb_pred']:.4}\", \n",
    "              fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plot_path = os.path.join(PLOT_SAVE_DIR, \"XGB_perpatient_\" + uid + \".png\")\n",
    "    plt.savefig(plot_path, dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = concat_n_images([lstm_path, xgb_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(12, 18))\n",
    "#ax.imshow(output)\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
