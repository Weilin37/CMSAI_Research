{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###<NOT YET UPDATED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model Training and SHAP computation using the Synthetic-events Dataset\n",
    "\n",
    "**Author: Tesfagabir Meharizghi<br>Last Updated: 01/07/2021**\n",
    "\n",
    "This notebook does the following actions:\n",
    "- Model training using the given parameters\n",
    "- Model selection using Intersection Similarity Score between ground truth helping features and predicted ones\n",
    "    * Early stopping using Intersection similarity score criteria\n",
    "- Computes SHAP values and visualizes for a few examples\n",
    "- Visualizes the train/val/test probability scores from each trained model\n",
    "- Visualizes the Intersection Similarity Scores for val/test splits\n",
    "- Finally, after tweaking the parameters, it gets the best model for the given model architecture and dataset\n",
    "\n",
    "Outputs:\n",
    "- The following artifacts are saved:\n",
    "    * Model artifacts\n",
    "    * SHAP values and their corresponding scores for the specified number of val/test examples\n",
    "\n",
    "Model Architecture Used:\n",
    "- XGB\n",
    "\n",
    "Dataset:\n",
    "- Synthetic-events (Toy Dataset)\n",
    "\n",
    "Requirements:\n",
    "- Make sure that you have already generated the synthetic toy dataset (train/val/test splits) using [Create_toy_dataset.ipynb](../../data/toy_dataset/Create_toy_dataset.ipynb).\n",
    "\n",
    "Next Steps:\n",
    "- Once you train different models, save the best one you found\n",
    "- Do also the same for other models architectures (SimpleLSTM, XGB, etc.) using the separate notebooks\n",
    "- Finally, go to [this ipynb]() to compare to compare the models' performances and shap values usig Jaccard Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "import xgboost_utils as xgb_utils\n",
    "import shap_jacc_utils as sj_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+Attention Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1000#1e9 #TODO\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "batch_size = 64  # For model training\n",
    "\n",
    "n_epochs = 10\n",
    "stop_num = 2\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "nlayers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.2\n",
    "\n",
    "target_colname = \"label\"\n",
    "uid_colname = \"patient_id\"\n",
    "target_value = \"1\"\n",
    "\n",
    "rev = False\n",
    "\n",
    "model_name = \"xgb\"\n",
    "dataset = 'synthetic-events'\n",
    "\n",
    "# For model early stopping criteria\n",
    "EARLY_STOPPING = \"intersection_similarity\"  # Values are any of these: ['intersection_similarity', 'loss']\n",
    "\n",
    "# SHAP related constants\n",
    "N_BACKGROUND = 500  # Number of background examples\n",
    "BACKGROUND_NEGATIVE_ONLY = True  # If negative examples are used as background\n",
    "N_VALID_EXAMPLES = 32  # Number of validation examples to be used during model training\n",
    "N_TEST_EXAMPLES = 64  # Number of test examples\n",
    "TEST_POSITIVE_ONLY = True  # If only positive examples are selected\n",
    "IS_TEST_RANDOM = (\n",
    "    True  # If random test/val examples are selected for shap value computation\n",
    ")\n",
    "SORT_SHAP_VALUES = False  # Whether to sort per-patient shap values for visualization\n",
    "\n",
    "train_data_path = f\"../../data/toy_dataset/data/{seq_len}/train.csv\"\n",
    "valid_data_path = f\"../../data/toy_dataset/data/{seq_len}/val.csv\"\n",
    "test_data_path = f\"../../data/toy_dataset/data/{seq_len}/test.csv\"\n",
    "\n",
    "model_save_path = \"./output/{}/{}/models/model_{}.pkl\".format(seq_len, model_name, \"{}\")\n",
    "shap_save_path = \"./output/{}/{}/shap/{}_shap_{}.pkl\".format(\n",
    "    seq_len, model_name, \"{}\", \"{}\"\n",
    ")  # SHAP values path for a given dataset split (train/val/test) (data format (features, scores, patient_ids))\n",
    "\n",
    "# Dataset preprocessing\n",
    "x_train_one_hot_path = f\"output/{seq_len}/{model_name}/data/train_one_hot.csv\"\n",
    "x_valid_one_hot_path = f\"output/{seq_len}/{model_name}/data/val_one_hot.csv\"\n",
    "x_test_one_hot_path = f\"output/{seq_len}/{model_name}/data/test_one_hot.csv\"\n",
    "\n",
    "x_train_data_path = f\"output/{seq_len}/{model_name}/data/train.csv\"\n",
    "x_valid_data_path = f\"output/{seq_len}/{model_name}/data/val.csv\"\n",
    "x_test_data_path = f\"output/{seq_len}/{model_name}/data/test.csv\"\n",
    "\n",
    "s3_output_data_dir = f\"s3://merck-paper-bucket/{dataset}/{seq_len}/data\"\n",
    "\n",
    "\n",
    "#Model training\n",
    "BUCKET = \"merck-paper-bucket\"\n",
    "DATA_PREFIX = f\"{dataset}/{seq_len}/data\"\n",
    "MODEL_PREFIX = f\"{dataset}/{seq_len}/{model_name}\".format(seq_len)\n",
    "label = \"label\"\n",
    "\n",
    "output_results_path = f\"output/{seq_len}/{model_name}/train/train_results.csv\"\n",
    "local_model_dir = f\"output/{seq_len}/{model_name}/models/\"\n",
    "s3_output_path = f\"s3://{BUCKET}/{MODEL_PREFIX}/output\"\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = \"xgboost\"\n",
    "REPO_VERSION = \"1.2-1\"\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = \"ml.m5.4xlarge\"  #'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 1  # 4 #TODO: Remove\n",
    "MAX_TRAIN_JOBS = 1  # 20\n",
    "\n",
    "EVALUATION_METRIC = \"auc\"\n",
    "OBJECTIVE = \"binary:logistic\"\n",
    "OBJECTIVE_METRIC_NAME = \"validation:auc\"\n",
    "\n",
    "# Update hyperparameter ranges\n",
    "# HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0, 1),\n",
    "#                         'alpha': ContinuousParameter(0, 2),\n",
    "#                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "HYPERPARAMETER_RANGES = {\n",
    "    \"eta\": ContinuousParameter(0.1, 0.5),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"gamma\": ContinuousParameter(0, 5),\n",
    "    \"num_round\": IntegerParameter(200, 500),\n",
    "    \"colsample_bylevel\": ContinuousParameter(0.1, 1.0),\n",
    "    \"colsample_bynode\": ContinuousParameter(0.1, 1.0),\n",
    "    \"colsample_bytree\": ContinuousParameter(0.5, 1.0),\n",
    "    \"lambda\": ContinuousParameter(0, 1000),\n",
    "    \"max_delta_step\": IntegerParameter(0, 10),\n",
    "    \"min_child_weight\": ContinuousParameter(0, 120),\n",
    "    \"subsample\": ContinuousParameter(0.5, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output/30/xgb/models\n",
      "New directory created: ./output/30/xgb/shap\n"
     ]
    }
   ],
   "source": [
    "# LSTM+Attention Model Output Directory\n",
    "model_save_dir = os.path.dirname(model_save_path)\n",
    "shap_save_dir = os.path.dirname(shap_save_path)\n",
    "if os.path.exists(model_save_dir):\n",
    "    # Remove model save directory if exists\n",
    "    shutil.rmtree(model_save_dir)\n",
    "if os.path.exists(shap_save_dir):\n",
    "    # Remove model save directory if exists\n",
    "    shutil.rmtree(shap_save_dir)\n",
    "os.makedirs(model_save_dir)\n",
    "os.makedirs(shap_save_dir)\n",
    "print(f\"New directory created: {model_save_dir}\")\n",
    "print(f\"New directory created: {shap_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>29</th>\n",
       "      <th>28</th>\n",
       "      <th>27</th>\n",
       "      <th>26</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>23</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2741</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>apnea_H</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>high_creatinine_H</td>\n",
       "      <td>high_creatinine_H</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>1</td>\n",
       "      <td>YYZP9V6PWV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1615</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>apnea_H</td>\n",
       "      <td>normal_bmi_U</td>\n",
       "      <td>...</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>furosemide_H</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>1</td>\n",
       "      <td>4VX2RBGZE0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>852</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>...</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>0</td>\n",
       "      <td>6U018NSHLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2749</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>...</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>tachycardia_H</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>1</td>\n",
       "      <td>CLEDNASY68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2090</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>1</td>\n",
       "      <td>XV86JBVK2M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     29     28          27              26          25          24  \\\n",
       "0   2741  <pad>  <pad>       <pad>           <pad>       <pad>       <pad>   \n",
       "1   1615  <pad>  <pad>       <pad>        myopia_N    myopia_N  headache_N   \n",
       "2    852  <pad>  <pad>       <pad>           <pad>       <pad>       <pad>   \n",
       "3   2749  <pad>  <pad>  eye_exam_N  ingrown_nail_N  headache_N    myopia_N   \n",
       "4   2090  <pad>  <pad>       <pad>           <pad>       <pad>       <pad>   \n",
       "\n",
       "           23              22            21  ...            7              6  \\\n",
       "0       <pad>           <pad>         <pad>  ...      apnea_H     headache_N   \n",
       "1    myopia_N         apnea_H  normal_bmi_U  ...  foot_pain_N     headache_N   \n",
       "2  eye_exam_N  ankle_sprain_N      myopia_N  ...  cold_sore_N    hay_fever_N   \n",
       "3  headache_N  ingrown_nail_N  cut_finger_N  ...     myopia_N  tachycardia_H   \n",
       "4       <pad>           <pad>         <pad>  ...  cold_sore_N    foot_pain_N   \n",
       "\n",
       "             5                  4                  3             2  \\\n",
       "0  cold_sore_N  high_creatinine_H  high_creatinine_H      myopia_N   \n",
       "1   backache_N        foot_pain_N        hay_fever_N  furosemide_H   \n",
       "2   headache_N        foot_pain_N        cold_sore_N   cold_sore_N   \n",
       "3  cold_sore_N           myopia_N         backache_N  cut_finger_N   \n",
       "4   eye_exam_N         backache_N           myopia_N  cut_finger_N   \n",
       "\n",
       "             1             0 label  patient_id  \n",
       "0  cold_sore_N  cut_finger_N     1  YYZP9V6PWV  \n",
       "1   ACL_tear_N    ACL_tear_N     1  4VX2RBGZE0  \n",
       "2  hay_fever_N    backache_N     0  6U018NSHLS  \n",
       "3  hay_fever_N    headache_N     1  CLEDNASY68  \n",
       "4   ACL_tear_N   hay_fever_N     1  XV86JBVK2M  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_tokens(df, seq_len):\n",
    "    feature_cols = [str(i) for i in range(seq_len-1, -1, -1)]\n",
    "    tokens = list(set(df[feature_cols].values.flatten().tolist()))\n",
    "    pad = '<pad>'\n",
    "    if pad in tokens:\n",
    "        tokens.remove('<pad>')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_valid_tokens(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess!\n",
      "Sucess!\n",
      "Sucess!\n"
     ]
    }
   ],
   "source": [
    "prepare_data(\n",
    "    train_data_path,\n",
    "    x_train_one_hot_path,\n",
    "    x_train_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    tokens,\n",
    "    s3_output_data_dir,\n",
    ")\n",
    "prepare_data(\n",
    "    valid_data_path,\n",
    "    x_valid_one_hot_path,\n",
    "    x_valid_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    tokens,\n",
    "    s3_output_data_dir,\n",
    ")\n",
    "prepare_data(\n",
    "    test_data_path,\n",
    "    x_test_one_hot_path,\n",
    "    x_test_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    tokens,\n",
    "    s3_output_data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for seq_len=30, label=label...\n",
      "....................................................!\n",
      "Total jobs completed: 1\n",
      "Metric: validation:auc\n",
      "Best AUC: 0.9015\n",
      "Success! Total training time=4.404632727305095 mins.\n",
      "ALL SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Training for seq_len={}, label={}...\".format(seq_len, label))\n",
    "# Prepare the input train & validation data path\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/train\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/val\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    ")\n",
    "\n",
    "# Class Imbalance\n",
    "scale_pos_weight = 1.0  # negative/positive\n",
    "\n",
    "data_channels = {\"train\": s3_input_train, \"validation\": s3_input_validation}\n",
    "\n",
    "tuner = train_hpo(\n",
    "    hyperparameter_ranges=HYPERPARAMETER_RANGES,\n",
    "    container=container,\n",
    "    execution_role=role,\n",
    "    instance_count=TRAIN_INSTANCE_COUNT,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,\n",
    "    output_path=s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    eval_metric=EVALUATION_METRIC,\n",
    "    objective=OBJECTIVE,\n",
    "    objective_metric_name=OBJECTIVE_METRIC_NAME,\n",
    "    max_train_jobs=MAX_TRAIN_JOBS,\n",
    "    max_parallel_jobs=MAX_PARALLEL_JOBS,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    data_channels=data_channels,\n",
    ")\n",
    "\n",
    "# Get the hyperparameter tuner status at regular interval\n",
    "val_auc, best_model_path = get_tuner_status_and_result_until_completion(\n",
    "    tuner, seq_len, label\n",
    ")\n",
    "\n",
    "result = [label, seq_len, val_auc, best_model_path]\n",
    "training_results = [result]\n",
    "\n",
    "print(\"Success! Total training time={} mins.\".format((time.time() - start) / 60.0))\n",
    "# Save the results to file\n",
    "df_results = pd.DataFrame(\n",
    "    training_results, columns=[\"class\", \"seq_len\", \"val_auc\", \"best_model_path\"]\n",
    ")\n",
    "\n",
    "if not os.path.isdir(os.path.split(output_results_path)[0]):\n",
    "    os.makedirs(os.path.split(output_results_path)[0])\n",
    "\n",
    "df_results.to_csv(output_results_path, index=False)\n",
    "print(\"ALL SUCCESS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_models = len(models_paths)\n",
    "for i, model_path in enumerate(models_paths):\n",
    "    print(f\"Processing for model {os.path.basename(model_path)} ...\")\n",
    "    # Load trained weights\n",
    "    print(\"Loading the trained weights...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    ##Get Train/Val/Test Scores\n",
    "    print(\"Computing the models performances for train/val/test splits...\")\n",
    "    train_loss, train_auc, train_labels, train_scores = l_utils.epoch_val_lstm(\n",
    "        model, train_dataloader, loss_function, return_preds=True\n",
    "    )\n",
    "    val_loss, val_auc, val_labels, val_scores = l_utils.epoch_val_lstm(\n",
    "        model, valid_dataloader, loss_function, return_preds=True\n",
    "    )\n",
    "    test_loss, test_auc, test_labels, test_scores = l_utils.epoch_val_lstm(\n",
    "        model, test_dataloader, loss_function, return_preds=True\n",
    "    )\n",
    "    print(\"Ploting Histograms of Train/Val/Test Predicted Scores...\")\n",
    "    _, axes = plt.subplots(1, 3, sharex=False, figsize=(15, 5))\n",
    "    # Train\n",
    "    scores = train_scores.flatten().tolist()\n",
    "    axes = sj_utils.plot_histogram(\n",
    "        scores,\n",
    "        title=f\"Train Scores (Loss={train_loss:.4f}, AUC={train_auc:.4f})\",\n",
    "        xlabel=\"Prediction Scores\",\n",
    "        ylabel=\"Frequencies\",\n",
    "        axes=axes,\n",
    "        axes_idx=0,\n",
    "    )\n",
    "    # Val\n",
    "    scores = val_scores.flatten().tolist()\n",
    "    axes = sj_utils.plot_histogram(\n",
    "        scores,\n",
    "        title=f\"Val Scores (Loss={val_loss:.4f}, AUC={val_auc:.4f})\",\n",
    "        xlabel=\"Prediction Scores\",\n",
    "        ylabel=\"\",\n",
    "        axes=axes,\n",
    "        axes_idx=1,\n",
    "    )\n",
    "    # Test\n",
    "    scores = test_scores.flatten().tolist()\n",
    "    axes = sj_utils.plot_histogram(\n",
    "        scores,\n",
    "        title=f\"Test Scores (Loss={test_loss:.4f}, AUC={test_auc:.4f})\",\n",
    "        xlabel=\"Prediction Scores\",\n",
    "        ylabel=\"\",\n",
    "        axes=axes,\n",
    "        axes_idx=2,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Computing SHAP for {N_VALID_EXAMPLES} positive val examples...\")\n",
    "    epoch = sj_utils.get_epoch_number_from_path(model_path)\n",
    "    val_shap_path = shap_save_path.format(\"val\", f\"{epoch:02}\")\n",
    "    (\n",
    "        features,\n",
    "        scores,\n",
    "        patients,\n",
    "    ) = sj_utils.load_pickle(val_shap_path)\n",
    "\n",
    "    for idx in range(N_VALID_EXAMPLES):\n",
    "        if idx > 2:\n",
    "            break\n",
    "        features1 = features[idx]\n",
    "        scores1 = scores[idx]\n",
    "        patient_id = patients[idx]\n",
    "\n",
    "        df_shap = pd.DataFrame(\n",
    "            np.array([features1, scores1]).T, columns=[\"events\", \"shap_vals\"]\n",
    "        )\n",
    "        df_shap[\"shap_vals\"] = pd.to_numeric(df_shap[\"shap_vals\"])\n",
    "\n",
    "        sj_utils.plot_shap_values(\n",
    "            df_shap, patient_id, sort=SORT_SHAP_VALUES, figsize=(10, 5)\n",
    "        )\n",
    "\n",
    "    print(\"Computing Intersection Similarity...\")\n",
    "    avg_sim, sim = sj_utils.get_model_intersection_similarity((features, scores))\n",
    "    sj_utils.plot_histogram(\n",
    "        sim,\n",
    "        title=f\"Average Intersection Simi={avg_sim:.4f}\",\n",
    "        xlabel=\"Intersection Similarity\",\n",
    "        ylabel=\"Frequencies\",\n",
    "        axes=None,\n",
    "    )\n",
    "\n",
    "    # For the best model, get the final performance (test set) (intersection similarity)\n",
    "    if i == (total_models - 1):\n",
    "        print(\n",
    "            f\"Computing SHAP for {N_TEST_EXAMPLES} positive TEST examples for the final model...\"\n",
    "        )\n",
    "        test_shap_path = shap_save_path.format(\"test\", f\"{epoch:02}\")\n",
    "        (features, scores, patients,) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "            seq_len,\n",
    "            test_shap_path,\n",
    "            save_output=True,\n",
    "            n_background=N_BACKGROUND,\n",
    "            background_negative_only=BACKGROUND_NEGATIVE_ONLY,\n",
    "            n_test=N_TEST_EXAMPLES,\n",
    "            test_positive_only=TEST_POSITIVE_ONLY,\n",
    "            is_test_random=IS_TEST_RANDOM,\n",
    "        )\n",
    "\n",
    "        for idx in range(N_TEST_EXAMPLES):\n",
    "            if idx > 2:\n",
    "                break\n",
    "            features1 = features[idx]\n",
    "            scores1 = scores[idx]\n",
    "            patient_id = patients[idx]\n",
    "\n",
    "            df_shap = pd.DataFrame(\n",
    "                np.array([features1, scores1]).T,\n",
    "                columns=[\"events\", \"shap_vals\"],\n",
    "            )\n",
    "            df_shap[\"shap_vals\"] = pd.to_numeric(df_shap[\"shap_vals\"])\n",
    "\n",
    "            sj_utils.plot_shap_values(\n",
    "                df_shap, patient_id, sort=SORT_SHAP_VALUES, figsize=(10, 5)\n",
    "            )\n",
    "\n",
    "        print(\"Computing Intersection Similarity...\")\n",
    "        avg_sim, sim = sj_utils.get_model_intersection_similarity((features, scores))\n",
    "        sj_utils.plot_histogram(\n",
    "            sim,\n",
    "            title=f\"Average Intersection Simi={avg_sim:.4f}\",\n",
    "            xlabel=\"Intersection Similarity\",\n",
    "            ylabel=\"Frequencies\",\n",
    "            axes=None,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Finally computing and visualizing the global feature importance of the best model....\"\n",
    "        )\n",
    "        feat_importance = sj_utils.get_global_feature_importance(features, scores)\n",
    "        sj_utils.plot_global_feature_importance(feat_importance)\n",
    "        print(\"All tasks SUCCESSFULLY completed!\")\n",
    "\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
