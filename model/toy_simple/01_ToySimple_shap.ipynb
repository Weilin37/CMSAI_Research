{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! jupyter nbextension enable jupyter-black-master/jupyter-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 11),\n",
       " (2, 12),\n",
       " (3, 13),\n",
       " (4, 14),\n",
       " (5, 15),\n",
       " (6, 16),\n",
       " (7, 17),\n",
       " (8, 18),\n",
       " (9, 19),\n",
       " (10, 20)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 11), (2, 12), (3, 13), (4, 14), (5, 15), (6, 16), (7, 17), (8, 18), (9, 19), (10, 20)]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def add(args):\n",
    "    x, y = args\n",
    "    return x+y\n",
    "\n",
    "x_range = range(1, 11)\n",
    "y_range = range(11, 21)\n",
    "\n",
    "x_range = range(1, 11)\n",
    "y_range = range(11, 21)\n",
    "params = list(zip(x_range, y_range))\n",
    "print(params)\n",
    "pool = Pool(os.cpu_count())      # Create a multiprocessing Pool\n",
    "outputs = pool.map(add, params)  # process data_inputs iterable with pool\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 14, 16, 18, 20, 22, 24, 26, 28, 30]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1000\n",
    "min_freq = 1\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/{}/train.csv\".format(seq_len)\n",
    "valid_data_path = \"../../data/toy_dataset/data/{}/val.csv\".format(seq_len)\n",
    "test_data_path = \"../../data/toy_dataset/data/{}/test.csv\".format(seq_len)\n",
    "\n",
    "model_save_path = './output/{}/lstm/model'.format(seq_len)\n",
    "results_save_path = \"./output/{}/lstm/results\".format(seq_len)\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 10\n",
    "stop_num = 6\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "nlayers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "target_colname = 'label'\n",
    "uid_colname = 'patient_id'\n",
    "x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = '1'\n",
    "\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "for fp in [model_save_path, results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../data/toy_dataset/data/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "                                train_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=None,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "                                valid_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "                                test_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-01afccb0878e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/lstm_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, emb_dim, hidden_dim, vocab, device, nlayers, bidi, use_gpu, pad_idx, dropout)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     ):\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "model = SimpleLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "       model, valid_dataloader, loss_function)#, return_preds=False\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   model, test_dataloader, loss_function)#, return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index of the patient where you get compute shap\n",
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = next(iter(train_dataloader))\n",
    "background_ids, background_labels, background_idxes = background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_idxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data, bg_masks = model.get_all_ids_masks(background_idxes, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = deep_id_pytorch.CustomPyTorchDeepIDExplainer(model, bg_data, bg_masks,\n",
    "                                                         gpu_memory_efficient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # in case that shap complains that autograd cannot be called\n",
    "lstm_values = []\n",
    "features = []\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = list(test_dataloader)[3]\n",
    "test = next(iter(test_dataloader))\n",
    "test_ids, test_labels, test_idxes = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_masks = model.get_all_ids_masks(test_idxes, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shap_values = explainer.shap_values(test_data, test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_shap, patient_id = get_per_patient_shap(lstm_shap_values, test, model.vocab, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_values(df_lstm_shap, patient_id, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot_path = 'output/{}/xgboost/data/train_one_hot.csv'.format(seq_len)\n",
    "x_valid_one_hot_path = 'output/{}/xgboost/data/val_one_hot.csv'.format(seq_len)\n",
    "x_test_one_hot_path = 'output/{}/xgboost/data/test_one_hot.csv'.format(seq_len)\n",
    "\n",
    "x_train_data_path = 'output/{}/xgboost/data/train.csv'.format(seq_len)\n",
    "x_valid_data_path = 'output/{}/xgboost/data/val.csv'.format(seq_len)\n",
    "x_test_data_path = 'output/{}/xgboost/data/test.csv'.format(seq_len)\n",
    "\n",
    "s3_output_data_dir = 's3://merck-paper-bucket/{}/data'.format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_tokens(tokens):\n",
    "    \"\"\"Get all tokens except <pad> and <unk>\"\"\"\n",
    "    my_tokens = []\n",
    "    for key, val in tokens.items():\n",
    "        if val>=2:\n",
    "            my_tokens.append(key)\n",
    "    my_tokens\n",
    "    return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.vocab._vocab\n",
    "my_tokens = get_valid_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data(train_data_path, x_train_one_hot_path, x_train_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(valid_data_path, x_valid_one_hot_path, x_valid_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(test_data_path, x_test_one_hot_path, x_test_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'merck-paper-bucket'\n",
    "DATA_PREFIX = '{}/data'.format(seq_len)\n",
    "MODEL_PREFIX = '{}/xgboost/model'.format(seq_len)\n",
    "label = 'label'\n",
    "\n",
    "output_results_path = 'output/{}/xgboost/models/train_results.csv'.format(seq_len)\n",
    "local_model_dir = 'output/{}/xgboost/models/'\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, MODEL_PREFIX)\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = 'xgboost'\n",
    "REPO_VERSION = '1.2-1'\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = 'ml.m5.4xlarge'#'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 1#4 #TODO: Remove\n",
    "MAX_TRAIN_JOBS = 1#20\n",
    "\n",
    "EVALUATION_METRIC = 'auc'\n",
    "OBJECTIVE = 'binary:logistic'\n",
    "OBJECTIVE_METRIC_NAME = 'validation:auc'\n",
    "\n",
    "#Update hyperparameter ranges\n",
    "# HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0, 1),\n",
    "#                         'alpha': ContinuousParameter(0, 2),\n",
    "#                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0.1, 0.5),\n",
    "                       'alpha': ContinuousParameter(0, 2),\n",
    "                       'max_depth': IntegerParameter(1, 10),\n",
    "                       'gamma': ContinuousParameter(0, 5),\n",
    "                       'num_round': IntegerParameter(200, 500),\n",
    "                       'colsample_bylevel': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bynode': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "                       'lambda': ContinuousParameter(0, 1000),\n",
    "                       'max_delta_step': IntegerParameter(0, 10),\n",
    "                       'min_child_weight': ContinuousParameter(0, 120),\n",
    "                       'subsample': ContinuousParameter(0.5, 1.0),\n",
    "                       }\n",
    "\n",
    "\n",
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "start = time.time()\n",
    "print('Training for seq_len={}, label={}...'.format(seq_len, label))\n",
    "#Prepare the input train & validation data path\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/val'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "\n",
    "#Class Imbalance\n",
    "scale_pos_weight = 1.0 # negative/positive\n",
    "\n",
    "data_channels = {'train': s3_input_train, 'validation': s3_input_validation}\n",
    "\n",
    "tuner = train_hpo(hyperparameter_ranges=HYPERPARAMETER_RANGES, \n",
    "                  container=container, \n",
    "                  execution_role=role, \n",
    "                  instance_count=TRAIN_INSTANCE_COUNT, \n",
    "                  instance_type=TRAIN_INSTANCE_TYPE, \n",
    "                  output_path=s3_output_path, \n",
    "                  sagemaker_session=sess, \n",
    "                  eval_metric=EVALUATION_METRIC, \n",
    "                  objective=OBJECTIVE, \n",
    "                  objective_metric_name=OBJECTIVE_METRIC_NAME, \n",
    "                  max_train_jobs=MAX_TRAIN_JOBS, \n",
    "                  max_parallel_jobs=MAX_PARALLEL_JOBS, \n",
    "                  scale_pos_weight=scale_pos_weight, \n",
    "                  data_channels=data_channels)\n",
    "\n",
    "#Get the hyperparameter tuner status at regular interval\n",
    "val_auc, best_model_path = get_tuner_status_and_result_until_completion(tuner, seq_len, label)\n",
    "\n",
    "result = [label, seq_len, val_auc, best_model_path]\n",
    "training_results = [result]\n",
    "\n",
    "print('Success! Total training time={} mins.'.format((time.time()-start)/60.0))\n",
    "#Save the results to file\n",
    "df_results = pd.DataFrame(training_results, columns=['class', 'seq_len', 'val_auc', 'best_model_path'])\n",
    "\n",
    "if not os.path.isdir(os.path.split(output_results_path)[0]):\n",
    "    os.makedirs(os.path.split(output_results_path)[0])\n",
    "\n",
    "df_results.to_csv(output_results_path, index=False)\n",
    "print('ALL SUCCESS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_results_path = 'output/{}/xgboost/models/train_results.csv'.format(seq_len)\n",
    "local_model_dir = 'output/{}/xgboost/models/'.format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(x_train_one_hot_path)\n",
    "df_test = pd.read_csv(x_test_one_hot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 1:-1]\n",
    "X_test = df_test.iloc[:, 1:-1]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = pd.read_csv(output_results_path)\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_best_model_path = df_best.iloc[0]['best_model_path']\n",
    "s3_best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the best model from s3 to local\n",
    "output_path = copy_model_from_s3(s3_best_model_path, local_model_dir)\n",
    "#Load the copied model\n",
    "xgb_model = load_model(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "xgb_shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = X_test.columns.tolist()\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_shap_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = df_test.patient_id[idx]\n",
    "pat_shap_values = xgb_shap_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_shap.reindex(df_lstm_shap.shap_vals.abs().sort_values(ascending=False).index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reindex(df.shap_vals.abs().sort_values(ascending=False).index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([events, pat_shap_values]).T, columns=['events', 'shap_vals'])\n",
    "df[\"shap_vals\"] = pd.to_numeric(df[\"shap_vals\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_values(df_lstm_shap, patient_id, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_values(df, patient_id, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = xgb_model.eval(xgb.DMatrix(X_test.values, df_test[target_colname].values))\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_shap_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
