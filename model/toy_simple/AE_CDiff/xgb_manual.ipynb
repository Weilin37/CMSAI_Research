{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model Training and SHAP computation for AE CDiff Dataset\n",
    "\n",
    "**Author: Tesfagabir Meharizghi<br>Last Updated: 02/03/2021**\n",
    "\n",
    "In this jupyter notebook, we trained XGB models by manually tuning their parameters to get the best performance and explainability scores from the different datasets.\n",
    "\n",
    "Tasks done here are:\n",
    "- Training XGB models with the specified parameters and datasets\n",
    "- Computing different model performance measures such as AUCs, intersection similarity, RBOs, etc.\n",
    "- Visualizing global and local SHAP scores\n",
    "- And finally copying the well trained models to S3 if needed\n",
    "\n",
    "Outputs:\n",
    "- The following artifacts are saved:\n",
    "    * Model artifacts\n",
    "    * SHAP values and their corresponding scores for the specified number of val/test examples\n",
    "\n",
    "Model Architecture Used:\n",
    "- XGB\n",
    "\n",
    "Dataset:\n",
    "- Synthetic Dataset\n",
    "\n",
    "Requirements:\n",
    "- Make sure that you have already generated sequence-based dataset (train/val/test splits) using [Create_toy_dataset_sequence.ipynb](../../../data/toy_dataset/Create_toy_dataset_sequence.ipynb).\n",
    "\n",
    "Discussions/observations:\n",
    "- Since the models' parameters are manually specified with the best ones, we could get almost the best performance in terms of its AUCs and explainability scores.\n",
    "- In addition, we could also get almost perfect global SHAP importance scores where all the important events are selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases and Observations:\n",
    "- Increasing the token variety for each category\n",
    "      - Performance and importance scores didn't change from that of lower number of tokens\n",
    "      - The same also for 30 & 300 seq lengths (didn't change performance)\n",
    "- Decreasing the 0.99 prob for the ad_seq to 0.8\n",
    "    - The models' performance lowered\n",
    "        - 87% --> 83% AUC\n",
    "    - Sequence lengths didn't make a difference        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.inspection import permutation_importance\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo\n",
    "# import rbo\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import xgboost_utils as xgb_utils\n",
    "import shap_jacc_utils as sj_utils\n",
    "import cdiff_utils as cd_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature suffices that help predict the positive or negative class\n",
    "# HELPING_FEATURES_SUFFICES = [\"_A\", \"_H\", \"_U\"]  # Noises Removed\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "# Whether to preprocess data\n",
    "PREPROCESS_DATA = False\n",
    "\n",
    "# Whether to save SHAP scores\n",
    "SAVE_SHAP_OUTPUT = True\n",
    "\n",
    "# Whether to ouput SHAP explainer expected value\n",
    "OUTPUT_SHAP_EXPLAINER = True\n",
    "\n",
    "# Whether to shuffle val & test dataset for shap visualization purposes\n",
    "SHUFFLE = False\n",
    "\n",
    "USE_FREQ = True  # Whether to use feature frequencies or one-hot for data preprocessing\n",
    "\n",
    "nrows = 1e9\n",
    "\n",
    "target_colname = \"d_00845\"\n",
    "uid_colname = \"patient_id\"\n",
    "target_value = \"1\"\n",
    "\n",
    "rev = False\n",
    "\n",
    "model_name = \"xgb\"\n",
    "dataset = \"AE_CDiff\"\n",
    "\n",
    "# For model early stopping criteria\n",
    "EARLY_STOPPING = \"intersection_similarity\"  # Values are any of these: ['intersection_similarity', 'loss']\n",
    "\n",
    "# SHAP related constants\n",
    "N_BACKGROUND = None  # Number of background examples\n",
    "BACKGROUND_NEGATIVE_ONLY = False  # If negative examples are used as background\n",
    "BACKGROUND_POSITIVE_ONLY = False  # If positive examples are used as background\n",
    "TEST_POSITIVE_ONLY = False  # If only positive examples are selected\n",
    "IS_TEST_RANDOM = (\n",
    "    False  # If random test/val examples are selected for shap value computation\n",
    ")\n",
    "SORT_SHAP_VALUES = False  # Whether to sort per-patient shap values for visualization\n",
    "\n",
    "SHAP_SCORE_ABSOLUTE = True  # Whether to consider the absolute value of a shap score #TODO: Check this before running.\n",
    "\n",
    "# For seq_len=30\n",
    "seq_len = 100\n",
    "min_freq = 1000\n",
    "\n",
    "DATA_TYPE = \"original\"  # \"downsampled\"\n",
    "FNAME = \"20110101\"  # \"all\"\n",
    "\n",
    "train_data_path = f\"./output-mix/AE_CDiff/{seq_len}/{DATA_TYPE}/lstm-att-lrp/splits/{FNAME}/{min_freq}/train_balanced.csv\"\n",
    "valid_data_path = f\"./output-mix/AE_CDiff/{seq_len}/{DATA_TYPE}/lstm-att-lrp/splits/{FNAME}/{min_freq}/val.csv\"\n",
    "test_data_path = f\"./output-mix/AE_CDiff/{seq_len}/{DATA_TYPE}/lstm-att-lrp/splits/{FNAME}/{min_freq}/test.csv\"\n",
    "examples_path = f\"./output-mix/AE_CDiff/{seq_len}/{DATA_TYPE}/lstm-att-lrp/splits/{FNAME}/{min_freq}/visualized_test_patients.csv\"\n",
    "vocab_path = f\"./output-mix/AE_CDiff/{seq_len}/{DATA_TYPE}/lstm-att-lrp/splits/{FNAME}/{min_freq}/vocab.pkl\"\n",
    "\n",
    "# Dataset preprocessing\n",
    "x_train_one_hot_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/train_one_hot.csv\"\n",
    "x_valid_one_hot_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/val_one_hot.csv\"\n",
    "x_test_one_hot_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/test_one_hot.csv\"\n",
    "\n",
    "x_train_data_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/train.csv\"\n",
    "x_valid_data_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/val.csv\"\n",
    "x_test_data_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data/test.csv\"\n",
    "\n",
    "GT_CODES_PATH = \"../../../data/AE_CDiff_d00845/cdiff_risk_factors_codes.csv\"\n",
    "\n",
    "model_save_path = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/models/model_{'{}'}.pkl\"\n",
    "shap_save_path_pattern = f\"./output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split (train/val/test) (data format (features, scores, patient_ids))\n",
    "\n",
    "PARAMS_PATH = f\"output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/train_results/model_params.json\"\n",
    "\n",
    "# Model training\n",
    "BUCKET = \"cmsai-mrk-amzn\"\n",
    "s3_output_data_dir = f\"s3://{BUCKET}/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data\"\n",
    "\n",
    "DATA_PREFIX = f\"{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/data\"\n",
    "MODEL_PREFIX = f\"{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/models\"\n",
    "\n",
    "output_results_path = f\"output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/train_results/results.csv\"\n",
    "\n",
    "local_model_dir = f\"output-mix/{dataset}/{seq_len}/{DATA_TYPE}/{model_name}/{FNAME}/{min_freq}/models/\"\n",
    "s3_output_path = f\"s3://{BUCKET}/{MODEL_PREFIX}/output\"\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = \"xgboost\"\n",
    "REPO_VERSION = \"1.2-1\"\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 3\n",
    "N_BACKGROUND = 10000  # None if you want to use all (for shap)\n",
    "N_SHAP_EXAMPLES = 10000  # None if all\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"n_jobs\": 25,\n",
    "    \"random_state\": 10,\n",
    "    \"max_depth\": 2,\n",
    "    \"n_estimators\": 500,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"reg_alpha\": 0.8,\n",
    "    \"reg_lambda\": 0.7,\n",
    "    \"colsample_bytree\": 0.35,\n",
    "    \"use_label_encoder\": False,\n",
    "}\n",
    "# EARLY_STOPPING_ROUNDS = 5\n",
    "\n",
    "# MODEL_PARAMS = {\n",
    "#     \"n_jobs\": 25,\n",
    "#     \"random_state\": 10,\n",
    "#     \"max_depth\": 2,\n",
    "#     \"n_estimators\": 500,\n",
    "#     \"eval_metric\": \"auc\",\n",
    "#     \"learning_rate\": 0.3,\n",
    "#     \"reg_alpha\": 0.2,\n",
    "#     \"reg_lambda\": 0.3,\n",
    "#     \"colsample_bytree\": 0.35,\n",
    "#     \"use_label_encoder\": False,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output-mix/AE_CDiff/100/original/xgb/20110101/1000/models\n",
      "New directory created: ./output-mix/AE_CDiff/100/original/xgb/20110101/1000/shap\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Model Output Directory\n",
    "    model_save_dir = os.path.dirname(model_save_path)\n",
    "    shap_save_dir = os.path.dirname(shap_save_path_pattern)\n",
    "\n",
    "    if os.path.exists(model_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(model_save_dir)\n",
    "    if os.path.exists(shap_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(shap_save_dir)\n",
    "    os.makedirs(model_save_dir)\n",
    "    os.makedirs(shap_save_dir)\n",
    "    print(f\"New directory created: {model_save_dir}\")\n",
    "    print(f\"New directory created: {shap_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(train_data_path)\n",
    "\n",
    "# columns = [str(i) for i in range(seq_len - 1, -1, -1)]\n",
    "# df = df[columns]\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = xgb_utils.get_valid_tokens(df, seq_len)\n",
    "with open(vocab_path, \"rb\") as fp:\n",
    "    vocab = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(vocab._vocab.keys())\n",
    "tokens.remove(\"<pad>\")\n",
    "tokens.remove(\"<unk>\")\n",
    "tokens = [token for token in tokens if \"day\" not in token]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    xgb_utils.prepare_data(\n",
    "        train_data_path,\n",
    "        x_train_one_hot_path,\n",
    "        x_train_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        valid_data_path,\n",
    "        x_valid_one_hot_path,\n",
    "        x_valid_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        test_data_path,\n",
    "        x_test_one_hot_path,\n",
    "        x_test_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "# else:\n",
    "#     local_dir = os.path.dirname(x_train_data_path)\n",
    "#     xgb_utils.copy_data_to_s3(local_dir, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xgb_classifier(model_path, model_params):\n",
    "    \"\"\"Load XGBClassifier model from saved Booster model.\"\"\"\n",
    "    xgb_model = xgb.XGBClassifier(**model_params)\n",
    "    # xgb_model.set_params()\n",
    "    xgb_model._Booster = sj_utils.load_pickle(model_path)\n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "def get_model_paths(model_save_dir, sort=True):\n",
    "    \"\"\"Get list models paths in sorted order if needed.\"\"\"\n",
    "    fnames = os.listdir(model_save_dir)\n",
    "    if sort:\n",
    "        fnames.sort(key=lambda f: int(re.sub(\"\\D\", \"\", f)))\n",
    "    model_paths = [os.path.join(model_save_dir, fname) for fname in fnames]\n",
    "    return model_paths\n",
    "\n",
    "\n",
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x, cdiff=False):\n",
    "    outcome = False\n",
    "    if cdiff:\n",
    "        if x.endswith(\"_rf\"):\n",
    "            outcome = True\n",
    "    else:\n",
    "        if not x.endswith(\"_N\"):\n",
    "            outcome = True\n",
    "    return outcome\n",
    "\n",
    "\n",
    "def get_model_intersection_similarity_v2(\n",
    "    features_scores, cdiff, absolute=True, freedom=1\n",
    "):\n",
    "    # gt similarity\n",
    "    all_features, all_scores = features_scores[0], features_scores[1]\n",
    "    sims = []\n",
    "    for i, features in enumerate(all_features):\n",
    "        scores = all_scores[i]\n",
    "        gt_features = [feature for feature in features if is_value(feature, cdiff)]\n",
    "        n_gt = len(gt_features)\n",
    "        if n_gt > 0:\n",
    "            dict_features_scores = sj_utils.create_dict_features_scores(\n",
    "                features, scores, absolute\n",
    "            )\n",
    "            top_features_scores = sj_utils.top_k(\n",
    "                dict_features_scores, len(gt_features) + freedom\n",
    "            )\n",
    "            top_features = top_features_scores[0]\n",
    "            pred_features = [\n",
    "                feature for feature in top_features if is_value(feature, cdiff)\n",
    "            ]\n",
    "            sim = len(set(pred_features).intersection(gt_features)) / float(n_gt)\n",
    "        else:\n",
    "            sim = -1\n",
    "        sims.append(sim)\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    return avg_sim, sims\n",
    "\n",
    "\n",
    "def compute_shap(xgb_model, df_train0, df_test0, explainer=None):\n",
    "    # Load the copied model\n",
    "    # xgb_model = sj_utils.load_pickle(model_path)\n",
    "\n",
    "    df_train = df_train0.copy()\n",
    "    df_test = df_test0.copy()\n",
    "\n",
    "    feature_names = [\n",
    "        col\n",
    "        for col in df_train.columns.tolist()\n",
    "        if col not in [\"patient_id\", target_colname]\n",
    "    ]\n",
    "    X_train = df_train[feature_names]\n",
    "    X_test = df_test[feature_names]\n",
    "\n",
    "    if explainer is None:\n",
    "        explainer = shap.TreeExplainer(xgb_model, X_train)\n",
    "    shap_scores = explainer.shap_values(X_test).tolist()\n",
    "    features = [feature_names[:]] * X_test.shape[0]\n",
    "    patients = df_test.patient_id.tolist()\n",
    "\n",
    "    return ((features, shap_scores, patients), explainer)\n",
    "\n",
    "\n",
    "def save_results(patients, features, shap_scores, y_true, y_pred, output_path):\n",
    "    \"\"\"Save all model training results to file.\"\"\"\n",
    "    results = {}\n",
    "    for i, patient_id in enumerate(patients):\n",
    "        results[patient_id] = {}\n",
    "        results[patient_id][\"features_xgb\"] = features[i]\n",
    "        results[patient_id][target_colname] = y_true[i]\n",
    "        results[patient_id][\"xgb_pred\"] = y_pred[i]\n",
    "        results[patient_id][\"xgb_shap\"] = shap_scores[i]\n",
    "\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sj_utils.save_pickle(results, output_path, verbose=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_features_and_global_shap(results, exp_num=1):\n",
    "    \"\"\"Get a dataframe of features and global shap values.\"\"\"\n",
    "    features = None\n",
    "    ##Get features and global shap\n",
    "    all_shap = []\n",
    "    for patient_id, result in results.items():\n",
    "        if features is None:\n",
    "            features = results[patient_id][\"features_xgb\"]\n",
    "        all_shap.append(results[patient_id][\"xgb_shap\"])\n",
    "    all_shap = np.absolute(np.array(all_shap)).mean(axis=0)\n",
    "    df_shap = pd.DataFrame()\n",
    "    df_shap[\"features\"] = features\n",
    "    df_shap[\"scores\"] = all_shap\n",
    "    df_shap[\"exp_num\"] = exp_num\n",
    "    return df_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(x_train_one_hot_path)\n",
    "df_val = pd.read_csv(x_valid_one_hot_path)\n",
    "df_test = pd.read_csv(x_test_one_hot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>h_71020</th>\n",
       "      <th>h_93010</th>\n",
       "      <th>h_99284</th>\n",
       "      <th>h_A0425</th>\n",
       "      <th>h_A0427</th>\n",
       "      <th>h_99214</th>\n",
       "      <th>d_72887</th>\n",
       "      <th>d_V5869</th>\n",
       "      <th>h_36415</th>\n",
       "      <th>...</th>\n",
       "      <th>p_4414_rf</th>\n",
       "      <th>p_4621_rf</th>\n",
       "      <th>p_4603_rf</th>\n",
       "      <th>p_4679_rf</th>\n",
       "      <th>p_527_rf</th>\n",
       "      <th>p_5214_rf</th>\n",
       "      <th>p_4281_rf</th>\n",
       "      <th>p_5222_rf</th>\n",
       "      <th>p_44_rf</th>\n",
       "      <th>d_00845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BWCA3E6IA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PFU8RB3WB</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OS3G482UT</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPZAEJNQX</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5N955AFNU</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id  h_71020  h_93010  h_99284  h_A0425  h_A0427  h_99214  d_72887  \\\n",
       "0  BWCA3E6IA        1        1        0        0        1        0        0   \n",
       "1  PFU8RB3WB        3        0        0        0        0        6        0   \n",
       "2  OS3G482UT        2        5        0        0        0        4        0   \n",
       "3  IPZAEJNQX        0        3        3        2        0        8        0   \n",
       "4  5N955AFNU        1        1        0        0        0        1        0   \n",
       "\n",
       "   d_V5869  h_36415  ...  p_4414_rf  p_4621_rf  p_4603_rf  p_4679_rf  \\\n",
       "0        0        0  ...          0          0          0          0   \n",
       "1        0        1  ...          0          0          0          0   \n",
       "2        0        2  ...          0          0          0          0   \n",
       "3        0        2  ...          0          0          0          0   \n",
       "4        0        0  ...          0          0          0          0   \n",
       "\n",
       "   p_527_rf  p_5214_rf  p_4281_rf  p_5222_rf  p_44_rf  d_00845  \n",
       "0         0          0          0          0        0        0  \n",
       "1         0          0          0          0        0        0  \n",
       "2         0          0          0          0        0        0  \n",
       "3         0          0          0          0        0        0  \n",
       "4         0          0          0          0        0        0  \n",
       "\n",
       "[5 rows x 389 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count: 0-->7220, 1-->7748, Total-->14968\n",
      "Val Classes Count: 0-->248494, 1-->159, Total-->248653\n",
      "Test Classes Count: 0-->248494, 1-->160, Total-->248654\n",
      "(14968, 389)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>h_71020</th>\n",
       "      <th>h_93010</th>\n",
       "      <th>h_99284</th>\n",
       "      <th>h_A0425</th>\n",
       "      <th>h_A0427</th>\n",
       "      <th>h_99214</th>\n",
       "      <th>d_72887</th>\n",
       "      <th>d_V5869</th>\n",
       "      <th>h_36415</th>\n",
       "      <th>...</th>\n",
       "      <th>p_4414_rf</th>\n",
       "      <th>p_4621_rf</th>\n",
       "      <th>p_4603_rf</th>\n",
       "      <th>p_4679_rf</th>\n",
       "      <th>p_527_rf</th>\n",
       "      <th>p_5214_rf</th>\n",
       "      <th>p_4281_rf</th>\n",
       "      <th>p_5222_rf</th>\n",
       "      <th>p_44_rf</th>\n",
       "      <th>d_00845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YYLTRUF86_20110301</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8PYNAJSL_20111101</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DFC51O5ON_20110601</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D9SD87CG5_20110701</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6WO1LFAKJ_20110501</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           patient_id  h_71020  h_93010  h_99284  h_A0425  h_A0427  h_99214  \\\n",
       "0  YYLTRUF86_20110301        1        1        1        1        1        6   \n",
       "1  Q8PYNAJSL_20111101        1        6        1        4        0        2   \n",
       "2  DFC51O5ON_20110601        2        4        0        0        0        9   \n",
       "3  D9SD87CG5_20110701        0        0        0        0        0        7   \n",
       "4  6WO1LFAKJ_20110501        2        3        1        6        2        4   \n",
       "\n",
       "   d_72887  d_V5869  h_36415  ...  p_4414_rf  p_4621_rf  p_4603_rf  p_4679_rf  \\\n",
       "0        1        1        2  ...          0          0          0          0   \n",
       "1        0        0        3  ...          0          0          0          0   \n",
       "2        0        0        2  ...          0          0          0          0   \n",
       "3        0        1        9  ...          0          0          0          0   \n",
       "4        0        0        3  ...          0          0          0          0   \n",
       "\n",
       "   p_527_rf  p_5214_rf  p_4281_rf  p_5222_rf  p_44_rf  d_00845  \n",
       "0         0          0          0          0        0        0  \n",
       "1         0          0          0          0        0        1  \n",
       "2         0          0          0          0        0        1  \n",
       "3         0          0          0          0        0        0  \n",
       "4         0          0          0          0        0        1  \n",
       "\n",
       "[5 rows x 389 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_0 = df_train[target_colname].value_counts()[0]\n",
    "count_1 = df_train[target_colname].value_counts()[1]\n",
    "print(\n",
    "    f\"Train Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "count_0 = df_val[target_colname].value_counts()[0]\n",
    "count_1 = df_val[target_colname].value_counts()[1]\n",
    "print(\n",
    "    f\"Val Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "count_0 = df_test[target_colname].value_counts()[0]\n",
    "count_1 = df_test[target_colname].value_counts()[1]\n",
    "print(\n",
    "    f\"Test Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7748\n",
       "0    7220\n",
       "Name: d_00845, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"d_00845\"].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = [\n",
    "    col\n",
    "    for col in df_train.columns.tolist()\n",
    "    if col not in [\"patient_id\", target_colname]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    248494\n",
       "1       159\n",
       "Name: d_00845, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"d_00845\"].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    248494\n",
       "1       160\n",
       "Name: d_00845, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"d_00845\"].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.dirname(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGB Classifier...\n",
      "[0]\tvalidation_0-auc:0.79229\n",
      "[1]\tvalidation_0-auc:0.81104\n",
      "[2]\tvalidation_0-auc:0.82812\n",
      "[3]\tvalidation_0-auc:0.83203\n",
      "[4]\tvalidation_0-auc:0.84512\n",
      "[5]\tvalidation_0-auc:0.85404\n",
      "[6]\tvalidation_0-auc:0.86198\n",
      "[7]\tvalidation_0-auc:0.86350\n",
      "[8]\tvalidation_0-auc:0.86972\n",
      "[9]\tvalidation_0-auc:0.87197\n",
      "[10]\tvalidation_0-auc:0.87447\n",
      "[11]\tvalidation_0-auc:0.87540\n",
      "[12]\tvalidation_0-auc:0.87432\n",
      "[13]\tvalidation_0-auc:0.87658\n",
      "[14]\tvalidation_0-auc:0.87720\n",
      "[15]\tvalidation_0-auc:0.87381\n",
      "[16]\tvalidation_0-auc:0.87493\n",
      "Successfully Done!\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_save_dir):\n",
    "        shutil.rmtree(model_save_dir)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    rounds = 1  # Saves model every one-epoch\n",
    "\n",
    "    check_point = xgb.callback.TrainingCheckPoint(\n",
    "        directory=model_save_dir,\n",
    "        iterations=rounds,\n",
    "        name=\"model\",\n",
    "        as_pickle=True,\n",
    "    )\n",
    "\n",
    "    X_train, y_train = df_train[feature_names], df_train[target_colname]\n",
    "    X_val, y_val = df_val[feature_names], df_val[target_colname]\n",
    "\n",
    "    print(\"Training XGB Classifier...\")\n",
    "    model = XGBClassifier(**MODEL_PARAMS)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        # eval_set=[(X_train, y_train)],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose=True,\n",
    "        callbacks=[check_point],\n",
    "    )\n",
    "\n",
    "    # Save Model Parameters\n",
    "    params_dir = os.path.dirname(PARAMS_PATH)\n",
    "    os.makedirs(params_dir, exist_ok=True)\n",
    "    with open(PARAMS_PATH, \"w\") as fp:\n",
    "        json.dump(MODEL_PARAMS, fp)\n",
    "\n",
    "    print(\"Successfully Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Minimize the number of background examples for SHAP\n",
    "# if N_BACKGROUND is not None:\n",
    "#     df_train = df_train.iloc[:N_BACKGROUND]\n",
    "\n",
    "# if N_SHAP_EXAMPLES is not None:\n",
    "#     df_val = df_val.iloc[:N_SHAP_EXAMPLES]\n",
    "#     df_test = df_test.iloc[:N_SHAP_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of models paths\n",
    "model_paths = get_model_paths(model_save_dir, sort=True)\n",
    "# model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss=0.5293 | Train AUC=0.8621 | Val Loss=0.5324 | Val AUC=0.811 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 02 | Train Loss=0.4888 | Train AUC=0.8757 | Val Loss=0.4923 | Val AUC=0.8281 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 03 | Train Loss=0.4627 | Train AUC=0.8822 | Val Loss=0.4627 | Val AUC=0.832 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 04 | Train Loss=0.4324 | Train AUC=0.8961 | Val Loss=0.4289 | Val AUC=0.8451 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 05 | Train Loss=0.4131 | Train AUC=0.9021 | Val Loss=0.4046 | Val AUC=0.854 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 06 | Train Loss=0.4041 | Train AUC=0.9053 | Val Loss=0.3924 | Val AUC=0.862 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 07 | Train Loss=0.3949 | Train AUC=0.9073 | Val Loss=0.3828 | Val AUC=0.8635 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 08 | Train Loss=0.3889 | Train AUC=0.91 | Val Loss=0.3726 | Val AUC=0.8697 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 09 | Train Loss=0.3802 | Train AUC=0.9121 | Val Loss=0.3606 | Val AUC=0.872 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 10 | Train Loss=0.3743 | Train AUC=0.9135 | Val Loss=0.3509 | Val AUC=0.8745 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 11 | Train Loss=0.3701 | Train AUC=0.9142 | Val Loss=0.3452 | Val AUC=0.8754 | RBO(p=0.9)=-1.0 | TAU=nan\n",
      "Epoch: 12 | Train Loss=0.3672 | Train AUC=0.9159 | Val Loss=0.3432 | Val AUC=0.8743 | RBO(p=0.9)=-1.0 | TAU=nan\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    p_rbo = 0.9\n",
    "    SIMILARITY_FREEDOM = 1  # For Intersection Similarity\n",
    "\n",
    "    train_aucs = []\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    gain_permute_rbo_scores = []\n",
    "    gain_permute_tau_scores = []\n",
    "\n",
    "    val_shap_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "\n",
    "    val_gain_shap_rbo_lst = []\n",
    "    val_gain_shap_tau_lst = []\n",
    "\n",
    "    epochs = []\n",
    "    for model_path in model_paths:\n",
    "        # Get epoch number from the model path\n",
    "        model_fname = model_path\n",
    "        model_fname = os.path.basename(model_path)\n",
    "        epoch = int(re.sub(\"\\D\", \"\", model_fname))\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        xgb_model = load_xgb_classifier(model_path, MODEL_PARAMS)\n",
    "\n",
    "        # feature_importances = xgb_model.feature_importances_\n",
    "        feature_names = xgb_model.get_booster().feature_names\n",
    "\n",
    "        # Compute AUCs\n",
    "        train_y_true = df_train[target_colname]\n",
    "        train_y_pred = xgb_model.predict_proba(df_train[feature_names])[:, 1]\n",
    "        train_auc = roc_auc_score(train_y_true, train_y_pred)\n",
    "        train_aucs.append(train_auc)\n",
    "\n",
    "        val_y_true = df_val[target_colname]\n",
    "        val_y_pred = xgb_model.predict_proba(df_val[feature_names])[:, 1]\n",
    "        val_auc = roc_auc_score(val_y_true, val_y_pred)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        test_y_true = df_test[target_colname]\n",
    "        test_y_pred = xgb_model.predict_proba(df_test[feature_names])[:, 1]\n",
    "        test_auc = roc_auc_score(test_y_true, test_y_pred)\n",
    "        test_aucs.append(test_auc)\n",
    "\n",
    "        # Losses\n",
    "        train_loss = log_loss(train_y_true, train_y_pred)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss = log_loss(val_y_true, val_y_pred)\n",
    "        val_losses.append(val_loss)\n",
    "        test_loss = log_loss(test_y_true, test_y_pred)\n",
    "        test_losses.append(test_loss)\n",
    "        #import pdb; pdb.set_trace()\n",
    "#         del xgb_model\n",
    "#         print(\n",
    "#             f\"Epoch: {epoch:02} | Train Loss={train_loss:.4} | Train AUC={train_auc:.4} | Val Loss={val_loss:.4} | Val AUC={val_auc:.4} | Test Loss={test_loss:.4} | Test AUC={test_auc:.4}\"\n",
    "#         )\n",
    "#         continue\n",
    "        xgb_gain = pd.DataFrame(\n",
    "            (xgb_model.get_booster().get_score(importance_type=\"gain\")).items()\n",
    "        )\n",
    "        xgb_gain.columns = [\"features\", \"xgb_gain\"]\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"sk_gain\"]\n",
    "\n",
    "        df_gains = xgb_gain.merge(feat_imp, how=\"inner\", on=[\"features\"])\n",
    "\n",
    "        df_gains[\"xgb_rank\"] = df_gains[\"xgb_gain\"].rank(ascending=False)\n",
    "        df_gains[\"sk_rank\"] = df_gains[\"sk_gain\"].rank(ascending=False)\n",
    "        df_gains.sort_values(\"xgb_gain\", ascending=False)\n",
    "        # print(df_gains)\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"scores\"]\n",
    "        feat_imp.set_index(\"features\", inplace=True)\n",
    "        feat_imp.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        # feat_imp.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Model Feature Importance Scores\")\n",
    "\n",
    "        # Check permutation\n",
    "#         perm_imp = permutation_importance(\n",
    "#             xgb_model, df_val[feature_names], df_val[target_colname]\n",
    "#         )\n",
    "\n",
    "        perm_df = pd.DataFrame()\n",
    "        perm_df[\"features\"] = feature_names\n",
    "        perm_df[\"scores\"] = [-1.0]*len(feature_names) #perm_imp.importances_mean\n",
    "        perm_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        perm_df.set_index(\"features\", inplace=True)\n",
    "        # perm_df.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Permutation Importance Scores\")\n",
    "\n",
    "        vals = feat_imp.merge(perm_df, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"permute_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        permute_rank = vals.sort_values(\"permute_rank\").index\n",
    "\n",
    "        # Plot for different p values\n",
    "        #     rbo_scores = []\n",
    "        #     p_vals = []\n",
    "        #     p_range = range(1, 10)\n",
    "        #     for p in p_range:\n",
    "        #         rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "        #             p=1.0 / p\n",
    "        #         )\n",
    "        #         rbo_scores.append(rbo_score)\n",
    "        #         p_vals.append(1.0 / p)\n",
    "\n",
    "        #     # plt.figure(figsize=(10, 5))\n",
    "        #     plt.plot(p_vals, rbo_scores, marker=\"x\")\n",
    "        #     plt.title(\"RBO Rankings for different p values.\")\n",
    "        #     plt.xlabel(\"p\")\n",
    "        #     plt.ylabel(\"RBO\")\n",
    "\n",
    "        #         rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "        #             p=p_rbo\n",
    "        #         )\n",
    "        rbo_score = -1.0\n",
    "        gain_permute_rbo_scores.append(rbo_score)\n",
    "        tau_score = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        gain_permute_tau_scores.append(tau_score)\n",
    "\n",
    "        val_shap_results, explainer = compute_shap(\n",
    "            xgb_model, df_train.iloc[:10000], df_val.iloc[:10000], explainer=None\n",
    "        )\n",
    "        (val_features, val_scores, val_patients) = val_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (val_features, val_scores),\n",
    "            cdiff=True,\n",
    "            freedom=SIMILARITY_FREEDOM,\n",
    "            absolute=True,\n",
    "        )\n",
    "        val_shap_sim_lst.append(avg_sim)\n",
    "\n",
    "        # Compute the shap global feature importance\n",
    "        shap_global = np.absolute(np.array(val_scores)).mean(axis=0)\n",
    "        df_shap = pd.DataFrame()\n",
    "        df_shap[\"features\"] = feature_names\n",
    "        df_shap[\"scores\"] = shap_global.tolist()\n",
    "        df_shap.set_index(\"features\", inplace=True)\n",
    "        # df_shap[\"shap_rank\"] = df_shap.scores.rank(ascending=False)\n",
    "        # df_shap.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "        vals = feat_imp.merge(df_shap, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"shap_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        shap_rank = vals.sort_values(\"shap_rank\").index\n",
    "\n",
    "        #         rbo_score_shap = rbo.RankingSimilarity(gain_rank.values, shap_rank.values).rbo(\n",
    "        #             p=p_rbo\n",
    "        #         )\n",
    "        rbo_score_shap = -1.0\n",
    "        val_gain_shap_rbo_lst.append(rbo_score_shap)\n",
    "\n",
    "        tau_score_shap = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        val_gain_shap_tau_lst.append(tau_score_shap)\n",
    "        test_shap_results, _ = compute_shap(xgb_model, df_train.iloc[:10000], df_test.iloc[:10000], explainer)\n",
    "        (test_features, test_scores, test_patients) = test_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (test_features, test_scores),\n",
    "            cdiff=True,\n",
    "            freedom=SIMILARITY_FREEDOM,\n",
    "            absolute=True,\n",
    "        )\n",
    "        test_shap_sim_lst.append(avg_sim)\n",
    "        # Save training results to file.\n",
    "        shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "        results = save_results(\n",
    "            val_patients, val_features, val_scores, val_y_true, val_y_pred, shap_path\n",
    "        )\n",
    "\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        results = save_results(\n",
    "            test_patients,\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_y_true,\n",
    "            test_y_pred,\n",
    "            shap_path,\n",
    "        )\n",
    "        del xgb_model\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02} | Train Loss={train_loss:.4} | Train AUC={train_auc:.4} | Val Loss={val_loss:.4} | Val AUC={val_auc:.4} | RBO(p={p_rbo})={rbo_score:.4} | TAU={tau_score:.4}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Aggregate results\n",
    "    columns = [\n",
    "        \"train_AUC\",\n",
    "        \"val_AUC\",\n",
    "        \"test_AUC\",\n",
    "        \"train_Loss\",\n",
    "        \"val_Loss\",\n",
    "        \"test_Loss\",\n",
    "        \"gain_permute_rbo\",\n",
    "        \"gain_permute_tau\",\n",
    "        \"val_GT_shap_sim\",\n",
    "        \"test_GT_shap_sim\",\n",
    "        \"val_gain_shap_rbo\",\n",
    "        \"val_gain_shap_tau\",\n",
    "    ]\n",
    "\n",
    "    df_results = pd.DataFrame(\n",
    "        np.array(\n",
    "            [\n",
    "                train_aucs,\n",
    "                val_aucs,\n",
    "                test_aucs,\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                test_losses,\n",
    "                gain_permute_rbo_scores,\n",
    "                gain_permute_tau_scores,\n",
    "                val_shap_sim_lst,\n",
    "                test_shap_sim_lst,\n",
    "                val_gain_shap_rbo_lst,\n",
    "                val_gain_shap_tau_lst,\n",
    "            ]\n",
    "        ).T,\n",
    "        columns=columns,\n",
    "    )\n",
    "    df_results[\"epoch\"] = epochs\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    output_dir = os.path.dirname(output_results_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_results.to_csv(output_results_path)\n",
    "else:\n",
    "    # save results summary\n",
    "    df_results = pd.read_csv(output_results_path)\n",
    "    df_results.set_index(\"epoch\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"SHAP vs GT Similarity on All Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_gain_shap_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs SHAP with Kendall-T on Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs Permutation Importance with Kendall-T\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Gain/Permutation withKendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(model_paths) + 1)\n",
    "figsize = (15, 10)\n",
    "max_tokens = 30\n",
    "for epoch in epochs[:1]:\n",
    "    shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "    print(shap_path)\n",
    "    val_results = sj_utils.load_pickle(shap_path)\n",
    "    df_shap = get_features_and_global_shap(val_results)\n",
    "    df_shap = df_shap.sort_values(\"scores\", ascending=True)\n",
    "    if max_tokens is not None:\n",
    "        df_shap = df_shap.iloc[-max_tokens:, :]\n",
    "    df_shap.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=\"scores\",\n",
    "        kind=\"barh\",\n",
    "    )\n",
    "    plt.title(f\"Global Feature Importance (epoch={epoch})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(train_data_path)\n",
    "patients_path = os.path.join(data_dir, \"visualized_test_patients.txt\")\n",
    "\n",
    "with open(patients_path, \"r\") as fp:\n",
    "    selected_patients = fp.readlines()\n",
    "    print(selected_patients)\n",
    "    selected_patients = [pat.strip() for pat in selected_patients]\n",
    "\n",
    "selected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients = [\n",
    "    \"BQ2TYCPYR_20110501\",\n",
    "    \"APDBFR363_20110201\",\n",
    "    \"FMBUO6UJH_20110101\",\n",
    "    \"VCN68G35I_20110701\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50\n",
    "sorted_epoch = 1\n",
    "\n",
    "n_jobs = len(model_paths)\n",
    "step = int(n_jobs / 2) - 1\n",
    "# epochs = range(1, len(model_paths) + 1, 2)\n",
    "epochs = range(1, len(model_paths) + 1, step)\n",
    "for patient_id in selected_patients:\n",
    "    df = pd.DataFrame()\n",
    "    label = None\n",
    "    pred_prob = None\n",
    "    for epoch in epochs:\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        test_results = sj_utils.load_pickle(shap_path)\n",
    "        if not len(df):\n",
    "            features = test_results[patient_id][\"features_xgb\"]\n",
    "            df[\"features\"] = features\n",
    "            df[\"values\"] = df_test[features][\n",
    "                df_test[\"patient_id\"] == patient_id\n",
    "            ].values[0]\n",
    "            df[\"features\"] = df[\"values\"].astype(str) + \"_\" + df[\"features\"]\n",
    "            label = test_results[patient_id][target_colname]\n",
    "        pred_prob = test_results[patient_id][\"xgb_pred\"]\n",
    "        df[f\"epoch_{epoch}\"] = test_results[patient_id][\"xgb_shap\"]\n",
    "    if max_tokens is not None:\n",
    "        df = df.sort_values(f\"epoch_{sorted_epoch}\", ascending=False)\n",
    "        df = df.iloc[:max_tokens, :]\n",
    "    figsize = (15, 10)\n",
    "    epoch_cols = [f\"epoch_{epoch}\" for epoch in epochs]\n",
    "    df.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=epoch_cols,\n",
    "        kind=\"bar\",\n",
    "        width=0.8,\n",
    "    )\n",
    "    plt.title(f\"SHAP Scores for {patient_id}: label={label}, pred={pred_prob:.4}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_exps = 10\n",
    "# df = pd.DataFrame()\n",
    "# for n_exp in range(1, n_exps + 1):\n",
    "#     summary_path = f\"output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/train_results/results.csv\"\n",
    "#     df_summary = pd.read_csv(summary_path)\n",
    "#     df_summary.set_index(\"epoch\", inplace=True)\n",
    "#     best_epoch = df_summary[\"val_GT_shap_sim\"].idxmax()\n",
    "\n",
    "#     best_results_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/test_shap_{best_epoch}.pkl\"\n",
    "#     dict_results = sj_utils.load_pickle(best_results_path)\n",
    "#     df_shap = get_features_and_global_shap(dict_results, n_exp)\n",
    "\n",
    "#     df = pd.concat([df, df_shap], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 10))\n",
    "# df_mean = df.groupby(\"features\")[\"scores\"].mean()\n",
    "# df_mean.sort_values(ascending=False, inplace=True)\n",
    "# df_mean_cols = df_mean.index\n",
    "# ax = sns.boxplot(x=\"features\", y=\"scores\", data=df, order=df_mean_cols)\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best models results to S3\n",
    "# n_exps = 10\n",
    "# for n_exp in range(1, n_exps + 1):\n",
    "#     print(f\"Copying best model data to s3 for exp={n_exp}...\")\n",
    "#     summary_path = f\"output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/train_results/results.csv\"\n",
    "#     df_summary = pd.read_csv(summary_path)\n",
    "#     df_summary.set_index(\"epoch\", inplace=True)\n",
    "#     best_epoch = df_summary[\"val_GT_shap_sim\"].idxmax()\n",
    "\n",
    "#     model_path = f\"./output/Final_Manually_Tuned/30/10/xgb/models/model_{best_epoch}.pkl\"\n",
    "#     val_best_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/val_shap_{best_epoch}.pkl\"\n",
    "#     test_best_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/test_shap_{best_epoch}.pkl\"\n",
    "\n",
    "#     s3_dir = f\"s3://merck-paper-bucket/Synthetic-events/final_event_30_xgb/{n_exp}/\"\n",
    "\n",
    "#     # copy files to s3\n",
    "#     command_pattern = f\"aws s3 cp {'{}'} {s3_dir}\"\n",
    "\n",
    "#     command = command_pattern.format(summary_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(model_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(val_best_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(test_best_path)\n",
    "#     os.system(command)\n",
    "# print(\"Models data successfully copied to S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
