{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model Training and SHAP computation using Sequence-based Synthetic Dataset (Seq-len=30)\n",
    "\n",
    "**Author: Tesfagabir Meharizghi<br>Last Updated: 02/03/2021**\n",
    "\n",
    "In this jupyter notebook, we trained XGB models by SageMaker Hyperparameter Tuning (HPO) from the specified range of parameters.\n",
    "\n",
    "Tasks done here are:\n",
    "- Training XGB models with the specified range of parameters and datasets\n",
    "- Computing different model performance measures such as AUCs, intersection similarity, RBOs, etc.\n",
    "- Visualizing global and local SHAP scores and other performance scores\n",
    "- And finally copying the well trained models to S3 if needed\n",
    "\n",
    "Outputs:\n",
    "- The following artifacts are saved:\n",
    "    * Model artifacts\n",
    "    * SHAP values and their corresponding scores for the specified number of val/test examples\n",
    "\n",
    "Model Architecture Used:\n",
    "- XGB\n",
    "\n",
    "Dataset:\n",
    "- Synthetic-events (Toy Dataset) - [seq_len=30]\n",
    "\n",
    "Requirements:\n",
    "- Make sure that you have already generated sequence-based dataset (train/val/test splits) using [Create_toy_dataset_sequence.ipynb](../../../data/toy_dataset/Create_toy_dataset_sequence.ipynb).\n",
    "\n",
    "Discussions/observations:\n",
    "- Since the models' parameters are automatically tuned from HPO, they are not as optimized as the ones manually specified.\n",
    "- Some important tokens/events are missing from the top SHAP scores.\n",
    "- However, the AUCs are still high as compared to a well-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.inspection import permutation_importance\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo\n",
    "import rbo\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import xgboost_utils as xgb_utils\n",
    "import shap_jacc_utils as sj_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature suffices that help predict the positive or negative class\n",
    "HELPING_FEATURES_SUFFICES = [\"_A\", \"_H\", \"_U\"]  # Noises Removed\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "TUNING_JOB_NAME = None\n",
    "\n",
    "# Whether to preprocess data\n",
    "PREPROCESS_DATA = False\n",
    "\n",
    "# Whether to save SHAP scores\n",
    "SAVE_SHAP_OUTPUT = True\n",
    "\n",
    "# Whether to ouput SHAP explainer expected value\n",
    "OUTPUT_SHAP_EXPLAINER = True\n",
    "\n",
    "# Whether to shuffle val & test dataset for shap visualization purposes\n",
    "SHUFFLE = False\n",
    "\n",
    "# For seq_len=30\n",
    "seq_len = 30\n",
    "\n",
    "USE_FREQ = True  # Whether to use feature frequencies or one-hot for data preprocessing\n",
    "\n",
    "nrows = 1e9\n",
    "\n",
    "target_colname = \"label\"\n",
    "uid_colname = \"patient_id\"\n",
    "target_value = \"1\"\n",
    "\n",
    "rev = False\n",
    "\n",
    "model_name = \"xgb\"\n",
    "dataset = \"Synthetic-seq-based-v2\"\n",
    "\n",
    "# For model early stopping criteria\n",
    "EARLY_STOPPING = \"intersection_similarity\"  # Values are any of these: ['intersection_similarity', 'loss']\n",
    "\n",
    "# SHAP related constants\n",
    "N_BACKGROUND = None  # Number of background examples\n",
    "BACKGROUND_NEGATIVE_ONLY = False  # If negative examples are used as background\n",
    "BACKGROUND_POSITIVE_ONLY = False  # If positive examples are used as background\n",
    "TEST_POSITIVE_ONLY = False  # If only positive examples are selected\n",
    "IS_TEST_RANDOM = (\n",
    "    False  # If random test/val examples are selected for shap value computation\n",
    ")\n",
    "SORT_SHAP_VALUES = False  # Whether to sort per-patient shap values for visualization\n",
    "\n",
    "SHAP_SCORE_ABSOLUTE = True  # Whether to consider the absolute value of a shap score #TODO: Check this before running.\n",
    "\n",
    "train_data_path = f\"../../../data/toy_dataset/data/seq_final_v2/{seq_len}/train.csv\"\n",
    "valid_data_path = f\"../../../data/toy_dataset/data/seq_final_v2/{seq_len}/val.csv\"\n",
    "test_data_path = f\"../../../data/toy_dataset/data/seq_final_v2/{seq_len}/test.csv\"\n",
    "\n",
    "model_save_path = (\n",
    "    f\"./output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/models/model_{'{}'}.pkl\"\n",
    ")\n",
    "shap_save_path_pattern = f\"./output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split (train/val/test) (data format (features, scores, patient_ids))\n",
    "\n",
    "# Dataset preprocessing\n",
    "x_train_one_hot_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/train_one_hot.csv\"\n",
    "x_valid_one_hot_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/val_one_hot.csv\"\n",
    "x_test_one_hot_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/test_one_hot.csv\"\n",
    "\n",
    "x_train_data_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/train.csv\"\n",
    "x_valid_data_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/val.csv\"\n",
    "x_test_data_path = f\"./output/seq_final_v2/{seq_len}/{model_name}/data/test.csv\"\n",
    "\n",
    "s3_output_data_dir = f\"s3://merck-paper-bucket/{dataset}/seq_final_v2/Final_HPO_Tuned/{seq_len}/data\"\n",
    "\n",
    "# Model training\n",
    "BUCKET = \"merck-paper-bucket\"\n",
    "DATA_PREFIX = f\"{dataset}/seq_final_v2/Final_HPO_Tuned/{seq_len}/data\"\n",
    "MODEL_PREFIX = f\"{dataset}/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}\".format(seq_len)\n",
    "label = \"label\"\n",
    "\n",
    "\n",
    "hpo_output_results_path = (\n",
    "    f\"output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/train_results/hpo_params.csv\"\n",
    ")\n",
    "output_results_path = (\n",
    "    f\"output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/train_results/results.csv\"\n",
    ")\n",
    "PARAMS_PATH = (\n",
    "    f\"output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/train_results/hyperparameter_ranges.pkl\"\n",
    ")\n",
    "\n",
    "local_model_dir = f\"output/seq_final_v2/Final_HPO_Tuned/{seq_len}/{model_name}/models/\"\n",
    "s3_output_path = f\"s3://{BUCKET}/{MODEL_PREFIX}/output\"\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = \"xgboost\"\n",
    "REPO_VERSION = \"1.2-1\"\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = \"ml.m5.4xlarge\"  #'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 4\n",
    "MAX_TRAIN_JOBS = 20\n",
    "\n",
    "EVALUATION_METRIC = \"auc\"\n",
    "OBJECTIVE = \"binary:logistic\"\n",
    "OBJECTIVE_METRIC_NAME = \"validation:auc\"\n",
    "\n",
    "HYPERPARAMETER_RANGES = {\n",
    "    \"eta\": ContinuousParameter(0.1, 0.3),\n",
    "    \"alpha\": ContinuousParameter(0, 1),\n",
    "    \"max_depth\": IntegerParameter(1, 7),\n",
    "    \"gamma\": ContinuousParameter(0, 2),\n",
    "    \"num_round\": IntegerParameter(20, 100),\n",
    "    \"colsample_bylevel\": ContinuousParameter(0.1, 0.2),\n",
    "    \"colsample_bynode\": ContinuousParameter(0.1, 0.3),\n",
    "    \"colsample_bytree\": ContinuousParameter(0.5, 0.7),\n",
    "    \"lambda\": ContinuousParameter(0, 50),\n",
    "    \"max_delta_step\": IntegerParameter(0, 3),\n",
    "    \"min_child_weight\": ContinuousParameter(0, 5),\n",
    "    \"subsample\": ContinuousParameter(0.5, 0.7),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output/Final_HPO_Tuned/30/xgb/models\n",
      "New directory created: ./output/Final_HPO_Tuned/30/xgb/shap\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Model Output Directory\n",
    "    model_save_dir = os.path.dirname(model_save_path)\n",
    "    shap_save_dir = os.path.dirname(shap_save_path_pattern)\n",
    "\n",
    "    if os.path.exists(model_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(model_save_dir)\n",
    "    if os.path.exists(shap_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(shap_save_dir)\n",
    "    os.makedirs(model_save_dir)\n",
    "    os.makedirs(shap_save_dir)\n",
    "    print(f\"New directory created: {model_save_dir}\")\n",
    "    print(f\"New directory created: {shap_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>29</th>\n",
       "      <th>28</th>\n",
       "      <th>27</th>\n",
       "      <th>26</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>23</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seq_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>CHF_A</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>1</td>\n",
       "      <td>NY60PULF9H</td>\n",
       "      <td>UHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>ACE_inhibitors_U</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>...</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>AMI_A</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>1</td>\n",
       "      <td>ZJ2X3I5QF0</td>\n",
       "      <td>UHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2421</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>0</td>\n",
       "      <td>DPM69KA4IL</td>\n",
       "      <td>AUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>748</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cardiac_rehab_U</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>0</td>\n",
       "      <td>PBAEMZTR8Q</td>\n",
       "      <td>AHU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>...</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>0</td>\n",
       "      <td>XP956YC525</td>\n",
       "      <td>HAU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     29          28            27          26           25  \\\n",
       "0     78  <pad>       <pad>         <pad>       <pad>        <pad>   \n",
       "1   1957  <pad>  ACL_tear_N  cut_finger_N  eye_exam_N  hay_fever_N   \n",
       "2   2421  <pad>       <pad>         <pad>       <pad>        <pad>   \n",
       "3    748  <pad>       <pad>         <pad>       <pad>        <pad>   \n",
       "4     52  <pad>       <pad>         <pad>       <pad>        <pad>   \n",
       "\n",
       "                 24              23            22                 21  ...  \\\n",
       "0             <pad>           <pad>         <pad>              <pad>  ...   \n",
       "1  ACE_inhibitors_U  ingrown_nail_N    backache_N  annual_physical_N  ...   \n",
       "2             <pad>           <pad>         <pad>              <pad>  ...   \n",
       "3             <pad>           <pad>         <pad>              <pad>  ...   \n",
       "4       foot_pain_N  ingrown_nail_N  cut_finger_N      dental_exam_N  ...   \n",
       "\n",
       "                6                5               4                  3  \\\n",
       "0  ingrown_nail_N     cut_finger_N           CHF_A       cut_finger_N   \n",
       "1     cold_sore_N            AMI_A     hay_fever_N  annual_physical_N   \n",
       "2  ingrown_nail_N    quad_injury_N  ankle_sprain_N     ingrown_nail_N   \n",
       "3      ACL_tear_N  cardiac_rehab_U        myopia_N         eye_exam_N   \n",
       "4      backache_N      hay_fever_N     foot_pain_N         headache_N   \n",
       "\n",
       "                  2                  1             0 label  patient_id  \\\n",
       "0  peanut_allergy_N  annual_physical_N  cut_finger_N     1  NY60PULF9H   \n",
       "1       cold_sore_N         backache_N  cut_finger_N     1  ZJ2X3I5QF0   \n",
       "2       foot_pain_N      quad_injury_N    eye_exam_N     0  DPM69KA4IL   \n",
       "3          myopia_N        hay_fever_N    eye_exam_N     0  PBAEMZTR8Q   \n",
       "4       foot_pain_N        foot_pain_N   foot_pain_N     0  XP956YC525   \n",
       "\n",
       "  seq_event  \n",
       "0       UHA  \n",
       "1       UHA  \n",
       "2       AUH  \n",
       "3       AHU  \n",
       "4       HAU  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = xgb_utils.get_valid_tokens(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train data to S3...\n",
      "Copying val data to S3...\n",
      "Copying test data to S3...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    xgb_utils.prepare_data(\n",
    "        train_data_path,\n",
    "        x_train_one_hot_path,\n",
    "        x_train_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        valid_data_path,\n",
    "        x_valid_one_hot_path,\n",
    "        x_valid_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        test_data_path,\n",
    "        x_test_one_hot_path,\n",
    "        x_test_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "else:\n",
    "    local_dir = os.path.dirname(x_train_data_path)\n",
    "    xgb_utils.copy_data_to_s3(local_dir, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xgb_classifier(model_path, model_params, feature_names=None):\n",
    "    \"\"\"Load XGBClassifier model from saved Booster model.\"\"\"\n",
    "    xgb_model = xgb.XGBClassifier(**model_params)\n",
    "    if model_path.endswith(\".gz\"):\n",
    "        xgb_model._Booster = xgb_utils.load_model(model_path, remove=False)\n",
    "    else:\n",
    "        xgb_model._Booster = sj_utils.load_pickle(model_path)\n",
    "\n",
    "    if feature_names is not None:\n",
    "        xgb_model._Booster.feature_names = feature_names\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "def get_model_paths(model_save_dir, sort=True):\n",
    "    \"\"\"Get list models paths in sorted order if needed.\"\"\"\n",
    "    fnames = os.listdir(model_save_dir)\n",
    "    if sort:\n",
    "        fnames.sort(key=lambda f: int(re.sub(\"\\D\", \"\", f)))\n",
    "    model_paths = [os.path.join(model_save_dir, fname) for fname in fnames]\n",
    "    return model_paths\n",
    "\n",
    "\n",
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x):\n",
    "    if x.endswith(\"_N\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_model_intersection_similarity_v2(features_scores, absolute=True):\n",
    "    # gt similarity\n",
    "    all_features, all_scores = features_scores[0], features_scores[1]\n",
    "    sims = []\n",
    "    for i, features in enumerate(all_features):\n",
    "        scores = all_scores[i]\n",
    "        gt_features = set([feature for feature in features if is_value(feature)])\n",
    "        n_gt = len(gt_features)\n",
    "        if n_gt > 0:\n",
    "            dict_features_scores = sj_utils.create_dict_features_scores(\n",
    "                features, scores, absolute\n",
    "            )\n",
    "            top_features_scores = sj_utils.top_k(dict_features_scores, len(gt_features))\n",
    "            top_features = top_features_scores[0]\n",
    "            pred_features = [feature for feature in top_features if is_value(feature)]\n",
    "            sim = len(set(pred_features).intersection(gt_features)) / float(n_gt)\n",
    "        else:\n",
    "            sim = -1\n",
    "        sims.append(sim)\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    return avg_sim, sims\n",
    "\n",
    "\n",
    "def compute_shap(xgb_model, df_train0, df_test0, explainer=None):\n",
    "    # Load the copied model\n",
    "    # xgb_model = sj_utils.load_pickle(model_path)\n",
    "\n",
    "    df_train = df_train0.copy()\n",
    "    df_test = df_test0.copy()\n",
    "\n",
    "    feature_names = [\n",
    "        col for col in df_train.columns.tolist() if col not in [\"patient_id\", \"label\"]\n",
    "    ]\n",
    "    X_train = df_train[feature_names]\n",
    "    X_test = df_test[feature_names]\n",
    "\n",
    "    if explainer is None:\n",
    "        explainer = shap.TreeExplainer(xgb_model, X_train)\n",
    "    shap_scores = explainer.shap_values(X_test).tolist()\n",
    "    features = [feature_names[:]] * X_test.shape[0]\n",
    "    patients = df_test.patient_id.tolist()\n",
    "\n",
    "    return ((features, shap_scores, patients), explainer)\n",
    "\n",
    "\n",
    "def save_results(patients, features, shap_scores, y_true, y_pred, output_path):\n",
    "    \"\"\"Save all model training results to file.\"\"\"\n",
    "    results = {}\n",
    "    for i, patient_id in enumerate(patients):\n",
    "        results[patient_id] = {}\n",
    "        results[patient_id][\"features_xgb\"] = features[i]\n",
    "        results[patient_id][\"label\"] = y_true[i]\n",
    "        results[patient_id][\"xgb_pred\"] = y_pred[i]\n",
    "        results[patient_id][\"xgb_shap\"] = shap_scores[i]\n",
    "\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sj_utils.save_pickle(results, output_path, verbose=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_features_and_global_shap(results, exp_num=1):\n",
    "    \"\"\"Get a dataframe of features and global shap values.\"\"\"\n",
    "    features = None\n",
    "    ##Get features and global shap\n",
    "    all_shap = []\n",
    "    for patient_id, result in results.items():\n",
    "        if features is None:\n",
    "            features = results[patient_id][\"features_xgb\"]\n",
    "        all_shap.append(results[patient_id][\"xgb_shap\"])\n",
    "    all_shap = np.absolute(np.array(all_shap)).mean(axis=0)\n",
    "    df_shap = pd.DataFrame()\n",
    "    df_shap[\"features\"] = features\n",
    "    df_shap[\"scores\"] = all_shap\n",
    "    df_shap[\"exp_num\"] = exp_num\n",
    "    return df_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(x_train_one_hot_path)\n",
    "df_val = pd.read_csv(x_valid_one_hot_path)\n",
    "df_test = pd.read_csv(x_test_one_hot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>backache_N</th>\n",
       "      <th>low_salt_diet_U</th>\n",
       "      <th>tachycardia_H</th>\n",
       "      <th>furosemide_H</th>\n",
       "      <th>annual_physical_N</th>\n",
       "      <th>AMI_A</th>\n",
       "      <th>myopia_N</th>\n",
       "      <th>cut_finger_N</th>\n",
       "      <th>ACL_tear_N</th>\n",
       "      <th>...</th>\n",
       "      <th>headache_N</th>\n",
       "      <th>ingrown_nail_N</th>\n",
       "      <th>ARR_A</th>\n",
       "      <th>apnea_H</th>\n",
       "      <th>normal_bmi_U</th>\n",
       "      <th>PH_A</th>\n",
       "      <th>peanut_allergy_N</th>\n",
       "      <th>resistent_hyp_H</th>\n",
       "      <th>hay_fever_N</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY60PULF9H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJ2X3I5QF0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DPM69KA4IL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PBAEMZTR8Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XP956YC525</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  backache_N  low_salt_diet_U  tachycardia_H  furosemide_H  \\\n",
       "0  NY60PULF9H           1                0              0             0   \n",
       "1  ZJ2X3I5QF0           3                0              0             0   \n",
       "2  DPM69KA4IL           0                1              0             0   \n",
       "3  PBAEMZTR8Q           1                0              0             1   \n",
       "4  XP956YC525           2                0              0             0   \n",
       "\n",
       "   annual_physical_N  AMI_A  myopia_N  cut_finger_N  ACL_tear_N  ...  \\\n",
       "0                  1      0         1             3           0  ...   \n",
       "1                  3      1         1             3           2  ...   \n",
       "2                  0      0         0             1           0  ...   \n",
       "3                  0      0         2             1           1  ...   \n",
       "4                  1      1         1             1           0  ...   \n",
       "\n",
       "   headache_N  ingrown_nail_N  ARR_A  apnea_H  normal_bmi_U  PH_A  \\\n",
       "0           0               1      0        0             0     0   \n",
       "1           0               3      0        0             0     0   \n",
       "2           0               2      0        1             0     1   \n",
       "3           2               0      0        0             0     1   \n",
       "4           1               2      0        0             0     0   \n",
       "\n",
       "   peanut_allergy_N  resistent_hyp_H  hay_fever_N  label  \n",
       "0                 1                1            0      1  \n",
       "1                 0                0            2      1  \n",
       "2                 0                0            0      0  \n",
       "3                 2                0            1      0  \n",
       "4                 1                0            4      0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for seq_len=30, label=label...\n",
      ".............."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a1d542086ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmax_parallel_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_PARALLEL_JOBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdata_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/xgboost_utils.py\u001b[0m in \u001b[0;36mtrain_hpo\u001b[0;34m(hyperparameter_ranges, container, execution_role, instance_count, instance_type, output_path, sagemaker_session, eval_metric, objective, objective_metric_name, max_train_jobs, max_parallel_jobs, scale_pos_weight, data_channels)\u001b[0m\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_with_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3181\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhyperparameter\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3182\u001b[0m         \"\"\"\n\u001b[0;32m-> 3183\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tuning_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperParameterTuningJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   4427\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4428\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4429\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4430\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    start = time.time()\n",
    "    print(\"Training for seq_len={}, label={}...\".format(seq_len, label))\n",
    "    # Prepare the input train & validation data path\n",
    "    s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "        s3_data=\"s3://{}/{}/train\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    "    )\n",
    "    s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "        s3_data=\"s3://{}/{}/val\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    "    )\n",
    "\n",
    "    # Class Imbalance\n",
    "    scale_pos_weight = 1.0  # negative/positive\n",
    "\n",
    "    data_channels = {\"train\": s3_input_train, \"validation\": s3_input_validation}\n",
    "\n",
    "    tuner = xgb_utils.train_hpo(\n",
    "        hyperparameter_ranges=HYPERPARAMETER_RANGES,\n",
    "        container=container,\n",
    "        execution_role=role,\n",
    "        instance_count=TRAIN_INSTANCE_COUNT,\n",
    "        instance_type=TRAIN_INSTANCE_TYPE,\n",
    "        output_path=s3_output_path,\n",
    "        sagemaker_session=sess,\n",
    "        eval_metric=EVALUATION_METRIC,\n",
    "        objective=OBJECTIVE,\n",
    "        objective_metric_name=OBJECTIVE_METRIC_NAME,\n",
    "        max_train_jobs=MAX_TRAIN_JOBS,\n",
    "        max_parallel_jobs=MAX_PARALLEL_JOBS,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        data_channels=data_channels,\n",
    "    )\n",
    "\n",
    "    # Get tuning job name\n",
    "    smclient = boto3.Session().client(\"sagemaker\")\n",
    "    tuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    "    )\n",
    "    TUNING_JOB_NAME = tuning_job_result[\"HyperParameterTuningJobName\"]\n",
    "    \n",
    "    # Save Model Parameters\n",
    "    with open(PARAMS_PATH, \"wb\") as fp:\n",
    "        pickle.dump(HYPERPARAMETER_RANGES, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    tuner = sagemaker.HyperparameterTuningJobAnalytics(TUNING_JOB_NAME)\n",
    "    is_minimize = True\n",
    "\n",
    "    full_df = tuner.dataframe()\n",
    "\n",
    "    if len(full_df) > 0:\n",
    "        df_hpo = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "        if len(df_hpo) > 0:\n",
    "            df_hpo = df_hpo.sort_values(\"FinalObjectiveValue\", ascending=is_minimize)\n",
    "            pd.set_option(\"display.max_colwidth\", -1)  # Don't truncate TrainingJobName\n",
    "        else:\n",
    "            print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "        # Add other info\n",
    "        df_hpo[\"model\"] = model_name\n",
    "        df_hpo[\"dataset\"] = dataset\n",
    "        df_hpo[\"seq_len\"] = seq_len\n",
    "    else:\n",
    "        print(\"No Training Job Found!\")\n",
    "\n",
    "    print(df_hpo.shape)\n",
    "    df_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_copy_training_job(row, s3_output_dir, local_output_dir):\n",
    "    \"\"\"Get a training job from s3 and copy to local dir.\"\"\"\n",
    "    job_name = row[\"TrainingJobName\"]\n",
    "    s3_model_path = os.path.join(s3_output_dir, job_name, \"output/model.tar.gz\")\n",
    "\n",
    "    if not os.path.exists(local_output_dir):\n",
    "        os.makedirs(local_output_dir)\n",
    "\n",
    "    output_fname = f\"{job_name}.tar.gz\"\n",
    "    local_model_path = os.path.join(local_output_dir, output_fname)\n",
    "\n",
    "    command = f\"aws s3 cp --quiet {s3_model_path} {local_model_path}\"\n",
    "    os.system(command)\n",
    "    return s3_model_path, local_model_path\n",
    "\n",
    "\n",
    "def save_hpo_results(df, output_path):\n",
    "    \"\"\"Save HPO training jobs metadata.\"\"\"\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    df[\"epoch\"] = range(1, len(df) + 1)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    print(\"Copying model artifacts from s3 to local...\")\n",
    "    model_paths = df_hpo.apply(\n",
    "        get_and_copy_training_job, args=(s3_output_path, local_model_dir), axis=1\n",
    "    )\n",
    "    s3_paths = [path[0] for path in model_paths]\n",
    "    local_paths = [path[1] for path in model_paths]\n",
    "    df_hpo[\"s3_path\"] = s3_paths\n",
    "    df_hpo[\"local_path\"] = local_paths\n",
    "\n",
    "    # print(f\"Saving hpo train results to {output_results_path}...\")\n",
    "    save_hpo_results(df_hpo, hpo_output_results_path)\n",
    "    print(\"SUCCESS!\")\n",
    "else:\n",
    "    df_hpo = pd.read_csv(hpo_output_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = [\n",
    "    col for col in df_train.columns.tolist() if col not in [\"patient_id\", \"label\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of models paths\n",
    "# model_paths = get_model_paths(model_save_dir, sort=False)\n",
    "model_paths = df_hpo[\"local_path\"].values.tolist()\n",
    "# model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(training_job_name, hpo_df):\n",
    "    \"\"\"Get the corresponding parameters for a given training job.\"\"\"\n",
    "    params = dict(hpo_df[hpo_df[\"TrainingJobName\"] == job_name].iloc[0, :12])\n",
    "    # Convert to int datatype to be compatible with the expectation\n",
    "    params[\"gamma\"] = int(params[\"gamma\"])\n",
    "    params[\"max_delta_step\"] = int(params[\"max_delta_step\"])\n",
    "    params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "    params[\"num_round\"] = int(params[\"num_round\"])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    p_rbo = 0.9\n",
    "\n",
    "    train_aucs = []\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    gain_permute_rbo_scores = []\n",
    "    gain_permute_tau_scores = []\n",
    "\n",
    "    val_shap_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "\n",
    "    val_gain_shap_rbo_lst = []\n",
    "    val_gain_shap_tau_lst = []\n",
    "\n",
    "    epochs = []\n",
    "    for model_path in model_paths:\n",
    "        feature_names = [\n",
    "            col\n",
    "            for col in df_train.columns.tolist()\n",
    "            if col not in [\"patient_id\", \"label\"]\n",
    "        ]\n",
    "        # Get epoch number from the model path\n",
    "        model_fname = model_path\n",
    "        model_fname = os.path.basename(model_path)\n",
    "\n",
    "        job_name = os.path.basename(model_path)\n",
    "        job_name = job_name.split(\".\")[0]\n",
    "\n",
    "        epoch = df_hpo[df_hpo[\"TrainingJobName\"] == job_name][\"epoch\"]\n",
    "        epoch = int(epoch.values[0])\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        model_params = get_params(job_name, df_hpo)\n",
    "        xgb_model = load_xgb_classifier(model_path, model_params, feature_names)\n",
    "\n",
    "        # feature_importances = xgb_model.feature_importances_\n",
    "        feature_names = xgb_model.get_booster().feature_names\n",
    "\n",
    "        # Compute AUCs\n",
    "        train_y_true = df_train.label\n",
    "        train_y_pred = xgb_model.predict_proba(df_train[feature_names])[:, 1]\n",
    "        train_auc = roc_auc_score(train_y_true, train_y_pred)\n",
    "        train_aucs.append(train_auc)\n",
    "\n",
    "        val_y_true = df_val.label\n",
    "        val_y_pred = xgb_model.predict_proba(df_val[feature_names])[:, 1]\n",
    "        val_auc = roc_auc_score(val_y_true, val_y_pred)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        test_y_true = df_test.label\n",
    "        test_y_pred = xgb_model.predict_proba(df_test[feature_names])[:, 1]\n",
    "        test_auc = roc_auc_score(test_y_true, test_y_pred)\n",
    "        test_aucs.append(test_auc)\n",
    "\n",
    "        # Losses\n",
    "        train_loss = log_loss(train_y_true, train_y_pred)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss = log_loss(val_y_true, val_y_pred)\n",
    "        val_losses.append(val_loss)\n",
    "        test_loss = log_loss(test_y_true, test_y_pred)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        xgb_gain = pd.DataFrame(\n",
    "            (xgb_model.get_booster().get_score(importance_type=\"gain\")).items()\n",
    "        )\n",
    "        xgb_gain.columns = [\"features\", \"xgb_gain\"]\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"sk_gain\"]\n",
    "\n",
    "        df_gains = xgb_gain.merge(feat_imp, how=\"inner\", on=[\"features\"])\n",
    "\n",
    "        df_gains[\"xgb_rank\"] = df_gains[\"xgb_gain\"].rank(ascending=False)\n",
    "        df_gains[\"sk_rank\"] = df_gains[\"sk_gain\"].rank(ascending=False)\n",
    "        df_gains.sort_values(\"xgb_gain\", ascending=False)\n",
    "        # print(df_gains)\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"scores\"]\n",
    "        feat_imp.set_index(\"features\", inplace=True)\n",
    "        feat_imp.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        # feat_imp.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Model Feature Importance Scores\")\n",
    "\n",
    "        # Check permutation\n",
    "        perm_imp = permutation_importance(\n",
    "            xgb_model, df_val[feature_names], df_val.label\n",
    "        )\n",
    "\n",
    "        perm_df = pd.DataFrame()\n",
    "        perm_df[\"features\"] = feature_names\n",
    "        perm_df[\"scores\"] = perm_imp.importances_mean\n",
    "        perm_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        perm_df.set_index(\"features\", inplace=True)\n",
    "        # perm_df.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Permutation Importance Scores\")\n",
    "\n",
    "        vals = feat_imp.merge(perm_df, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"permute_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        permute_rank = vals.sort_values(\"permute_rank\").index\n",
    "\n",
    "        # Plot for different p values\n",
    "        #     rbo_scores = []\n",
    "        #     p_vals = []\n",
    "        #     p_range = range(1, 10)\n",
    "        #     for p in p_range:\n",
    "        #         rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "        #             p=1.0 / p\n",
    "        #         )\n",
    "        #         rbo_scores.append(rbo_score)\n",
    "        #         p_vals.append(1.0 / p)\n",
    "\n",
    "        #     # plt.figure(figsize=(10, 5))\n",
    "        #     plt.plot(p_vals, rbo_scores, marker=\"x\")\n",
    "        #     plt.title(\"RBO Rankings for different p values.\")\n",
    "        #     plt.xlabel(\"p\")\n",
    "        #     plt.ylabel(\"RBO\")\n",
    "\n",
    "        rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "            p=p_rbo\n",
    "        )\n",
    "        gain_permute_rbo_scores.append(rbo_score)\n",
    "        tau_score = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        gain_permute_tau_scores.append(tau_score)\n",
    "\n",
    "        val_shap_results, explainer = compute_shap(\n",
    "            xgb_model, df_train, df_val, explainer=None\n",
    "        )\n",
    "        (val_features, val_scores, val_patients) = val_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (val_features, val_scores), absolute=True\n",
    "        )\n",
    "        val_shap_sim_lst.append(avg_sim)\n",
    "\n",
    "        # Compute the shap global feature importance\n",
    "        shap_global = np.absolute(np.array(val_scores)).mean(axis=0)\n",
    "        df_shap = pd.DataFrame()\n",
    "        df_shap[\"features\"] = feature_names\n",
    "        df_shap[\"scores\"] = shap_global.tolist()\n",
    "        df_shap.set_index(\"features\", inplace=True)\n",
    "        # df_shap[\"shap_rank\"] = df_shap.scores.rank(ascending=False)\n",
    "        # df_shap.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "        vals = feat_imp.merge(df_shap, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"shap_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        shap_rank = vals.sort_values(\"shap_rank\").index\n",
    "\n",
    "        rbo_score_shap = rbo.RankingSimilarity(gain_rank.values, shap_rank.values).rbo(\n",
    "            p=p_rbo\n",
    "        )\n",
    "        val_gain_shap_rbo_lst.append(rbo_score_shap)\n",
    "\n",
    "        tau_score_shap = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        val_gain_shap_tau_lst.append(tau_score_shap)\n",
    "\n",
    "        test_shap_results, _ = compute_shap(xgb_model, df_train, df_test, explainer)\n",
    "        (test_features, test_scores, test_patients) = test_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (test_features, test_scores), absolute=True\n",
    "        )\n",
    "        test_shap_sim_lst.append(avg_sim)\n",
    "\n",
    "        # Save training results to file.\n",
    "        shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "        results = save_results(\n",
    "            val_patients, val_features, val_scores, val_y_true, val_y_pred, shap_path\n",
    "        )\n",
    "\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        results = save_results(\n",
    "            test_patients,\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_y_true,\n",
    "            test_y_pred,\n",
    "            shap_path,\n",
    "        )\n",
    "        del xgb_model\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02} | Train Loss={train_loss:.4} | Train AUC={train_auc:.4} | Val Loss={val_loss:.4} | Val AUC={val_auc:.4} | RBO(p={p_rbo})={rbo_score:.4} | TAU={tau_score:.4}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Aggregate results\n",
    "    columns = [\n",
    "        \"epoch\",\n",
    "        \"train_AUC\",\n",
    "        \"val_AUC\",\n",
    "        \"test_AUC\",\n",
    "        \"train_Loss\",\n",
    "        \"val_Loss\",\n",
    "        \"test_Loss\",\n",
    "        \"gain_permute_rbo\",\n",
    "        \"gain_permute_tau\",\n",
    "        \"val_GT_shap_sim\",\n",
    "        \"test_GT_shap_sim\",\n",
    "        \"val_gain_shap_rbo\",\n",
    "        \"val_gain_shap_tau\",\n",
    "    ]\n",
    "\n",
    "    df_results = pd.DataFrame(\n",
    "        np.array(\n",
    "            [\n",
    "                epochs,\n",
    "                train_aucs,\n",
    "                val_aucs,\n",
    "                test_aucs,\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                test_losses,\n",
    "                gain_permute_rbo_scores,\n",
    "                gain_permute_tau_scores,\n",
    "                val_shap_sim_lst,\n",
    "                test_shap_sim_lst,\n",
    "                val_gain_shap_rbo_lst,\n",
    "                val_gain_shap_tau_lst,\n",
    "            ]\n",
    "        ).T,\n",
    "        columns=columns,\n",
    "    )\n",
    "    df_results[\"epoch\"] = df_results[\"epoch\"].astype(int)\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "    # save results summary\n",
    "    output_dir = os.path.dirname(output_results_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_results.to_csv(output_results_path)\n",
    "else:\n",
    "    # save results summary\n",
    "    df_results = pd.read_csv(output_results_path)\n",
    "    df_results.set_index(\"epoch\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"SHAP vs GT Similarity on All Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_gain_shap_rbo\", \"val_gain_shap_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs SHAP with RBO/Kendall-T on Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_rbo\", \"gain_permute_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs Permutation Importance with RBO/Kendall-T\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_rbo\", \"gain_permute_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Gain/Permutation with RBO/Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = len(model_paths)\n",
    "step = int(n_jobs / 2) - 1\n",
    "# epochs = range(1, len(model_paths) + 1, 2)\n",
    "step = 1\n",
    "epochs = range(1, len(model_paths) + 1, step)\n",
    "figsize = (15, 10)\n",
    "for epoch in epochs:\n",
    "    shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "    print(shap_path)\n",
    "    val_results = sj_utils.load_pickle(shap_path)\n",
    "    df_shap = get_features_and_global_shap(val_results)\n",
    "    df_shap = df_shap.sort_values(\"scores\", ascending=True)\n",
    "    df_shap.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=\"scores\",\n",
    "        kind=\"barh\",\n",
    "    )\n",
    "    plt.title(f\"Global Feature Importance (epoch={epoch})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(train_data_path)\n",
    "patients_path = os.path.join(data_dir, \"visualized_test_patients.txt\")\n",
    "\n",
    "with open(patients_path, \"r\") as fp:\n",
    "    selected_patients = fp.readlines()\n",
    "    selected_patients = [pat.strip() for pat in selected_patients]\n",
    "\n",
    "selected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = len(model_paths)\n",
    "step = int(n_jobs / 2) - 1\n",
    "# epochs = range(1, len(model_paths) + 1, 2)\n",
    "epochs = range(1, len(model_paths) + 1, step)\n",
    "for patient_id in selected_patients:\n",
    "    df = pd.DataFrame()\n",
    "    label = None\n",
    "    pred_prob = None\n",
    "    for epoch in epochs:\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        test_results = sj_utils.load_pickle(shap_path)\n",
    "        if not len(df):\n",
    "            features = test_results[patient_id][\"features_xgb\"]\n",
    "            df[\"features\"] = features\n",
    "            df[\"values\"] = df_test[features][\n",
    "                df_test[\"patient_id\"] == patient_id\n",
    "            ].values[0]\n",
    "            df[\"features\"] = df[\"values\"].astype(str) + \"_\" + df[\"features\"]\n",
    "            label = test_results[patient_id][\"label\"]\n",
    "        pred_prob = test_results[patient_id][\"xgb_pred\"]\n",
    "        df[f\"epoch_{epoch}\"] = test_results[patient_id][\"xgb_shap\"]\n",
    "    figsize = (15, 10)\n",
    "    epoch_cols = [f\"epoch_{epoch}\" for epoch in epochs]\n",
    "    df.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=epoch_cols,\n",
    "        kind=\"bar\",\n",
    "        width=0.8,\n",
    "    )\n",
    "    plt.title(f\"SHAP Scores for {patient_id}: label={label}, pred={pred_prob:.4}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best models results to S3\n",
    "# n_exps = 10\n",
    "# for n_exp in range(1, n_exps + 1):\n",
    "#     print(f\"Copying best model data to s3 for exp={n_exp}...\")\n",
    "#     summary_path = f\"output/Final_HPO_Tuned/{seq_len}/{n_exp:02}/{model_name}/train_results/results.csv\"\n",
    "#     df_summary = pd.read_csv(summary_path)\n",
    "#     df_summary.set_index(\"epoch\", inplace=True)\n",
    "#     best_epoch = df_summary[\"val_GT_shap_sim\"].idxmax()\n",
    "\n",
    "#     model_path = f\"./output/Final_HPO_Tuned/30/10/xgb/models/model_{best_epoch}.pkl\"\n",
    "#     val_best_path = f\"./output/Final_HPO_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/val_shap_{best_epoch}.pkl\"\n",
    "#     test_best_path = f\"./output/Final_HPO_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/test_shap_{best_epoch}.pkl\"\n",
    "\n",
    "#     s3_dir = f\"s3://merck-paper-bucket/Synthetic-events/final_event_30_xgb/{n_exp}/\"\n",
    "\n",
    "#     # copy files to s3\n",
    "#     command_pattern = f\"aws s3 cp {'{}'} {s3_dir}\"\n",
    "\n",
    "#     command = command_pattern.format(summary_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(model_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(val_best_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(test_best_path)\n",
    "#     os.system(command)\n",
    "# print(\"Models data successfully copied to S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
