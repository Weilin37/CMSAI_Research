{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP, LRP & attention scores with Attention-LSTM\n",
    "Author: Lin Lee Cheong <br>\n",
    "Modified by: Tesfagabir Meharizghi\n",
    "Date created: 1/13/2021 <br>\n",
    "Date updated: 2/10/2021 <br>\n",
    "\n",
    "**Data:** <br>\n",
    "Using the final version of 30 sequence length dataset (sequence-based) generated by Tes<br>\n",
    "Train, validation (for model training), test (for performance etc), and example (4 output)\n",
    "<br>\n",
    "\n",
    "\n",
    "**Steps:** <br>\n",
    "1. Read in datasets [DONE]\n",
    "2. LSTM model training \n",
    "    - TODO: check probab outputs\n",
    "    - save epoch train, val, loss, etc [DONE]\n",
    "    - calculated SHAP & relevance scores for val and test sets [DONE]\n",
    "    - calculate rbo, tau for val and test sets [DONE]\n",
    "    - plot rbo, tau\n",
    "3. Extract SHAP, attention and relevance scores for a TEST set\n",
    "    - calculate SHAP, relevance scores, performance (AUC, test loss)[DONE]\n",
    "    - calculate rbo, tau [DONE]\n",
    "3. Extract SHAP and relevance scores for example set of 4\n",
    "    - plot epoch evolution [DONE]\n",
    "    - add attention [DONE]\n",
    "4. Save output in dict format[DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install shap\n",
    "#!pip install xgboost\n",
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo\n",
    "\n",
    "#!pip install nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import newaxis as na\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n",
    "\n",
    "# from lrp_att_model import *\n",
    "import shap_jacc_utils as sj_utils\n",
    "\n",
    "from cdiff_utils import *\n",
    "\n",
    "# import rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm-att-lrp\"\n",
    "\n",
    "NROWS = 1000  # 1e9\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SEQ_LEN = 200\n",
    "\n",
    "N_EPOCHS = 9\n",
    "\n",
    "TARGET_COLNAME = \"d_00845\"\n",
    "UID_COLNAME = \"patient_id\"\n",
    "TARGET_VALUE = \"1\"\n",
    "\n",
    "DATA_TYPE = \"downsampled\"\n",
    "FNAME = \"all\"\n",
    "SAVE_DATASET = True\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "# Model Parameters\n",
    "MODEL_PARAMS = {\n",
    "    # Dataset/vocab related\n",
    "    \"min_freq\": 500,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    # Model related parameters\n",
    "    \"embedding_dim\": 30,  # 30\n",
    "    \"hidden_dim\": 30,  # 30\n",
    "    \"nlayers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"linear_bias\": False,\n",
    "    \"init_type\": \"zero\",  # zero/learned\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"scheduler_step\": 3,\n",
    "    \"clip\": True,\n",
    "    \"rev\": False,\n",
    "    # SHAP-related parameters\n",
    "    \"n_background\": 300,  # Number of background examples\n",
    "    \"background_negative_only\": False,  # If negative examples are used as background\n",
    "    \"background_positive_only\": False,\n",
    "    \"test_positive_only\": False,\n",
    "    \"is_test_random\": False,\n",
    "    \"n_valid_examples\": BATCH_SIZE,  # Number of validation examples to be used during shap computation\n",
    "    \"n_test_examples\": BATCH_SIZE,  # Number of the final test examples to be used in shap computation #TODO\n",
    "}\n",
    "\n",
    "\n",
    "# ALL_DOWN_DATA_PATH = (\n",
    "#     f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/all.csv\"\n",
    "# )\n",
    "\n",
    "# MONTH_DATA_PATH = (\n",
    "#     f\"../../../data/AE_CDiff_d00845/output/data/1000/original/preprocessed/20110101.csv\"\n",
    "# )\n",
    "\n",
    "TRAIN_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/train.csv\"\n",
    "VALID_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/val.csv\"\n",
    "TEST_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/test.csv\"\n",
    "SELECTED_EXAMPLES_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/visualized_test_patients.txt\"\n",
    "\n",
    "OUT_TRAIN_DATA_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/{MODEL_PARAMS['min_freq']}/train.csv\"\n",
    "OUT_VALID_DATA_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/{MODEL_PARAMS['min_freq']}/val.csv\"\n",
    "OUT_TEST_DATA_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/{MODEL_PARAMS['min_freq']}/test.csv\"\n",
    "OUT_SELECTED_EXAMPLES_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/{MODEL_PARAMS['min_freq']}/visualized_test_patients.csv\"\n",
    "\n",
    "VOCAB_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/{MODEL_PARAMS['min_freq']}/vocab.pkl\"\n",
    "\n",
    "GT_CODES_PATH = \"../../../data/AE_CDiff_d00845/cdiff_risk_factors_codes.csv\"\n",
    "\n",
    "MODEL_SAVE_PATH_PATTERN = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/{FNAME}/{MODEL_PARAMS['min_freq']}/model_weights/model_{'{}'}.pkl\"\n",
    "SHAP_SAVE_DIR_PATTERN = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/{FNAME}/{MODEL_PARAMS['min_freq']}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split\n",
    "\n",
    "OUTPUT_RESULTS_PATH = f\"output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/{FNAME}/{MODEL_PARAMS['min_freq']}/train_results/results.csv\"\n",
    "PARAMS_PATH = f\"output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/{FNAME}/{MODEL_PARAMS['min_freq']}/train_results/model_params.json\"\n",
    "\n",
    "# --------------------\n",
    "# From the original AE parameters\n",
    "# batch_size = 1024\n",
    "# N_EPOCHS = 20\n",
    "\n",
    "# EMBEDDING_DIM = 30\n",
    "# HIDDEN_DIM = 30\n",
    "# BIDIRECTIONAL = False\n",
    "# DROPOUT = 0.3\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if needed\n",
    "model_dir = os.path.dirname(MODEL_SAVE_PATH_PATTERN)\n",
    "shap_dir = os.path.dirname(SHAP_SAVE_DIR_PATTERN)\n",
    "output_dir = os.path.dirname(OUTPUT_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created: ./output/AE_CDiff/200/downsampled/lstm-att-lrp/all/500/model_weights\n",
      "Directory Created: ./output/AE_CDiff/200/downsampled/lstm-att-lrp/all/500/shap\n",
      "Directory Created: output/AE_CDiff/200/downsampled/lstm-att-lrp/all/500/train_results\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    if os.path.exists(shap_dir):\n",
    "        shutil.rmtree(shap_dir)\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(model_dir)\n",
    "    os.makedirs(shap_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory Created: {model_dir}\")\n",
    "    print(f\"Directory Created: {shap_dir}\")\n",
    "    print(f\"Directory Created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_codes = pd.read_csv(GT_CODES_PATH)\n",
    "gt_codes = list(set(gt_codes.Internal_Code))\n",
    "len(gt_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_MODEL:\n",
    "#     df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "#     df_val = pd.read_csv(VALID_DATA_PATH)\n",
    "#     df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "#     # Take upto seq_len cols\n",
    "#     # cols = df.columns.tolist()\n",
    "#     # exclude = [str(i) for i in range(1000, SEQ_LEN, -1)]\n",
    "#     # cols = [col for col in cols if col not in exclude]\n",
    "#     # df = df[cols]\n",
    "#     # df.head()\n",
    "\n",
    "#     cols = [str(i) for i in range(SEQ_LEN - 1, -1, -1)]\n",
    "#     vocab = Counter(df[cols].values.flatten().tolist())\n",
    "\n",
    "#     # Remove tokens not in gt_codes\n",
    "#     for token in list(vocab):\n",
    "#         if (token not in gt_codes) or (vocab[token] < MODEL_PARAMS[\"min_freq\"]):\n",
    "#             del vocab[token]\n",
    "#     print(len(vocab))\n",
    "#     gt_codes2 = sorted(vocab)\n",
    "\n",
    "#     df[\"num_gt_codes\"] = df.apply(\n",
    "#         get_gt_code_patient, args=(gt_codes2, SEQ_LEN), axis=1\n",
    "#     )\n",
    "#     df[\"has_gt_codes\"] = (df[\"num_gt_codes\"] > 0).astype(int)\n",
    "#     df = df.sort_values(\"has_gt_codes\", ascending=False)\n",
    "\n",
    "#     df_val[\"num_gt_codes\"] = df_val.apply(\n",
    "#         get_gt_code_patient, args=(gt_codes2, SEQ_LEN), axis=1\n",
    "#     )\n",
    "#     df_val[\"has_gt_codes\"] = (df_val[\"num_gt_codes\"] > 0).astype(int)\n",
    "#     df_val = df_val.sort_values(\"has_gt_codes\", ascending=False)\n",
    "\n",
    "#     df_test[\"num_gt_codes\"] = df_test.apply(\n",
    "#         get_gt_code_patient, args=(gt_codes2, SEQ_LEN), axis=1\n",
    "#     )\n",
    "#     df_test[\"has_gt_codes\"] = (df_test[\"num_gt_codes\"] > 0).astype(int)\n",
    "#     df_test = df_test.sort_values(\"has_gt_codes\", ascending=False)\n",
    "\n",
    "#     out_dir = os.path.dirname(OUT_TRAIN_DATA_PATH)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     df.to_csv(OUT_TRAIN_DATA_PATH, index=False)\n",
    "#     df_val.to_csv(OUT_VALID_DATA_PATH, index=False)\n",
    "#     df_test.to_csv(OUT_TEST_DATA_PATH, index=False)\n",
    "# else:\n",
    "#     pass\n",
    "# #     df_train = pd.read_csv(OUT_TRAIN_DATA_PATH)\n",
    "# #     df_val = pd.read_csv(OUT_VALID_DATA_PATH)\n",
    "# #     df_test = pd.read_csv(OUT_TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15386, 1004)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>999</th>\n",
       "      <th>998</th>\n",
       "      <th>997</th>\n",
       "      <th>996</th>\n",
       "      <th>995</th>\n",
       "      <th>994</th>\n",
       "      <th>993</th>\n",
       "      <th>992</th>\n",
       "      <th>991</th>\n",
       "      <th>990</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>d_00845</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_gt_codes</th>\n",
       "      <th>has_gt_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_94760</td>\n",
       "      <td>h_99214</td>\n",
       "      <td>8_days</td>\n",
       "      <td>d_4580</td>\n",
       "      <td>d_7202</td>\n",
       "      <td>h_99214</td>\n",
       "      <td>0</td>\n",
       "      <td>3FX9XX09I_20111101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_4019</td>\n",
       "      <td>h_99308</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_00845</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>1</td>\n",
       "      <td>J0BKBDW32_20110701</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>6_days</td>\n",
       "      <td>d_00845</td>\n",
       "      <td>h_99223</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_70700</td>\n",
       "      <td>h_99309</td>\n",
       "      <td>1</td>\n",
       "      <td>XPNM3HIFJ_20110201</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_E1390</td>\n",
       "      <td>h_E1392</td>\n",
       "      <td>28_days</td>\n",
       "      <td>d_496</td>\n",
       "      <td>h_E1390</td>\n",
       "      <td>h_E1392</td>\n",
       "      <td>0</td>\n",
       "      <td>RAP7LU7MF_20110401</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_99859</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_2800</td>\n",
       "      <td>d_99859</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>1</td>\n",
       "      <td>JGD47E2TP_20111001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     999    998    997    996    995    994    993    992    991    990  ...  \\\n",
       "0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "         5        4        3       2        1        0 d_00845  \\\n",
       "0  h_94760  h_99214   8_days  d_4580   d_7202  h_99214       0   \n",
       "1   1_days   d_4019  h_99308  1_days  d_00845  h_99232       1   \n",
       "2   6_days  d_00845  h_99223  1_days  d_70700  h_99309       1   \n",
       "3  h_E1390  h_E1392  28_days   d_496  h_E1390  h_E1392       0   \n",
       "4  d_99859  h_99232   1_days  d_2800  d_99859  h_99232       1   \n",
       "\n",
       "           patient_id num_gt_codes has_gt_codes  \n",
       "0  3FX9XX09I_20111101            1            1  \n",
       "1  J0BKBDW32_20110701            1            1  \n",
       "2  XPNM3HIFJ_20110201            3            1  \n",
       "3  RAP7LU7MF_20110401            1            1  \n",
       "4  JGD47E2TP_20111001            1            1  \n",
       "\n",
       "[5 rows x 1004 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(OUT_TRAIN_DATA_PATH)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Examples with and without Risk Factors\n",
      "0    8140\n",
      "1    7246\n",
      "Name: has_gt_codes, dtype: int64\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"# of Examples with and without Risk Factors\")\n",
    "print(df_train.has_gt_codes.value_counts())\n",
    "print(\"-\" * 20)\n",
    "df_train[\"num_gt_codes\"].plot.hist(bins=10)\n",
    "plt.xlabel(\"Number of Accurrences of Ground Truth Risk Factors/Patient\")\n",
    "plt.title(\"Histogram of GT Risk Factors per Patient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(OUT_TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_EXAMPLES_PATH = OUT_SELECTED_EXAMPLES_PATH.replace(\".csv\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    pos_examples = df_test[df_test[\"d_00845\"] == 1][\"patient_id\"].iloc[:2].tolist()\n",
    "    neg_examples = df_test[df_test[\"d_00845\"] == 0][\"patient_id\"].iloc[:2].tolist()\n",
    "    test_examples = pos_examples + neg_examples\n",
    "\n",
    "    with open(SELECTED_EXAMPLES_PATH, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(test_examples))\n",
    "else:\n",
    "    with open(SELECTED_EXAMPLES_PATH, \"r\") as fp:\n",
    "        test_examples = fp.readlines()\n",
    "        test_examples = [ex.strip() for ex in test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T3KEOAORA_20110401',\n",
       " 'WJVTT33KD_20110901',\n",
       " 'HTK5K07J3_20111101',\n",
       " '4ZV0N5VGS_20110501']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGPU_LST = []\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    for gpu in range(n_gpus):\n",
    "        MULTIGPU_LST.append(f\"cuda:{gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Test Examples: ['T3KEOAORA_20110401', 'WJVTT33KD_20110901', 'HTK5K07J3_20111101', '4ZV0N5VGS_20110501']\n"
     ]
    }
   ],
   "source": [
    "# Load Selected Patients for later SHAP visualization\n",
    "patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "patients = patients.values.flatten().tolist()\n",
    "print(f\"Selected Test Examples: {patients}\")\n",
    "# create the example set\n",
    "selected_patients_path = os.path.join(output_dir, \"selected_test_patients.csv\")\n",
    "test_df = pd.read_csv(OUT_TEST_DATA_PATH)\n",
    "test_df[test_df.patient_id.isin(patients)].to_csv(selected_patients_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3298, 1004)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>999</th>\n",
       "      <th>998</th>\n",
       "      <th>997</th>\n",
       "      <th>996</th>\n",
       "      <th>995</th>\n",
       "      <th>994</th>\n",
       "      <th>993</th>\n",
       "      <th>992</th>\n",
       "      <th>991</th>\n",
       "      <th>990</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>d_00845</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_gt_codes</th>\n",
       "      <th>has_gt_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_73313</td>\n",
       "      <td>h_01936</td>\n",
       "      <td>h_22523</td>\n",
       "      <td>h_72020</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>p_8166</td>\n",
       "      <td>1</td>\n",
       "      <td>T3KEOAORA_20110401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_4019</td>\n",
       "      <td>h_99307</td>\n",
       "      <td>7_days</td>\n",
       "      <td>h_PE104</td>\n",
       "      <td>h_RMC02</td>\n",
       "      <td>h_RVC20</td>\n",
       "      <td>1</td>\n",
       "      <td>WJVTT33KD_20110901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_78002</td>\n",
       "      <td>h_90732</td>\n",
       "      <td>h_99238</td>\n",
       "      <td>h_A0425</td>\n",
       "      <td>h_A0428</td>\n",
       "      <td>h_G0009</td>\n",
       "      <td>1</td>\n",
       "      <td>HNXXP31Z0_20110501</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>15_days</td>\n",
       "      <td>d_32723</td>\n",
       "      <td>h_A7030</td>\n",
       "      <td>h_A7035</td>\n",
       "      <td>h_A7037</td>\n",
       "      <td>h_A7039</td>\n",
       "      <td>1</td>\n",
       "      <td>OFOMG905K_20110801</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_36511</td>\n",
       "      <td>h_2027F</td>\n",
       "      <td>h_3284F</td>\n",
       "      <td>h_92020</td>\n",
       "      <td>h_92083</td>\n",
       "      <td>h_99213</td>\n",
       "      <td>1</td>\n",
       "      <td>ZKL2F4311_20110901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     999    998    997    996    995    994    993    992    991    990  ...  \\\n",
       "0  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "         5        4        3        2        1        0 d_00845  \\\n",
       "0  d_73313  h_01936  h_22523  h_72020  h_99232   p_8166       1   \n",
       "1   d_4019  h_99307   7_days  h_PE104  h_RMC02  h_RVC20       1   \n",
       "2  d_78002  h_90732  h_99238  h_A0425  h_A0428  h_G0009       1   \n",
       "3  15_days  d_32723  h_A7030  h_A7035  h_A7037  h_A7039       1   \n",
       "4  d_36511  h_2027F  h_3284F  h_92020  h_92083  h_99213       1   \n",
       "\n",
       "           patient_id num_gt_codes has_gt_codes  \n",
       "0  T3KEOAORA_20110401            2            1  \n",
       "1  WJVTT33KD_20110901            1            1  \n",
       "2  HNXXP31Z0_20110501            2            1  \n",
       "3  OFOMG905K_20110801            1            1  \n",
       "4  ZKL2F4311_20110901            1            1  \n",
       "\n",
       "[5 rows x 1004 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ./output/AE_CDiff/200/downsampled/lstm-att-lrp/splits/all/500/train.csv..\n",
      "Success!\n",
      "Building dataset from ./output/AE_CDiff/200/downsampled/lstm-att-lrp/splits/all/500/val.csv..\n",
      "Success!\n",
      "Building dataset from ./output/AE_CDiff/200/downsampled/lstm-att-lrp/splits/all/500/test.csv..\n",
      "Success!\n",
      "Building dataset from output/AE_CDiff/200/downsampled/lstm-att-lrp/all/500/train_results/selected_test_patients.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "    OUT_TRAIN_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=None,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "    OUT_VALID_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "    OUT_TEST_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")\n",
    "\n",
    "example_dataset, _ = build_lstm_dataset(\n",
    "    selected_patients_path,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab: 53\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Vocab: {len(vocab._vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GT codes original: 134\n",
      "Total vocab: 53\n",
      "Total GT Available: 4\n"
     ]
    }
   ],
   "source": [
    "gt_codes, _ = get_ground_truth_codes(GT_CODES_PATH, vocab, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and load LRP LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # LOAD Model Parameters\n",
    "    with open(PARAMS_PATH, \"r\") as fp:\n",
    "        MODEL_PARAMS = json.load(fp)\n",
    "\n",
    "lstm_model = AttNoHtLSTM(\n",
    "    MODEL_PARAMS[\"embedding_dim\"],\n",
    "    MODEL_PARAMS[\"hidden_dim\"],\n",
    "    vocab,\n",
    "    model_device,\n",
    "    bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "    nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "    dropout=MODEL_PARAMS[\"dropout\"],\n",
    "    init_type=MODEL_PARAMS[\"init_type\"],\n",
    "    linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttNoHtLSTM(\n",
       "  (emb_layer): Embedding(53, 30, padding_idx=0)\n",
       "  (lstm): LSTM(30, 30, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=60, out_features=1, bias=False)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lstm_model.parameters(), lr=MODEL_PARAMS[\"learning_rate\"], weight_decay=0.03\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, MODEL_PARAMS[\"scheduler_step\"], gamma=0.9\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.0001, weight_decay=0.02)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 11, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm_model.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 5\n",
    "# model_path = MODEL_SAVE_PATH_PATTERN.format(\n",
    "#     str(epoch).zfill(2)\n",
    "# )  # TODO: Remove this once done\n",
    "# print(model_path)\n",
    "# lstm_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = {}\n",
    "test_results = {}\n",
    "\n",
    "rbo_p = 0.8\n",
    "SIMILARITY_FREEDOM = 1  # For Intersection Similarity\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_auc_lst = []\n",
    "    train_loss_lst = []\n",
    "\n",
    "    val_auc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_lrp_sim_lst = []\n",
    "    val_shap_sim_lst = []\n",
    "    val_lrp_shap_rbo_lst = []\n",
    "    val_lrp_shap_tau_lst = []\n",
    "\n",
    "    test_auc_lst = []\n",
    "    test_loss_lst = []\n",
    "    test_lrp_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "    test_lrp_shap_rbo_lst = []\n",
    "    test_lrp_shap_tau_lst = []\n",
    "\n",
    "    val_patient_ids, val_labels, val_idxed_text = next(iter(valid_dataloader))\n",
    "    test_patient_ids, test_labels, test_idxed_text = next(iter(test_dataloader))\n",
    "\n",
    "    # patient_ids, labels, idxed_text = get_sub_valid_data(N_VALID_EXAMPLES, MODEL_PARAMS['batch_size'], valid_dataloader)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        lstm_model.train()\n",
    "        # model training & perf evaluation\n",
    "        train_loss, train_auc = epoch_train_lstm(\n",
    "            lstm_model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            clip=MODEL_PARAMS[\"clip\"],\n",
    "            device=model_device,\n",
    "        )\n",
    "        train_auc_lst.append(train_auc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "\n",
    "        valid_loss, valid_auc = epoch_val_lstm(\n",
    "            lstm_model, valid_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        val_auc_lst.append(valid_auc)\n",
    "        val_loss_lst.append(valid_loss)\n",
    "\n",
    "        test_loss, test_auc = epoch_val_lstm(\n",
    "            lstm_model, test_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        test_auc_lst.append(test_auc)\n",
    "        test_loss_lst.append(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # save model\n",
    "        save_path = MODEL_SAVE_PATH_PATTERN.format(str(epoch).zfill(2))\n",
    "        torch.save(lstm_model.state_dict(), save_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        #         print(\n",
    "        #             f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "        #             + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "        #             + f\"\\t Test. Loss: {test_loss:.4f} | Test. AUC: {test_auc:.4f} \"\n",
    "        #         )\n",
    "        #         continue\n",
    "\n",
    "        # calculate relevancy and SHAP\n",
    "        lstm_model.eval()\n",
    "        lrp_model = LSTM_LRP_MultiLayer(lstm_model.cpu())\n",
    "\n",
    "        # Save valid/test results\n",
    "        valid_results[epoch] = {}\n",
    "        test_results[epoch] = {}\n",
    "\n",
    "        for sel_idx in range(len(val_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in val_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if val_patient_ids[sel_idx] not in valid_results[epoch]:\n",
    "                valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"label\"] = val_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        for sel_idx in range(len(test_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in test_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if test_patient_ids[sel_idx] not in test_results[epoch]:\n",
    "                test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"label\"] = test_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        shap_start_time = time.time()\n",
    "        (\n",
    "            val_features,\n",
    "            val_scores,\n",
    "            val_patients,\n",
    "        ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "            lstm_model.cuda(),\n",
    "            train_dataloader,\n",
    "            valid_dataloader,\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        )\n",
    "\n",
    "        (\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_patients,\n",
    "        ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "            lstm_model.cuda(),\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        )\n",
    "\n",
    "        #         (\n",
    "        #             val_features,\n",
    "        #             val_scores,\n",
    "        #             val_patients,\n",
    "        #         ) = get_lstm_features_and_shap_scores_mp(\n",
    "        #             lstm_model.cpu(),\n",
    "        #             train_dataloader,\n",
    "        #             (val_patient_ids, val_labels, val_idxed_text),\n",
    "        #             SEQ_LEN,\n",
    "        #             \"\",\n",
    "        #             save_output=False,\n",
    "        #             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "        #             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "        #             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "        #             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        #             multigpu_lst=MULTIGPU_LST,  # [\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        #         )\n",
    "\n",
    "        #         (\n",
    "        #             test_features,\n",
    "        #             test_scores,\n",
    "        #             test_patients,\n",
    "        #         ) = get_lstm_features_and_shap_scores_mp(\n",
    "        #             lstm_model.cpu(),\n",
    "        #             train_dataloader,\n",
    "        #             (test_patient_ids, test_labels, test_idxed_text),\n",
    "        #             SEQ_LEN,\n",
    "        #             \"\",\n",
    "        #             save_output=False,\n",
    "        #             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "        #             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "        #             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "        #             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "        #             multigpu_lst=MULTIGPU_LST,  # [\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        #         )\n",
    "\n",
    "        shap_end_time = time.time()\n",
    "        shap_mins, shap_secs = epoch_time(shap_start_time, shap_end_time)\n",
    "\n",
    "        for idx, pid in enumerate(val_patients):\n",
    "            df = valid_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(val_scores[idx])\n",
    "            df[\"shap_scores\"] = val_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        for idx, pid in enumerate(test_patients):\n",
    "            df = test_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(test_scores[idx])\n",
    "            df[\"shap_scores\"] = test_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            test_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        # calculate similarity indexes for val\n",
    "        epoch_val_lrp_shap_t_corr = []\n",
    "        epoch_val_lrp_shap_rbo = []\n",
    "        epoch_val_lrp_sim = []\n",
    "        epoch_val_shap_sim = []\n",
    "\n",
    "        for pid in valid_results[epoch].keys():\n",
    "            imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            valid_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_val_lrp_shap_t_corr.append(\n",
    "                valid_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_val_lrp_shap_rbo.append(valid_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            lrp_scores = imp_df.lrp_scores.values\n",
    "            shap_scores = imp_df.shap_scores.values\n",
    "            tokens = imp_df.u_token\n",
    "            lrp_sim = get_similarity(lrp_scores, tokens, gt_codes, freedom=1)\n",
    "            shap_sim = get_similarity(shap_scores, tokens, gt_codes, freedom=1)\n",
    "            if lrp_sim != -1:\n",
    "                epoch_val_lrp_sim.append(lrp_sim)\n",
    "                epoch_val_shap_sim.append(shap_sim)\n",
    "            #             # gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            #             gt_idx = [x for x, tok in enumerate(imp_df.u_token) if tok in gt_codes]\n",
    "            #             n_gt = len(gt_idx)\n",
    "            #             if n_gt > 0:\n",
    "            #                 lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt + 1]\n",
    "            #                 shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "            #                     : n_gt + 1\n",
    "            #                 ]\n",
    "            #                 lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "            #                 shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "            #                 epoch_val_lrp_sim.append(lrp_sim)\n",
    "            #                 epoch_val_shap_sim.append(shap_sim)\n",
    "            #             else:\n",
    "            #                 lrp_sim = -1\n",
    "            #                 shap_sim = -1\n",
    "            valid_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            valid_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(valid_results[epoch], fp)\n",
    "\n",
    "        val_lrp_shap_rbo_lst.append(np.mean(epoch_val_lrp_shap_rbo))\n",
    "        val_lrp_shap_tau_lst.append(np.mean(epoch_val_lrp_shap_t_corr))\n",
    "        val_lrp_sim_lst.append(np.mean(epoch_val_lrp_sim))\n",
    "        val_shap_sim_lst.append(np.mean(epoch_val_shap_sim))\n",
    "\n",
    "        # calculate similarity indexes for test\n",
    "        epoch_test_lrp_shap_t_corr = []\n",
    "        epoch_test_lrp_shap_rbo = []\n",
    "        epoch_test_lrp_sim = []\n",
    "        epoch_test_shap_sim = []\n",
    "\n",
    "        for pid in test_results[epoch].keys():\n",
    "            imp_df = test_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            test_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            test_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_test_lrp_shap_t_corr.append(\n",
    "                test_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_test_lrp_shap_rbo.append(test_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            lrp_scores = imp_df.lrp_scores.values\n",
    "            shap_scores = imp_df.shap_scores.values\n",
    "            tokens = imp_df.u_token\n",
    "            lrp_sim = get_similarity(\n",
    "                lrp_scores, tokens, gt_codes, freedom=SIMILARITY_FREEDOM\n",
    "            )\n",
    "            shap_sim = get_similarity(\n",
    "                shap_scores, tokens, gt_codes, freedom=SIMILARITY_FREEDOM\n",
    "            )\n",
    "            if lrp_sim != -1:\n",
    "                epoch_test_lrp_sim.append(lrp_sim)\n",
    "                epoch_test_shap_sim.append(shap_sim)\n",
    "\n",
    "            #             #gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            #             gt_idx = [x for x, tok in enumerate(imp_df.u_token) if tok in gt_codes]\n",
    "            #             n_gt = len(gt_idx)\n",
    "            #             if n_gt > 0:\n",
    "            #                 lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt + 1]\n",
    "            #                 shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "            #                     : n_gt + 1\n",
    "            #                 ]\n",
    "            #                 lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "            #                 shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "            #                 epoch_test_lrp_sim.append(lrp_sim)\n",
    "            #                 epoch_test_shap_sim.append(shap_sim)\n",
    "            #             else:\n",
    "            #                 lrp_sim = -1\n",
    "            #                 shap_sim = -1\n",
    "            test_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            test_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(test_results[epoch], fp)\n",
    "\n",
    "        test_lrp_shap_rbo_lst.append(np.mean(epoch_test_lrp_shap_rbo))\n",
    "        test_lrp_shap_tau_lst.append(np.mean(epoch_test_lrp_shap_t_corr))\n",
    "        test_lrp_sim_lst.append(np.mean(epoch_test_lrp_sim))\n",
    "        test_shap_sim_lst.append(np.mean(epoch_test_shap_sim))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | \"\n",
    "            + f\"SHAP Time: {shap_mins}m {shap_secs}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "            + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "            + f\"| Val LRP Sim: {np.mean(epoch_val_lrp_sim):.4f} | Val SHAP Sim: {np.mean(epoch_val_shap_sim):.4f}\"\n",
    "        )\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results[\"epoch\"] = [x for x in range(N_EPOCHS)]\n",
    "    df_results[\"train_AUC\"] = train_auc_lst\n",
    "    df_results[\"train_Loss\"] = train_loss_lst\n",
    "    df_results[\"val_AUC\"] = val_auc_lst\n",
    "    df_results[\"val_Loss\"] = val_loss_lst\n",
    "    df_results[\"test_AUC\"] = test_auc_lst\n",
    "    df_results[\"test_Loss\"] = test_loss_lst\n",
    "    df_results[\"val_lrp_shap_rbo\"] = val_lrp_shap_rbo_lst\n",
    "    df_results[\"val_lrp_shap_tau\"] = val_lrp_shap_tau_lst\n",
    "    df_results[\"test_lrp_shap_rbo\"] = test_lrp_shap_rbo_lst\n",
    "    df_results[\"test_lrp_shap_tau\"] = test_lrp_shap_tau_lst\n",
    "    df_results[\"val_GT_lrp_sim\"] = val_lrp_sim_lst\n",
    "    df_results[\"val_GT_shap_sim\"] = val_shap_sim_lst\n",
    "    df_results[\"test_GT_lrp_sim\"] = test_lrp_sim_lst\n",
    "    df_results[\"test_GT_shap_sim\"] = test_shap_sim_lst\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    df_results.to_csv(OUTPUT_RESULTS_PATH)\n",
    "\n",
    "    # Save Model Parameters\n",
    "    with open(PARAMS_PATH, \"w\") as fp:\n",
    "        json.dump(MODEL_PARAMS, fp)\n",
    "\n",
    "else:\n",
    "    print(\"Loading Training results....\")\n",
    "    df_results = pd.read_csv(OUTPUT_RESULTS_PATH)\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Load valid results.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"rb\") as fp:\n",
    "            valid_results[epoch] = pickle.load(fp)\n",
    "        # Load test results.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"rb\") as fp:\n",
    "            test_results[epoch] = pickle.load(fp)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_EPOCHS = 9\n",
    "# df_results = pd.DataFrame()\n",
    "# df_results[\"epoch\"] = [x for x in range(6, N_EPOCHS)]\n",
    "# df_results[\"train_AUC\"] = train_auc_lst[:3]\n",
    "# df_results[\"train_Loss\"] = train_loss_lst[:3]\n",
    "# df_results[\"val_AUC\"] = val_auc_lst[:3]\n",
    "# df_results[\"val_Loss\"] = val_loss_lst[:3]\n",
    "# df_results[\"test_AUC\"] = test_auc_lst[:3]\n",
    "# df_results[\"test_Loss\"] = test_loss_lst[:3]\n",
    "# df_results[\"val_lrp_shap_rbo\"] = val_lrp_shap_rbo_lst\n",
    "# df_results[\"val_lrp_shap_tau\"] = val_lrp_shap_tau_lst\n",
    "# df_results[\"test_lrp_shap_rbo\"] = test_lrp_shap_rbo_lst\n",
    "# df_results[\"test_lrp_shap_tau\"] = test_lrp_shap_tau_lst\n",
    "# df_results[\"val_GT_lrp_sim\"] = val_lrp_sim_lst\n",
    "# df_results[\"val_GT_shap_sim\"] = val_shap_sim_lst\n",
    "# df_results[\"test_GT_lrp_sim\"] = test_lrp_sim_lst\n",
    "# df_results[\"test_GT_shap_sim\"] = test_shap_sim_lst\n",
    "# df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "# # save results summary\n",
    "# df_results.to_csv(OUTPUT_RESULTS_PATH)\n",
    "\n",
    "# # Save Model Parameters\n",
    "# with open(PARAMS_PATH, \"w\") as fp:\n",
    "#     json.dump(MODEL_PARAMS, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.shape)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"SHAP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_lrp_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_tau\"],  # \"val_lrp_shap_rbo\", ,\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs SHAP with Kendall-T on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_tau\", \"val_AUC\"],  # \"val_lrp_shap_rbo\", ],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"LRP/SHAP with Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 8\n",
    "selected_epochs = [0, 3, 8]\n",
    "# selected_epochs = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "selected_patients = selected_patients.values.flatten().tolist()\n",
    "selected_patients\n",
    "# selected_patients = [\"1OD472J277\", \"XEK4OM00KJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = {}\n",
    "for pat_id in selected_patients:\n",
    "    example_results[pat_id] = {}\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if pat_id in test_results[epoch].keys():\n",
    "            example_results[pat_id][epoch] = test_results[epoch][pat_id]\n",
    "example_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 100\n",
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"LRP for Epoch: \" + str(epoch))\n",
    "    df_lrp = sj_utils.plot_global_feature_importance(\n",
    "        global_scores, max_features=MAX_TOKENS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"SHAP for Epoch: \" + str(epoch))\n",
    "    df_shap = sj_utils.plot_global_feature_importance(\n",
    "        global_scores, max_features=MAX_TOKENS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_lrp = df_lrp.index.tolist()\n",
    "# lst_lrp = [token for token in lst_lrp if \"day\" not in token]\n",
    "\n",
    "# lst_shap = df_shap.index.tolist()\n",
    "# lst_shap = [token for token in lst_shap if \"day\" not in token]\n",
    "\n",
    "# len(set(lst_lrp).intersection(set(lst_shap))) / len(set(lst_lrp + lst_shap)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(lst_lrp).intersection(set(gt_codes))) / len(set(lst_lrp + gt_codes)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(lst_lrp).intersection(set(gt_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(lst_shap).intersection(set(gt_codes))) / len(set(lst_shap + gt_codes)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(lst_shap).intersection(set(gt_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_results[\"171GG8EXI_20111001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 30\n",
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[selected_epochs[-1]].values))[::-1][\n",
    "            :max_tokens\n",
    "        ].tolist()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"SHAP SCORES for {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.axisbelow\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[selected_epochs[-1]].values))[::-1][\n",
    "            :max_tokens\n",
    "        ].tolist()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    #     # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    #     df[selected_epochs].plot.bar(\n",
    "    #         align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    #     )\n",
    "    #     plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LSTM LRP & SHAP, LRP & SHAP & Attention scores separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[\"lrp_scores\"].values))[::-1][:max_tokens].tolist()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[[\"lrp_scores\", \"shap_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    # plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[\"lrp_scores\"].values))[::-1][:max_tokens].tolist()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[[\"lrp_scores\", \"shap_scores\", \"att_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    # plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP/SHAP/Attn scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which right RBO p to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbo_results = {}\n",
    "# for epoch in valid_results.keys():\n",
    "#     rbo_results[epoch] = []\n",
    "\n",
    "#     for p in range(1, 20, 1):\n",
    "#         p = p / 20.0\n",
    "\n",
    "#         rbo_val = []\n",
    "#         for pid in valid_results[epoch].keys():\n",
    "#             imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "\n",
    "#             rbo_val.append(\n",
    "#                 get_rbo(\n",
    "#                     imp_df[\"lrp_scores\"],\n",
    "#                     imp_df[\"shap_scores\"],\n",
    "#                     imp_df[\"u_token\"].tolist(),\n",
    "#                     p=p,\n",
    "#                 )\n",
    "#             )\n",
    "#         rbo_results[epoch].append((p, np.mean(rbo_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(17, 10))\n",
    "# for epoch in rbo_results.keys():\n",
    "#     results = rbo_results[epoch]\n",
    "#     plt.plot(\n",
    "#         [r[0] for r in results],\n",
    "#         [r[1] for r in results],\n",
    "#         label=\"epoch_\" + str(epoch),\n",
    "#         marker=\"o\",\n",
    "#     )\n",
    "\n",
    "# plt.ylabel(\"RBO\")\n",
    "# plt.xlabel(\"p\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.xlim([0, 1.05])\n",
    "# plt.title(\"RBO vs epoch and p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reformat the data\n",
    "\n",
    "# plt.figure(figsize=(17, 10))\n",
    "# new_keys = [x[0] for x in rbo_results[list(rbo_results.keys())[0]]]\n",
    "\n",
    "# melted_results = {}\n",
    "# for key in new_keys:\n",
    "#     melted_results[key] = []\n",
    "\n",
    "# for epoch in rbo_results.keys():\n",
    "#     results = rbo_results[epoch]\n",
    "\n",
    "#     for key, val in results:\n",
    "#         melted_results[key].append((epoch, val))\n",
    "\n",
    "# for p in melted_results.keys():\n",
    "#     results = melted_results[p]\n",
    "#     plt.plot(\n",
    "#         [str(r[0] + 1) for r in results],\n",
    "#         [r[1] for r in results],\n",
    "#         label=\"p=\" + str(p),\n",
    "#         marker=\"o\",\n",
    "#     )\n",
    "\n",
    "# plt.ylabel(\"RBO\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.xlim((0, 17))\n",
    "# plt.legend()\n",
    "# plt.grid(linestyle=\"--\")\n",
    "# plt.title(\"RBO vs epoch and p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.val_lrp_shap_tau.plot(marker=\"o\", figsize=(17, 10))\n",
    "# plt.grid(linestyle=\"--\")\n",
    "# plt.ylabel(\"correlation\")\n",
    "# plt.title(\"Weighted Kendall Tau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_codes = pd.read_csv(GT_CODES_PATH)\n",
    "# gt_codes = list(set(gt_codes.Internal_Code))\n",
    "# len(gt_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "# shap_path = \"./output/AE_CDiff/200/downsampled/lstm-att-lrp/shap/test_shap_2.pkl\"\n",
    "shap_path = SHAP_SAVE_DIR_PATTERN.format(split, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shap_path, \"rb\") as fp:\n",
    "    all_scores = pickle.load(fp)\n",
    "type(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "# gt_idx = [x for x, tok in enumerate(imp_df.u_token) if tok in gt_codes]\n",
    "# n_gt = len(gt_idx)\n",
    "# if n_gt > 0:\n",
    "#     lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt + 1]\n",
    "#     shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "#         : n_gt + 1\n",
    "#     ]\n",
    "#     lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "#     shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "#     epoch_val_lrp_sim.append(lrp_sim)\n",
    "#     epoch_val_shap_sim.append(shap_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freedom = 30\n",
    "all_shap_sims = []\n",
    "all_lrp_sims = []\n",
    "for freedom in range(max_freedom):\n",
    "    all_t_corr = []\n",
    "    shap_sims = []\n",
    "    lrp_sims = []\n",
    "    for pat_id, pat_scores in all_scores.items():\n",
    "        tokens = pat_scores[\"imp\"][\"u_token\"]\n",
    "        shap_scores = pat_scores[\"imp\"][\"shap_scores\"].values\n",
    "        lrp_scores = pat_scores[\"imp\"][\"lrp_scores\"].values\n",
    "        shap_sims.append(get_similarity(shap_scores, tokens, gt_codes, freedom))\n",
    "        lrp_sims.append(get_similarity(lrp_scores, tokens, gt_codes, freedom))\n",
    "        all_t_corr.append(get_wtau(shap_scores, lrp_scores))\n",
    "    all_shap_sims.append(sum(shap_sims) / float(len(shap_sims)))\n",
    "    all_lrp_sims.append(sum(lrp_sims) / float(len(lrp_sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SHAP/LRP T-Corr:\", np.array(all_t_corr).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_t_corr)\n",
    "avg_t_corr = sum(all_t_corr) / float(len(all_t_corr))\n",
    "plt.title(f\"Avg SHAP/LRP T-Corr: {avg_t_corr:.4f}\")\n",
    "plt.xlabel(\"Patients\")\n",
    "plt.ylabel(\"SHAP/LRP T-Corr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_freedom), all_lrp_sims)\n",
    "plt.title(\"LRP/GT Similarity with different Freedom\")\n",
    "plt.xlabel(\"Degree of Freedom\")\n",
    "plt.ylabel(\"Intersection Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(max_freedom), all_shap_sims)\n",
    "plt.title(\"SHAP/GT Similarity with different Freedom\")\n",
    "plt.xlabel(\"Degree of Freedom\")\n",
    "plt.ylabel(\"Intersection Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
