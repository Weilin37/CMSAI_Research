{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP, LRP & attention scores with Attention-LSTM\n",
    "Author: Lin Lee Cheong <br>\n",
    "Modified by: Tesfagabir Meharizghi\n",
    "Date created: 1/13/2021 <br>\n",
    "Date updated: 2/10/2021 <br>\n",
    "\n",
    "**Data:** <br>\n",
    "Using the final version of 30 sequence length dataset (sequence-based) generated by Tes<br>\n",
    "Train, validation (for model training), test (for performance etc), and example (4 output)\n",
    "<br>\n",
    "\n",
    "\n",
    "**Steps:** <br>\n",
    "1. Read in datasets [DONE]\n",
    "2. LSTM model training \n",
    "    - TODO: check probab outputs\n",
    "    - save epoch train, val, loss, etc [DONE]\n",
    "    - calculated SHAP & relevance scores for val and test sets [DONE]\n",
    "    - calculate rbo, tau for val and test sets [DONE]\n",
    "    - plot rbo, tau\n",
    "3. Extract SHAP, attention and relevance scores for a TEST set\n",
    "    - calculate SHAP, relevance scores, performance (AUC, test loss)[DONE]\n",
    "    - calculate rbo, tau [DONE]\n",
    "3. Extract SHAP and relevance scores for example set of 4\n",
    "    - plot epoch evolution [DONE]\n",
    "    - add attention [DONE]\n",
    "4. Save output in dict format[DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# * EDA with GT availability\n",
    "# * Take all data (weights)\n",
    "# * There is seasonality in the data?\n",
    "# *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install shap\n",
    "#!pip install xgboost\n",
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from numpy import newaxis as na\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n",
    "\n",
    "# from lrp_att_model import *\n",
    "import shap_jacc_utils as sj_utils\n",
    "\n",
    "import rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm-att-lrp\"\n",
    "\n",
    "NROWS = 1e9\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SEQ_LEN = 300\n",
    "\n",
    "DATA_TYPE = \"downsampled\"\n",
    "FNAME = \"all\"\n",
    "SAVE_DATASET = True\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# Model Parameters\n",
    "MODEL_PARAMS = {\n",
    "    # Dataset/vocab related\n",
    "    \"min_freq\": 500,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    # Model related parameters\n",
    "    \"embedding_dim\": 30,\n",
    "    \"hidden_dim\": 30,\n",
    "    \"nlayers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"linear_bias\": False,\n",
    "    \"init_type\": \"zero\",  # zero/learned\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"scheduler_step\": 3,\n",
    "    \"clip\": True,\n",
    "    \"rev\": False,\n",
    "    # SHAP-related parameters\n",
    "    \"n_background\": 300,  # Number of background examples\n",
    "    \"background_negative_only\": False,  # If negative examples are used as background\n",
    "    \"background_positive_only\": False,\n",
    "    \"test_positive_only\": False,\n",
    "    \"is_test_random\": False,\n",
    "    \"n_valid_examples\": BATCH_SIZE,  # Number of validation examples to be used during shap computation\n",
    "    \"n_test_examples\": BATCH_SIZE,  # Number of the final test examples to be used in shap computation #TODO\n",
    "}\n",
    "\n",
    "\n",
    "# ALL_DOWN_DATA_PATH = (\n",
    "#     f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/all.csv\"\n",
    "# )\n",
    "\n",
    "# MONTH_DATA_PATH = (\n",
    "#     f\"../../../data/AE_CDiff_d00845/output/data/1000/original/preprocessed/20110101.csv\"\n",
    "# )\n",
    "\n",
    "TRAIN_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/train.csv\"\n",
    "VALID_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/val.csv\"\n",
    "TEST_DATA_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/test.csv\"\n",
    "SELECTED_EXAMPLES_PATH = f\"../../../data/AE_CDiff_d00845/output/data/1000/{DATA_TYPE}/preprocessed/splits/{FNAME}/visualized_test_patients.txt\"\n",
    "\n",
    "OUT_TRAIN_DATA_PATH = (\n",
    "    f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/train.pkl\"\n",
    ")\n",
    "OUT_VALID_DATA_PATH = (\n",
    "    f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/val.pkl\"\n",
    ")\n",
    "OUT_TEST_DATA_PATH = (\n",
    "    f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/test.pkl\"\n",
    ")\n",
    "OUT_SELECTED_EXAMPLES_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/visualized_test_patients.pkl\"\n",
    "\n",
    "VOCAB_PATH = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/splits/{FNAME}/vocab_minFreq{MODEL_PARAMS['min_freq']}.pkl\"\n",
    "\n",
    "GT_CODES_PATH = \"../../../data/AE_CDiff_d00845/cdiff_risk_factors_codes.csv\"\n",
    "\n",
    "MODEL_SAVE_PATH_PATTERN = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/model_weights/model_{'{}'}.pkl\"\n",
    "SHAP_SAVE_DIR_PATTERN = f\"./output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split\n",
    "\n",
    "OUTPUT_RESULTS_PATH = (\n",
    "    f\"output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/train_results/results.csv\"\n",
    ")\n",
    "PARAMS_PATH = f\"output/AE_CDiff/{SEQ_LEN}/{DATA_TYPE}/{MODEL_NAME}/train_results/model_params.json\"\n",
    "\n",
    "N_EPOCHS = 3  # 10\n",
    "\n",
    "TARGET_COLNAME = \"d_00845\"\n",
    "UID_COLNAME = \"patient_id\"\n",
    "TARGET_VALUE = \"1\"\n",
    "\n",
    "# --------------------\n",
    "# From the original AE parameters\n",
    "# batch_size = 1024\n",
    "# N_EPOCHS = 20\n",
    "\n",
    "# EMBEDDING_DIM = 30\n",
    "# HIDDEN_DIM = 30\n",
    "# BIDIRECTIONAL = False\n",
    "# DROPOUT = 0.3\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = '../../../data/AE_CDiff_d00845/output/data/1000/original/preprocessed/splits/20110101/test.csv'\n",
    "# df = pd.read_csv(test_data_path, nrows=100)\n",
    "# df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_codes(codes_path, vocab0, seq_len):\n",
    "    \"\"\"Get the available ground truth codes.\"\"\"\n",
    "    df_gt_codes = pd.read_csv(codes_path)\n",
    "    gt_codes = list(set(df_gt_codes.Internal_Code.tolist()))\n",
    "    print(f\"Total GT codes original: {len(gt_codes)}\")\n",
    "    cols = [str(i) for i in range(seq_len - 1, -1, -1)]\n",
    "    vocab = list(vocab0._vocab.keys())\n",
    "    print(f\"Total vocab: {len(vocab)}\")\n",
    "    # vocab = set(df[cols].values.flatten().tolist())\n",
    "    gt_codes = [code for code in gt_codes if code in vocab]\n",
    "    print(f\"Total GT Available: {len(gt_codes)}\")\n",
    "    return gt_codes, df_gt_codes\n",
    "\n",
    "\n",
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "def get_rbo(x, y, uid, p=0.7):\n",
    "    x_idx = np.argsort(x)[::-1]\n",
    "    y_idx = np.argsort(y)[::-1]\n",
    "\n",
    "    return rbo.RankingSimilarity(\n",
    "        [uid[idx] for idx in x_idx], [uid[idx] for idx in y_idx]\n",
    "    ).rbo(p=p)\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x):\n",
    "    if \"_N\" in x:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class AttNoHtLSTM(SimpleLSTM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        hidden_dim,\n",
    "        vocab,\n",
    "        device,\n",
    "        nlayers=1,\n",
    "        bidi=True,\n",
    "        use_gpu=True,\n",
    "        pad_idx=0,\n",
    "        dropout=None,\n",
    "        init_type=\"zero\",\n",
    "        linear_bias=True,\n",
    "    ):\n",
    "        super(AttNoHtLSTM, self).__init__(\n",
    "            emb_dim=emb_dim, hidden_dim=hidden_dim, vocab=vocab, device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.emb_layer = nn.Embedding(self.input_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidi = bidi\n",
    "        self.nlayers = nlayers\n",
    "        self.linear_bias = linear_bias\n",
    "\n",
    "        \"\"\"\n",
    "        self.attn_layer = (\n",
    "            nn.Linear(hidden_dim *2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        if dropout is None:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=nlayers,\n",
    "                bidirectional=bidi,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "        self.pred_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias)\n",
    "            if bidi\n",
    "            else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "\n",
    "        self.dpt = nn.Dropout(dropout)\n",
    "\n",
    "        \"\"\"\n",
    "        self.context_layer = (\n",
    "            nn.Linear(hidden_dim * 2, 1, bias=linear_bias) \n",
    "            if bidi else nn.Linear(hidden_dim, 1, bias=linear_bias)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, tokens, ret_attn=False):\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            embedded = self.dpt(self.emb_layer(tokens))\n",
    "        else:\n",
    "            embedded = self.emb_layer(tokens)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = True\n",
    "            self.c0.requires_grad = True\n",
    "            hidden = (\n",
    "                self.h0.repeat(1, tokens.shape[0], 1),\n",
    "                self.c0.repeat(1, tokens.shape[0], 1),\n",
    "            )\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(tokens.shape[0])\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        text_lengths = torch.sum(tokens != self.pad_idx, dim=1).to(\"cpu\")\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "\n",
    "        packed_output, (final_hidden, cell) = self.lstm(packed_embedded, hidden)\n",
    "\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=tokens.shape[1]\n",
    "        )\n",
    "\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "\n",
    "        # Switch to multiplicative attention\n",
    "        mask_feats = np.array(tokens.cpu().numpy() == 0)\n",
    "        mask_feats = -1000 * mask_feats.astype(np.int)\n",
    "\n",
    "        mask_feats = torch.Tensor(mask_feats).to(self.device)\n",
    "\n",
    "        attn_weights_int = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            (tokens.shape[1]) ** 0.5\n",
    "        )\n",
    "        attn_weights = nn.functional.softmax(attn_weights_int + mask_feats, -1)\n",
    "\n",
    "        context = torch.bmm(output.transpose(1, 2), attn_weights.unsqueeze(-1)).squeeze(\n",
    "            -1\n",
    "        )\n",
    "\n",
    "        concat_out = context\n",
    "\n",
    "        if self.dpt is not None:\n",
    "            pred = self.pred_layer(self.dpt(concat_out))\n",
    "        else:\n",
    "            pred = self.pred_layer(concat_out)\n",
    "\n",
    "        if ret_attn:\n",
    "            return (\n",
    "                pred.detach().cpu().numpy(),\n",
    "                attn_weights.detach().cpu().numpy(),\n",
    "                context.detach().cpu().numpy(),\n",
    "                attn_weights_int.detach().cpu().numpy(),\n",
    "                out.detach().cpu().numpy(),\n",
    "                output.detach().cpu().numpy(),\n",
    "            )\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def forward_shap(self, token_ids, mask, full_id_matrix=False):\n",
    "        token_ids = token_ids if token_ids.is_cuda else token_ids.to(self.device)\n",
    "\n",
    "        if self.init_type == \"learned\":\n",
    "            self.h0.requires_grad = False\n",
    "            self.c0.requires_grad = False\n",
    "\n",
    "            hidden = (self.h0.repeat(1, 1, 1), self.c0.repeat(1, 1, 1))\n",
    "\n",
    "        else:  # default behavior\n",
    "            hidden = self.init_hidden(1)\n",
    "            hidden = self.repackage_hidden(hidden)\n",
    "\n",
    "        token_ids[sum(mask) :, :] = 0\n",
    "        embedded = torch.matmul(token_ids, self.emb_layer.weight).unsqueeze(0)\n",
    "\n",
    "        embedded = embedded[:, : sum(mask), :]\n",
    "\n",
    "        output, _ = self.lstm(embedded, hidden)\n",
    "\n",
    "        # output = output.permute(1, 0, 2)  # [batch, text_length, hidden_dim]\n",
    "        # print(f'Output dimensions: {output.shape}')\n",
    "        if self.bidi:\n",
    "            out = torch.cat(\n",
    "                [output[:, -1, : self.hidden_dim], output[:, 0, self.hidden_dim :]],\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        # print(f'Stacked hidden dimensions: {stacked_hidden.shape}')\n",
    "        # print(f'mask weight dimensions: {mask_feats.shape}')\n",
    "        # attention = self.context_layer(output).squeeze(-1)\n",
    "        # att_weights = nn.functional.softmax(attention, dim=-1)\n",
    "        # context = torch.bmm(att_weights.unsqueeze(1), output).squeeze(1)\n",
    "        attn_weights = torch.bmm(output, out.unsqueeze(2)).squeeze(2) / (\n",
    "            sum(mask) ** 0.5\n",
    "        )\n",
    "\n",
    "        soft_attn_weights = nn.functional.softmax(attn_weights, 1)\n",
    "\n",
    "        context = torch.bmm(\n",
    "            output.transpose(1, 2), soft_attn_weights.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # concat_out = torch.cat((context, out), dim=1)\n",
    "        concat_out = context\n",
    "        pred = self.pred_layer(concat_out)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class LSTM_LRP_MultiLayer:\n",
    "    def __init__(self, pymodel):\n",
    "        super(LSTM_LRP_MultiLayer, self).__init__()\n",
    "\n",
    "        self.init_model(pymodel)\n",
    "\n",
    "    def init_model(self, pymodel):\n",
    "\n",
    "        self.device = pymodel.device\n",
    "        self.use_gpu = pymodel.use_gpu\n",
    "        self.bidi = pymodel.bidi\n",
    "\n",
    "        self.emb_dim = pymodel.emb_dim\n",
    "        self.vocab = pymodel.vocab\n",
    "        self.input_dim = len(self.vocab)\n",
    "        self.pad_idx = pymodel.pad_idx\n",
    "        self.hidden_dim = pymodel.hidden_dim\n",
    "\n",
    "        self.emb = pymodel.emb_layer.weight.detach().numpy()\n",
    "\n",
    "        param_list = list(pymodel.lstm.named_parameters())\n",
    "        param_dict = {}\n",
    "        for param_tuple in param_list:\n",
    "            param_dict[param_tuple[0]] = param_tuple[-1].detach().numpy()\n",
    "\n",
    "        # rearrange, pytorch uses ifgo format, need to move to icfo/igfo format\n",
    "        idx_list = (\n",
    "            list(range(0, self.hidden_dim))\n",
    "            + list(range(self.hidden_dim * 2, self.hidden_dim * 3))\n",
    "            + list(range(self.hidden_dim, self.hidden_dim * 2))\n",
    "            + list(range(self.hidden_dim * 3, self.hidden_dim * 4))\n",
    "        )\n",
    "        self.nlayers = pymodel.nlayers\n",
    "\n",
    "        # i (input), g (candidate), f (forget), o (output) order\n",
    "        # (4 * hidden_dim, emb_dim)\n",
    "        self.Wxh_Left = {}\n",
    "        self.bxh_Left = {}\n",
    "        self.Whh_Left = {}\n",
    "        self.bhh_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wxh_Right = {}\n",
    "            self.bxh_Right = {}\n",
    "            self.Whh_Right = {}\n",
    "            self.bhh_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.Wxh_Left[layer] = param_dict[f\"weight_ih_l{layer}\"][idx_list]\n",
    "            self.bxh_Left[layer] = param_dict[f\"bias_ih_l{layer}\"][idx_list]  # shape 4d\n",
    "            self.Whh_Left[layer] = param_dict[f\"weight_hh_l{layer}\"][\n",
    "                idx_list\n",
    "            ]  # shape 4d*d\n",
    "            self.bhh_Left[layer] = param_dict[f\"bias_hh_l{layer}\"][idx_list]  # shape 4d\n",
    "\n",
    "            if self.bidi:\n",
    "                # LSTM right encoder\n",
    "                self.Wxh_Right[layer] = param_dict[f\"weight_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bxh_Right[layer] = param_dict[f\"bias_ih_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.Whh_Right[layer] = param_dict[f\"weight_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "                self.bhh_Right[layer] = param_dict[f\"bias_hh_l{layer}_reverse\"][\n",
    "                    idx_list\n",
    "                ]\n",
    "\n",
    "        # START ADDED: CONTEXT LAYER INIT\n",
    "        # linear output layer: shape C * 4d\n",
    "        # 0-d: fwd & context\n",
    "        # d-2d: rev & context\n",
    "        # 2d-3d: fwd & final hidden\n",
    "        # 3d-4d: rev & final hidden\n",
    "        Why = pymodel.pred_layer.weight.detach().numpy()\n",
    "\n",
    "        self.Why_Left = Why[:, 2 * self.hidden_dim : 3 * self.hidden_dim]  # shape C*d\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Why_Right = Why[:, 3 * self.hidden_dim :]  # shape C*d\n",
    "\n",
    "        self.Wcy_Left = Why[:, : self.hidden_dim]\n",
    "\n",
    "        if self.bidi:\n",
    "            self.Wcy_Right = Why[:, self.hidden_dim : 2 * self.hidden_dim]\n",
    "        # END ADDED: CONTEXT LAYER INIT\n",
    "\n",
    "    def set_input(self, tokens):\n",
    "        T = len(tokens)  # sequence length\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)  # hidden layer dimension\n",
    "        e = self.emb.shape[1]  # word embedding dimension\n",
    "\n",
    "        self.w = tokens\n",
    "        self.x = {}\n",
    "        self.x_rev = {}\n",
    "        x = np.zeros((T, e))\n",
    "        x[:, :] = self.emb[tokens, :]\n",
    "        self.x[0] = x\n",
    "        self.x_rev[0] = x[::-1, :].copy()\n",
    "        self.h_Left = {}\n",
    "        self.c_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.h_Right = {}\n",
    "            self.c_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.h_Left[layer] = np.zeros((T + 1, d))\n",
    "            self.c_Left[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "            if self.bidi:\n",
    "                self.h_Right[layer] = np.zeros((T + 1, d))\n",
    "                self.c_Right[layer] = np.zeros((T + 1, d))\n",
    "\n",
    "        self.att_score = None\n",
    "\n",
    "    def forward_gate(self, layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir):\n",
    "\n",
    "        if gate_dir == \"left\":\n",
    "            self.gates_xh_Left[layer][t] = np.dot(\n",
    "                self.Wxh_Left[layer], self.x[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Left[layer][t] = np.dot(\n",
    "                self.Whh_Left[layer], self.h_Left[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Left[layer][t] = (\n",
    "                self.gates_xh_Left[layer][t]\n",
    "                + self.gates_hh_Left[layer][t]\n",
    "                + self.bxh_Left[layer]\n",
    "                + self.bhh_Left[layer]\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Left[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Left[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Left[layer][t] = (\n",
    "                self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1]\n",
    "                + self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Left[layer][t] = self.gates_Left[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Left[layer][t]\n",
    "            )\n",
    "\n",
    "        if gate_dir == \"right\":\n",
    "            self.gates_xh_Right[layer][t] = np.dot(\n",
    "                self.Wxh_Right[layer], self.x_rev[layer][t]\n",
    "            )\n",
    "            self.gates_hh_Right[layer][t] = np.dot(\n",
    "                self.Whh_Right[layer], self.h_Right[layer][t - 1]\n",
    "            )\n",
    "            self.gates_pre_Right[layer][t] = (\n",
    "                self.gates_xh_Right[layer][t]\n",
    "                + self.gates_hh_Right[layer][t]\n",
    "                + self.bxh_Right[layer]\n",
    "                + self.bhh_Right[layer]\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx] = 1.0 / (\n",
    "                1.0 + np.exp(-self.gates_pre_Right[layer][t, idx])\n",
    "            )\n",
    "            self.gates_Right[layer][t, idx_g] = np.tanh(\n",
    "                self.gates_pre_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.c_Right[layer][t] = (\n",
    "                self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1]\n",
    "                + self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g]\n",
    "            )\n",
    "            self.h_Right[layer][t] = self.gates_Right[layer][t, idx_o] * np.tanh(\n",
    "                self.c_Right[layer][t]\n",
    "            )\n",
    "\n",
    "    def forward_lrp(self):\n",
    "        \"\"\"\n",
    "        Standard forward pass.\n",
    "        Compute the hidden layer values (assuming input x/x_rev was previously set)\n",
    "        \"\"\"\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "\n",
    "        # gate indices (assuming the gate ordering in the LSTM weights is i,g,f,o):\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        self.gates_xh_Left = {}\n",
    "        self.gates_hh_Left = {}\n",
    "        self.gates_pre_Left = {}\n",
    "        self.gates_Left = {}\n",
    "\n",
    "        if self.bidi:\n",
    "            self.gates_xh_Right = {}\n",
    "            self.gates_hh_Right = {}\n",
    "            self.gates_pre_Right = {}\n",
    "            self.gates_Right = {}\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            self.gates_xh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_hh_Left[layer] = np.zeros((T, 4 * d))\n",
    "            self.gates_pre_Left[layer] = np.zeros((T, 4 * d))  # gates pre-activation\n",
    "            self.gates_Left[layer] = np.zeros((T, 4 * d))  # gates activation\n",
    "\n",
    "            if self.bidi:\n",
    "                self.gates_xh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_hh_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_pre_Right[layer] = np.zeros((T, 4 * d))\n",
    "                self.gates_Right[layer] = np.zeros((T, 4 * d))\n",
    "\n",
    "        # START ADDED: INITIALIZE CONTEXT LAYERS\n",
    "        self.ctxt_Left = np.zeros((1, d))\n",
    "        self.ctxt_Right = np.zeros((1, d))\n",
    "        self.att_wgt_Left = np.zeros((T, 1))\n",
    "        self.att_wgt_Right = np.zeros((T, 1))\n",
    "        self.att_score = np.zeros((T, 1))\n",
    "\n",
    "        # END ADDED: INITIALIZE CONTEXT LAYERS\n",
    "\n",
    "        # START EDIT: cycle through first layer first\n",
    "        layer = 0\n",
    "        for t in range(T):\n",
    "            self.forward_gate(\n",
    "                layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "            )\n",
    "            if self.bidi:\n",
    "                self.forward_gate(\n",
    "                    layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                )\n",
    "\n",
    "        # go through all the rest of the layers\n",
    "        if self.nlayers > 1:\n",
    "            ## TODO: fix init t-1 (zero time step) Zeroes!!\n",
    "            self.x[layer + 1] = (\n",
    "                np.concatenate(\n",
    "                    (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                )\n",
    "                if self.bidi\n",
    "                else self.h_Left[layer][:T]\n",
    "            )\n",
    "\n",
    "            self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "            for layer in range(1, self.nlayers):\n",
    "                for t in range(T):\n",
    "                    self.forward_gate(\n",
    "                        layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"left\"\n",
    "                    )\n",
    "                    if self.bidi:\n",
    "                        self.forward_gate(\n",
    "                            layer, t, idx, idx_i, idx_g, idx_f, idx_o, gate_dir=\"right\"\n",
    "                        )\n",
    "\n",
    "                    self.x[layer + 1] = np.concatenate(\n",
    "                        (self.h_Left[layer][:T], self.h_Right[layer][:T][::-1]), axis=1\n",
    "                    )\n",
    "                    self.x_rev[layer + 1] = self.x[layer + 1][::-1].copy()\n",
    "\n",
    "        # calculate attention layer & context layer\n",
    "        top_layer = self.nlayers - 1\n",
    "        self.att_wgt_Left = np.dot(\n",
    "            self.h_Left[top_layer][:T, :], self.h_Left[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_wgt_Right = np.dot(\n",
    "            self.h_Right[top_layer][:T, :], self.h_Right[top_layer][T - 1]\n",
    "        )\n",
    "        self.att_score = self.stable_softmax(\n",
    "            (self.att_wgt_Left + self.att_wgt_Right) / (T ** 0.5)\n",
    "        )\n",
    "\n",
    "        self.ctxt_Left = (self.att_score[:, na] * self.h_Left[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "        self.ctxt_Right = (self.att_score[:, na] * self.h_Right[top_layer][:T]).sum(\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # CALCULATE WITH CONTEXT & OUT, NOT JUST HIDDEN\n",
    "        # self.y_Left = np.dot(self.Why_Left, self.h_Left[top_layer][T - 1])\n",
    "        self.y_Left = np.dot(self.Wcy_Left, self.ctxt_Left)\n",
    "\n",
    "        # self.y_Right = np.dot(self.Why_Right, self.h_Right[top_layer][T - 1])\n",
    "        self.y_Right = np.dot(self.Wcy_Right, self.ctxt_Right)\n",
    "\n",
    "        self.s = self.y_Left + self.y_Right\n",
    "\n",
    "        return self.s.copy()  # prediction scores\n",
    "\n",
    "    def stable_softmax(self, x):\n",
    "        z = x - np.max(x)\n",
    "        num = np.exp(z)\n",
    "        denom = np.sum(num)\n",
    "        softmax_vals = num / denom\n",
    "\n",
    "        return softmax_vals\n",
    "\n",
    "    def lrp_left_gate(\n",
    "        self,\n",
    "        Rc_Left,\n",
    "        Rh_Left,\n",
    "        Rg_Left,\n",
    "        Rx,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "\n",
    "        # import IPython\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        Rc_Left[layer][t] += Rh_Left[layer][t]\n",
    "        Rc_Left[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_f] * self.c_Left[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rg_Left[layer][t] += lrp_linear(\n",
    "            self.gates_Left[layer][t, idx_i] * self.gates_Left[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Left[layer][t],\n",
    "            Rc_Left[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx[layer][t] += lrp_linear(\n",
    "            self.x[layer][t],\n",
    "            self.Wxh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Left[layer][t - 1] += lrp_linear(\n",
    "            self.h_Left[layer][t - 1],\n",
    "            self.Whh_Left[layer][idx_g].T,\n",
    "            self.bxh_Left[layer][idx_g] + self.bhh_Left[layer][idx_g],\n",
    "            self.gates_pre_Left[layer][t, idx_g],\n",
    "            Rg_Left[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Left, Rh_Left, Rg_Left, Rx\n",
    "\n",
    "    def lrp_right_gate(\n",
    "        self,\n",
    "        Rc_Right,\n",
    "        Rh_Right,\n",
    "        Rg_Right,\n",
    "        Rx_rev,\n",
    "        layer,\n",
    "        t,\n",
    "        d,\n",
    "        ee,\n",
    "        idx,\n",
    "        idx_f,\n",
    "        idx_i,\n",
    "        idx_g,\n",
    "        idx_o,\n",
    "        eps,\n",
    "        bias_factor,\n",
    "    ):\n",
    "        Rc_Right[layer][t] += Rh_Right[layer][t]\n",
    "        Rc_Right[layer][t - 1] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_f] * self.c_Right[layer][t - 1],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        Rg_Right[layer][t] += lrp_linear(\n",
    "            self.gates_Right[layer][t, idx_i] * self.gates_Right[layer][t, idx_g],\n",
    "            np.identity(d),\n",
    "            np.zeros((d)),\n",
    "            self.c_Right[layer][t],\n",
    "            Rc_Right[layer][t],\n",
    "            2 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rx_rev[layer][t] += lrp_linear(\n",
    "            self.x_rev[layer][t],\n",
    "            self.Wxh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "\n",
    "        Rh_Right[layer][t - 1] += lrp_linear(\n",
    "            self.h_Right[layer][t - 1],\n",
    "            self.Whh_Right[layer][idx_g].T,\n",
    "            self.bxh_Right[layer][idx_g] + self.bhh_Right[layer][idx_g],\n",
    "            self.gates_pre_Right[layer][t, idx_g],\n",
    "            Rg_Right[layer][t],\n",
    "            d + ee,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        return Rc_Right, Rh_Right, Rg_Right, Rx_rev\n",
    "\n",
    "    def lrp(self, w, LRP_class, eps=0.001, bias_factor=0.0):\n",
    "        \"\"\"\n",
    "        Layer-wise Relevance Propagation (LRP) backward pass.\n",
    "        Compute the hidden layer relevances by performing LRP for the target class LRP_class\n",
    "        (according to the papers:\n",
    "            - https://doi.org/10.1371/journal.pone.0130140\n",
    "            - https://doi.org/10.18653/v1/W17-5221 )\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        self.set_input(w)\n",
    "        self.forward_lrp()\n",
    "\n",
    "        T = len(self.w)\n",
    "        d = int(self.Wxh_Left[0].shape[0] / 4)\n",
    "        e = self.emb.shape[1]\n",
    "        C = self.Why_Left.shape[0]  # number of classes\n",
    "        idx = np.hstack((np.arange(0, d), np.arange(2 * d, 4 * d))).astype(\n",
    "            int\n",
    "        )  # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = (\n",
    "            np.arange(0, d),\n",
    "            np.arange(d, 2 * d),\n",
    "            np.arange(2 * d, 3 * d),\n",
    "            np.arange(3 * d, 4 * d),\n",
    "        )  # indices of gates i,g,f,o separately\n",
    "\n",
    "        # initialize\n",
    "        Rx = {}\n",
    "        Rx_rev = {}\n",
    "        Rx_all = {}\n",
    "\n",
    "        Rh_Left = {}\n",
    "        Rc_Left = {}\n",
    "        Rg_Left = {}  # gate g only\n",
    "\n",
    "        if self.bidi:\n",
    "            Rh_Right = {}\n",
    "            Rc_Right = {}\n",
    "            Rg_Right = {}  # gate g only\n",
    "\n",
    "        for layer in range(self.nlayers):\n",
    "            Rx[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_rev[layer] = np.zeros(self.x[layer].shape)\n",
    "            Rx_all[layer] = np.zeros(self.x[layer].shape)\n",
    "\n",
    "            Rh_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rc_Left[layer] = np.zeros((T + 1, d))\n",
    "            Rg_Left[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "            if self.bidi:\n",
    "                Rh_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rc_Right[layer] = np.zeros((T + 1, d))\n",
    "                Rg_Right[layer] = np.zeros((T, d))  # gate g only\n",
    "\n",
    "        Rctxt_Left = np.zeros((1, d))\n",
    "        Rctxt_Right = np.zeros((1, d))\n",
    "\n",
    "        Rout_mask = np.zeros((C))\n",
    "        Rout_mask[LRP_class] = 1.0\n",
    "\n",
    "        # process top most layer first\n",
    "        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
    "        layer = self.nlayers - 1\n",
    "        \"\"\"\n",
    "        Rh_Left[layer][T - 1] = lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            self.Why_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] = lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                self.Why_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "        \"\"\"\n",
    "        # ADD CONTEXT CALCULATIONS TO CONTEXT LAYER\n",
    "        Rctxt_Left = lrp_linear(\n",
    "            self.ctxt_Left,\n",
    "            self.Wcy_Left.T,  # 8d\n",
    "            np.zeros((C)),\n",
    "            self.s,\n",
    "            self.s * Rout_mask,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rctxt_Right = lrp_linear(\n",
    "                self.ctxt_Right,\n",
    "                self.Wcy_Right.T,  # 8d\n",
    "                np.zeros((C)),\n",
    "                self.s,\n",
    "                self.s * Rout_mask,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        # CONTRIBUTION FROM ATTN LAYER\n",
    "        Rh_Left[layer][T - 1] += lrp_linear(\n",
    "            self.h_Left[layer][T - 1],\n",
    "            np.identity((d)),\n",
    "            np.zeros((d)),\n",
    "            self.ctxt_Left,\n",
    "            self.att_score[T - 1] * Rctxt_Left,\n",
    "            4 * d,\n",
    "            eps,\n",
    "            bias_factor,\n",
    "            debug=False,\n",
    "        )\n",
    "        if self.bidi:\n",
    "            Rh_Right[layer][T - 1] += lrp_linear(\n",
    "                self.h_Right[layer][T - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Right,\n",
    "                self.att_score[T - 1] * Rctxt_Right,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        ee = e if self.nlayers == 1 else 2 * d\n",
    "        for t in reversed(range(T)):\n",
    "\n",
    "            Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                Rc_Left,\n",
    "                Rh_Left,\n",
    "                Rg_Left,\n",
    "                Rx,\n",
    "                layer,\n",
    "                t,\n",
    "                d,\n",
    "                ee,\n",
    "                idx,\n",
    "                idx_f,\n",
    "                idx_i,\n",
    "                idx_g,\n",
    "                idx_o,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "            )\n",
    "\n",
    "            # ATTN Relevance scores\n",
    "            Rh_Left[layer][t - 1] += lrp_linear(\n",
    "                self.h_Left[layer][t - 1],\n",
    "                np.identity((d)),\n",
    "                np.zeros((d)),\n",
    "                self.ctxt_Left,\n",
    "                self.att_score[t - 1] * Rctxt_Left,\n",
    "                4 * d,\n",
    "                eps,\n",
    "                bias_factor,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "            if self.bidi:\n",
    "                Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                    Rc_Right,\n",
    "                    Rh_Right,\n",
    "                    Rg_Right,\n",
    "                    Rx_rev,\n",
    "                    layer,\n",
    "                    t,\n",
    "                    d,\n",
    "                    ee,\n",
    "                    idx,\n",
    "                    idx_f,\n",
    "                    idx_i,\n",
    "                    idx_g,\n",
    "                    idx_o,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                )\n",
    "                # ATTN Relevance scores for top-most layer\n",
    "                Rh_Right[layer][t - 1] += lrp_linear(\n",
    "                    self.h_Right[layer][t - 1],\n",
    "                    np.identity((d)),\n",
    "                    np.zeros((d)),\n",
    "                    self.ctxt_Right,\n",
    "                    self.att_score[t - 1] * Rctxt_Right,\n",
    "                    4 * d,\n",
    "                    eps,\n",
    "                    bias_factor,\n",
    "                    debug=False,\n",
    "                )\n",
    "\n",
    "        # propagate through remaining layers\n",
    "        if self.nlayers > 1:\n",
    "            remaining_layers = list(range(0, self.nlayers - 1))[::-1]\n",
    "            # print(f\"remaining layers: {remaining_layers}\")\n",
    "\n",
    "            # no more attn layer flow back\n",
    "            for layer in remaining_layers:\n",
    "\n",
    "                # Sum up all the relevances for each of the inputs in sequence\n",
    "                Rx_all[layer + 1] = Rx[layer + 1] + Rx_rev[layer + 1][::-1, :]\n",
    "\n",
    "                ee = e if layer == 0 else 2 * d\n",
    "                for t in reversed(range(T)):\n",
    "                    # Rh_Left[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Left[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Left[layer][t], #self.x[layer+1][t, :d],\n",
    "                    #    Rx_all[layer+1][t, :d],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Left[layer][t] += Rx_all[layer + 1][t, :d]\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rc_Left, Rh_Left, Rg_Left, Rx = self.lrp_left_gate(\n",
    "                        Rc_Left,\n",
    "                        Rh_Left,\n",
    "                        Rg_Left,\n",
    "                        Rx,\n",
    "                        layer,\n",
    "                        t,\n",
    "                        d,\n",
    "                        ee,\n",
    "                        idx,\n",
    "                        idx_f,\n",
    "                        idx_i,\n",
    "                        idx_g,\n",
    "                        idx_o,\n",
    "                        eps,\n",
    "                        bias_factor,\n",
    "                    )\n",
    "\n",
    "                    ### RIGHT +++++++++\n",
    "                    # Rh_Right[layer][t]   += lrp_linear(\n",
    "                    #    self.h_Right[layer][t], np.identity((d)) ,\n",
    "                    #    np.zeros((d)), self.h_Right[layer][t], #self.x_rev[layer+1][::-1, :][t, d:],\n",
    "                    #    Rx_all[layer+1][t, d:],\n",
    "                    #    d, eps, bias_factor, debug=False)\n",
    "                    # @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                    Rh_Right[layer][t] += Rx_all[layer + 1][::-1, :][t, d:]\n",
    "                    if self.bidi:\n",
    "                        Rc_Right, Rh_Right, Rg_Right, Rx_rev = self.lrp_right_gate(\n",
    "                            Rc_Right,\n",
    "                            Rh_Right,\n",
    "                            Rg_Right,\n",
    "                            Rx_rev,\n",
    "                            layer,\n",
    "                            t,\n",
    "                            d,\n",
    "                            ee,\n",
    "                            idx,\n",
    "                            idx_f,\n",
    "                            idx_i,\n",
    "                            idx_g,\n",
    "                            idx_o,\n",
    "                            eps,\n",
    "                            bias_factor,\n",
    "                        )\n",
    "\n",
    "        # record\n",
    "        self.Rx_all = Rx_all\n",
    "        self.Rx = Rx\n",
    "        self.Rx_rev = Rx_rev\n",
    "        self.Rh_Left = Rh_Left\n",
    "        self.Rh_Right = Rh_Right\n",
    "        self.Rc_Left = Rc_Left\n",
    "        self.Rc_Right = Rc_Right\n",
    "        self.Rg_Right = Rg_Right\n",
    "        self.d = d\n",
    "        self.ee = ee\n",
    "        self.Rctxt_Left = Rctxt_Left\n",
    "        self.Rctxt_Right = Rctxt_Right\n",
    "\n",
    "        return (\n",
    "            Rx[0],\n",
    "            Rx_rev[0][::-1, :],\n",
    "            Rh_Left[0][-1].sum()\n",
    "            + Rc_Left[0][-1].sum()\n",
    "            + Rh_Right[0][-1].sum()\n",
    "            + Rc_Right[0][-1].sum(),\n",
    "        )\n",
    "\n",
    "    def get_attn_values(self):\n",
    "        return self.att_score\n",
    "\n",
    "\n",
    "def get_sim(idx_model, idx_gt):\n",
    "    return len(set(idx_model).intersection(set(idx_gt))) / len(idx_gt)\n",
    "\n",
    "\n",
    "def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):\n",
    "    \"\"\"\n",
    "    LRP for a linear layer with input dim D and output dim M.\n",
    "    Args:\n",
    "    - hin:            forward pass input, of shape (D,)\n",
    "    - w:              connection weights, of shape (D, M)\n",
    "    - b:              biases, of shape (M,)\n",
    "    - hout:           forward pass output, of shape (M,) (unequal to np.dot(w.T,hin)+b if more than one incoming layer!)\n",
    "    - Rout:           relevance at layer output, of shape (M,)\n",
    "    - bias_nb_units:  total number of connected lower-layer units (onto which the bias/stabilizer contribution is redistributed for sanity check)\n",
    "    - eps:            stabilizer (small positive number)\n",
    "    - bias_factor:    set to 1.0 to check global relevance conservation, otherwise use 0.0 to ignore bias/stabilizer redistribution (recommended)\n",
    "    Returns:\n",
    "    - Rin:            relevance at layer input, of shape (D,)\n",
    "    \"\"\"\n",
    "    sign_out = np.where(hout[na, :] >= 0, 1.0, -1.0)  # shape (1, M)\n",
    "\n",
    "    # numer    = (w * hin[:,na]) + ( (bias_factor*b[na,:]*1.) * 1./bias_nb_units )\n",
    "    numer = (w * hin[:, na]) + (\n",
    "        bias_factor * (b[na, :] * 1.0 + eps * sign_out * 1.0) / bias_nb_units\n",
    "    )  # shape (D, M)\n",
    "\n",
    "    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact\n",
    "    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check\n",
    "    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)\n",
    "\n",
    "    denom = hout[na, :] + (eps * sign_out * 1.0)  # shape (1, M)\n",
    "\n",
    "    message = (numer / denom) * Rout[na, :]  # shape (D, M)\n",
    "\n",
    "    Rin = message.sum(axis=1)  # shape (D,)\n",
    "\n",
    "    if debug:\n",
    "        print(\"local diff: \", Rout.sum() - Rin.sum())\n",
    "    # Note:\n",
    "    # - local  layer   relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)\n",
    "    # - global network relevance conservation\n",
    "    #   if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections\n",
    "    # -> can be used for sanity check\n",
    "\n",
    "    return Rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_valid_data(n_val_eval, batch_size, valid_dataloader):\n",
    "    \"\"\"Get subset of validation dataset to run SHAP/LRP on\"\"\"\n",
    "\n",
    "    n_loads = int(np.ceil(n_val_eval / batch_size))\n",
    "    counter = 0\n",
    "\n",
    "    for ids, labels, idxed_text in valid_dataloader:\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 1:\n",
    "            sub_val_ids, sub_val_labels, sub_val_idxed_text = ids, labels, idxed_text\n",
    "        else:\n",
    "            sub_val_ids = sub_val_ids + ids\n",
    "            sub_val_labels = torch.cat([sub_val_labels, labels])\n",
    "            sub_val_idxed_text = torch.cat([sub_val_idxed_text, idxed_text])\n",
    "\n",
    "        if counter == n_loads:\n",
    "            break\n",
    "\n",
    "    sub_val_ids = sub_val_ids[:n_val_eval]\n",
    "    sub_val_labels = sub_val_labels[:n_val_eval]\n",
    "    sub_val_idxed_text = sub_val_idxed_text[:n_val_eval]\n",
    "\n",
    "    return (sub_val_ids, sub_val_labels, sub_val_idxed_text)\n",
    "\n",
    "\n",
    "def glfass_single(cpu_model, background, test, seq_len, device):\n",
    "    \"\"\"\n",
    "    Single-thread function for Get Lstm Features And Shap Scores\n",
    "    Called by get_lstm_features_and_shap_scores_mp\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = cpu_model.to(device)\n",
    "\n",
    "    try:\n",
    "\n",
    "        background_ids, background_labels, background_idxes = background\n",
    "        bg_data, bg_masks = model.get_all_ids_masks(background_idxes, seq_len)\n",
    "\n",
    "        explainer = deep_id_pytorch.CustomPyTorchDeepIDExplainer(\n",
    "            model, bg_data, bg_masks, gpu_memory_efficient=True\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        test_ids, test_labels, test_idxes = test\n",
    "        test_data, test_masks = model.get_all_ids_masks(test_idxes, seq_len)\n",
    "\n",
    "        #         import pdb\n",
    "\n",
    "        #         pdb.set_trace()\n",
    "\n",
    "        lstm_shap_values = explainer.shap_values(\n",
    "            test_data, test_masks, model_device=device\n",
    "        )\n",
    "\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        raise Exception\n",
    "        # import IPython.core.debugger\n",
    "\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "    end_time = time.time()\n",
    "    mins, secs = epoch_time(start_time, end_time)\n",
    "    # print(f\"{device}: test_ids={len(test_ids)}, test_labels={len(test_labels)}, test_idxes={len(test_idxes)}\")\n",
    "    # print(f\"Completed on {device} taking {mins}:{secs}\")\n",
    "    return (test_ids, test_labels, test_idxes, lstm_shap_values)\n",
    "\n",
    "\n",
    "def mycallback(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def myerrorcallback(exception):\n",
    "    print(exception)\n",
    "    return exception\n",
    "\n",
    "\n",
    "def get_lstm_features_and_shap_scores_mp(\n",
    "    model,\n",
    "    tr_dataloader,\n",
    "    test,  # don't use dataloader to fix dataset (test_ids, test_labels, test_idxes)\n",
    "    seq_len,\n",
    "    shap_path,\n",
    "    save_output=True,\n",
    "    n_background=None,\n",
    "    background_negative_only=False,\n",
    "    test_positive_only=False,\n",
    "    is_test_random=False,\n",
    "    output_explainer=False,\n",
    "    multigpu_lst=None,  # cuda:1, cuda:2 ...\n",
    "):\n",
    "    \"\"\"Get all features and shape importance scores for each example in te_dataloader.\"\"\"\n",
    "\n",
    "    # Get background dataset\n",
    "    background = sj_utils.get_lstm_background(\n",
    "        tr_dataloader, n_background=n_background, negative_only=background_negative_only\n",
    "    )\n",
    "\n",
    "    # split up test datasets\n",
    "\n",
    "    n_gpu = len(multigpu_lst)\n",
    "    gpu_model_tuple = []\n",
    "    for gpu in multigpu_lst:\n",
    "        model = copy.deepcopy(model)\n",
    "        model.device = gpu\n",
    "        model = model.to(gpu)\n",
    "        gpu_model_tuple.append((gpu, model))\n",
    "\n",
    "    # test = sj_utils.get_lstm_data(\n",
    "    #    te_dataloader,\n",
    "    #    n_test,\n",
    "    #    positive_only=test_positive_only,\n",
    "    #    is_random=is_test_random,\n",
    "    # )\n",
    "    test_ids, test_labels, test_idxes = test\n",
    "\n",
    "    test_labels_lst, test_idxes_lst, test_ids_lst = [], [], []\n",
    "    n_per_gpu = int(np.ceil(len(test_ids) / n_gpu))\n",
    "    for idx in range(n_gpu):\n",
    "        if idx == (n_gpu - 1):\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu :])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu :])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu :])\n",
    "        else:\n",
    "            test_ids_lst.append(test_ids[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_labels_lst.append(test_labels[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "            test_idxes_lst.append(test_idxes[idx * n_per_gpu : (idx + 1) * n_per_gpu])\n",
    "\n",
    "    # multiprocess one core one gpu\n",
    "    # print(f'Starting multiprocess for {n_gpu} cores')\n",
    "    try:\n",
    "        from multiprocessing.dummy import Pool as dThreadPool\n",
    "\n",
    "        pool = dThreadPool(n_gpu)\n",
    "        # pool = torch.multiprocessing.Pool(n_gpu)  # one feeding each gpu\n",
    "        func_call_lst = []\n",
    "        for cur_test_id, cur_test_label, cur_test_idxes, (gpu, model) in zip(\n",
    "            test_ids_lst, test_labels_lst, test_idxes_lst, gpu_model_tuple\n",
    "        ):\n",
    "            # print(f\"\\nlength of tests={len(cur_test_id)}\")\n",
    "            # print(f\"gpu: {n_gpu}\")\n",
    "            # print(f\"model: {model.device}\")\n",
    "\n",
    "            func_call = pool.apply_async(\n",
    "                glfass_single,\n",
    "                (\n",
    "                    model.cpu(),\n",
    "                    background,\n",
    "                    (cur_test_id, cur_test_label, cur_test_idxes),\n",
    "                    seq_len,\n",
    "                    gpu,\n",
    "                ),\n",
    "                callback=mycallback,\n",
    "                error_callback=myerrorcallback,\n",
    "            )\n",
    "            func_call_lst.append(func_call)\n",
    "\n",
    "        # print('Starting to wait')\n",
    "        for func_call in func_call_lst:\n",
    "            func_call.wait()\n",
    "\n",
    "        # print('Collecting results')\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "\n",
    "        test_ids, test_labels, test_idxes, lstm_shap_values = None, None, None, None\n",
    "        for func_call in func_call_lst:\n",
    "            init_results = func_call.get()\n",
    "\n",
    "            # first one\n",
    "            if test_ids is None:\n",
    "                test_ids, test_labels, test_idxes, lstm_shap_values = init_results\n",
    "                test_ids = list(test_ids)\n",
    "            else:\n",
    "                test_ids = test_ids + list(init_results[0])\n",
    "                test_labels = torch.cat([test_labels, init_results[1]], dim=0)\n",
    "                test_idxes = torch.cat([test_idxes, init_results[2]], dim=0)\n",
    "                lstm_shap_values = np.concatenate(\n",
    "                    [lstm_shap_values, init_results[3]], axis=0\n",
    "                )\n",
    "\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # raise Exception\n",
    "    #         import IPython.core.debugger\n",
    "\n",
    "    #         dbg = IPython.core.debugger.Pdb()\n",
    "    #         dbg.set_trace()\n",
    "\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pool.terminate()\n",
    "        # print('Multiprocessing pool closed')\n",
    "\n",
    "    # print('collating per patient results')\n",
    "    try:\n",
    "        # import IPython.core.debugger\n",
    "        # dbg = IPython.core.debugger.Pdb()\n",
    "        # dbg.set_trace()\n",
    "        test = (test_ids, test_labels, test_idxes)\n",
    "        features = []\n",
    "        scores = []\n",
    "        patients = []\n",
    "        total = len(test[0])\n",
    "        import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for idx in range(total):\n",
    "            df_shap, patient_id = sj_utils.get_per_patient_shap(\n",
    "                lstm_shap_values, test, model.vocab, idx\n",
    "            )\n",
    "            events = df_shap[\"events\"].values.tolist()\n",
    "            vals = df_shap[\"shap_vals\"].values.tolist()\n",
    "\n",
    "            pad = \"<pad>\"\n",
    "            if pad in events:\n",
    "                pad_indx = events.index(pad)\n",
    "                events = events[:pad_indx]\n",
    "                vals = vals[:pad_indx]\n",
    "\n",
    "            features.append(events)\n",
    "            scores.append(vals[:])\n",
    "            patients.append(patient_id)\n",
    "\n",
    "        shap_values = (features, scores, patients)\n",
    "    except Exception as excpt:\n",
    "        print(excpt)\n",
    "        # import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        raise Exception\n",
    "\n",
    "    if save_output:\n",
    "        if not os.path.isdir(os.path.split(shap_path)[0]):\n",
    "            os.makedirs(os.path.split(shap_path)[0])\n",
    "        save_pickle(shap_values, shap_path)\n",
    "\n",
    "    if output_explainer:\n",
    "        return shap_values, explainer.expected_value\n",
    "\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created: ./output/AE_CDiff/300/downsampled/lstm-att-lrp/model_weights\n",
      "Directory Created: ./output/AE_CDiff/300/downsampled/lstm-att-lrp/shap\n",
      "Directory Created: output/AE_CDiff/300/downsampled/lstm-att-lrp/train_results\n"
     ]
    }
   ],
   "source": [
    "# Create output directories if needed\n",
    "model_dir = os.path.dirname(MODEL_SAVE_PATH_PATTERN)\n",
    "shap_dir = os.path.dirname(SHAP_SAVE_DIR_PATTERN)\n",
    "output_dir = os.path.dirname(OUTPUT_RESULTS_PATH)\n",
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    if os.path.exists(shap_dir):\n",
    "        shutil.rmtree(shap_dir)\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(model_dir)\n",
    "    os.makedirs(shap_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory Created: {model_dir}\")\n",
    "    print(f\"Directory Created: {shap_dir}\")\n",
    "    print(f\"Directory Created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGPU_LST = []\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    for gpu in range(n_gpus):\n",
    "        MULTIGPU_LST.append(f\"cuda:{gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Test Examples: ['171GG8EXI_20111001', 'JBBQ17F2I_20110401', 'C3HRDWH1A_20110501', 'VRBFYDQ3B_20111101']\n"
     ]
    }
   ],
   "source": [
    "# Load Selected Patients for later SHAP visualization\n",
    "patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "patients = patients.values.flatten().tolist()\n",
    "print(f\"Selected Test Examples: {patients}\")\n",
    "# create the example set\n",
    "selected_patients_path = os.path.join(output_dir, \"selected_test_patients.csv\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df[test_df.patient_id.isin(patients)].to_csv(selected_patients_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3298, 1004)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>999</th>\n",
       "      <th>998</th>\n",
       "      <th>997</th>\n",
       "      <th>996</th>\n",
       "      <th>995</th>\n",
       "      <th>994</th>\n",
       "      <th>993</th>\n",
       "      <th>992</th>\n",
       "      <th>991</th>\n",
       "      <th>990</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>d_00845</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>num_gt_codes</th>\n",
       "      <th>has_gt_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>d_73313</td>\n",
       "      <td>h_01936</td>\n",
       "      <td>h_22523</td>\n",
       "      <td>h_72020</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>p_8166</td>\n",
       "      <td>1</td>\n",
       "      <td>T3KEOAORA_20110401</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_5856</td>\n",
       "      <td>d_78650</td>\n",
       "      <td>h_90935</td>\n",
       "      <td>h_90960</td>\n",
       "      <td>h_99232</td>\n",
       "      <td>1</td>\n",
       "      <td>171GG8EXI_20111001</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6_days</td>\n",
       "      <td>d_s1540</td>\n",
       "      <td>h_99306</td>\n",
       "      <td>2_days</td>\n",
       "      <td>d_s1540</td>\n",
       "      <td>d_s2639</td>\n",
       "      <td>h_99309</td>\n",
       "      <td>5_days</td>\n",
       "      <td>d_s5849</td>\n",
       "      <td>h_99315</td>\n",
       "      <td>...</td>\n",
       "      <td>h_96366</td>\n",
       "      <td>2_days</td>\n",
       "      <td>h_83735</td>\n",
       "      <td>h_96360</td>\n",
       "      <td>h_96361</td>\n",
       "      <td>h_96368</td>\n",
       "      <td>0</td>\n",
       "      <td>C3HRDWH1A_20110501</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_RUB10</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_5856</td>\n",
       "      <td>d_78791</td>\n",
       "      <td>h_90966</td>\n",
       "      <td>h_99305</td>\n",
       "      <td>1</td>\n",
       "      <td>Q2AR87BH9_20110301</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>h_93306</td>\n",
       "      <td>h_99221</td>\n",
       "      <td>h_99233</td>\n",
       "      <td>1_days</td>\n",
       "      <td>d_4264</td>\n",
       "      <td>h_93010</td>\n",
       "      <td>1</td>\n",
       "      <td>2Y1IK8N1O_20110201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      999      998      997     996      995      994      993     992  \\\n",
       "0   <pad>    <pad>    <pad>   <pad>    <pad>    <pad>    <pad>   <pad>   \n",
       "1   <pad>    <pad>    <pad>   <pad>    <pad>    <pad>    <pad>   <pad>   \n",
       "2  6_days  d_s1540  h_99306  2_days  d_s1540  d_s2639  h_99309  5_days   \n",
       "3   <pad>    <pad>    <pad>   <pad>    <pad>    <pad>    <pad>   <pad>   \n",
       "4   <pad>    <pad>    <pad>   <pad>    <pad>    <pad>    <pad>   <pad>   \n",
       "\n",
       "       991      990  ...        5        4        3        2        1  \\\n",
       "0    <pad>    <pad>  ...  d_73313  h_01936  h_22523  h_72020  h_99232   \n",
       "1    <pad>    <pad>  ...   1_days   d_5856  d_78650  h_90935  h_90960   \n",
       "2  d_s5849  h_99315  ...  h_96366   2_days  h_83735  h_96360  h_96361   \n",
       "3    <pad>    <pad>  ...  h_RUB10   1_days   d_5856  d_78791  h_90966   \n",
       "4    <pad>    <pad>  ...  h_93306  h_99221  h_99233   1_days   d_4264   \n",
       "\n",
       "         0 d_00845          patient_id num_gt_codes has_gt_codes  \n",
       "0   p_8166       1  T3KEOAORA_20110401            2            1  \n",
       "1  h_99232       1  171GG8EXI_20111001            5            1  \n",
       "2  h_96368       0  C3HRDWH1A_20110501            2            1  \n",
       "3  h_99305       1  Q2AR87BH9_20110301            4            1  \n",
       "4  h_93010       1  2Y1IK8N1O_20110201            2            1  \n",
       "\n",
       "[5 rows x 1004 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../../data/AE_CDiff_d00845/output/data/1000/downsampled/preprocessed/splits/all/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/AE_CDiff_d00845/output/data/1000/downsampled/preprocessed/splits/all/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../../data/AE_CDiff_d00845/output/data/1000/downsampled/preprocessed/splits/all/test.csv..\n",
      "Success!\n",
      "Building dataset from output/AE_CDiff/300/downsampled/lstm-att-lrp/train_results/selected_test_patients.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "if SAVE_DATASET:\n",
    "    train_dataset, vocab = build_lstm_dataset(\n",
    "        TRAIN_DATA_PATH,\n",
    "        min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "        uid_colname=UID_COLNAME,\n",
    "        target_colname=TARGET_COLNAME,\n",
    "        max_len=SEQ_LEN,\n",
    "        target_value=TARGET_VALUE,\n",
    "        vocab=None,\n",
    "        nrows=NROWS,\n",
    "        rev=MODEL_PARAMS[\"rev\"],\n",
    "    )\n",
    "\n",
    "    valid_dataset, _ = build_lstm_dataset(\n",
    "        VALID_DATA_PATH,\n",
    "        min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "        uid_colname=UID_COLNAME,\n",
    "        target_colname=TARGET_COLNAME,\n",
    "        max_len=SEQ_LEN,\n",
    "        target_value=TARGET_VALUE,\n",
    "        vocab=vocab,\n",
    "        nrows=NROWS,\n",
    "        rev=MODEL_PARAMS[\"rev\"],\n",
    "    )\n",
    "\n",
    "    test_dataset, _ = build_lstm_dataset(\n",
    "        TEST_DATA_PATH,\n",
    "        min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "        uid_colname=UID_COLNAME,\n",
    "        target_colname=TARGET_COLNAME,\n",
    "        max_len=SEQ_LEN,\n",
    "        target_value=TARGET_VALUE,\n",
    "        vocab=vocab,\n",
    "        nrows=NROWS,\n",
    "        rev=MODEL_PARAMS[\"rev\"],\n",
    "    )\n",
    "\n",
    "    example_dataset, _ = build_lstm_dataset(\n",
    "        selected_patients_path,\n",
    "        min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "        uid_colname=UID_COLNAME,\n",
    "        target_colname=TARGET_COLNAME,\n",
    "        max_len=SEQ_LEN,\n",
    "        target_value=TARGET_VALUE,\n",
    "        vocab=vocab,\n",
    "        nrows=NROWS,\n",
    "        rev=MODEL_PARAMS[\"rev\"],\n",
    "    )\n",
    "    data_dir = os.path.dirname(OUT_TRAIN_DATA_PATH)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    with open(OUT_TRAIN_DATA_PATH, \"wb\") as fp:\n",
    "        pickle.dump(train_dataset, fp)\n",
    "\n",
    "    with open(OUT_VALID_DATA_PATH, \"wb\") as fp:\n",
    "        pickle.dump(valid_dataset, fp)\n",
    "\n",
    "    with open(OUT_TEST_DATA_PATH, \"wb\") as fp:\n",
    "        pickle.dump(test_dataset, fp)\n",
    "\n",
    "    with open(OUT_SELECTED_EXAMPLES_PATH, \"wb\") as fp:\n",
    "        pickle.dump(example_dataset, fp)\n",
    "\n",
    "    with open(VOCAB_PATH, \"wb\") as fp:\n",
    "        pickle.dump(vocab, fp)\n",
    "else:\n",
    "    with open(VOCAB_PATH, \"rb\") as fp:\n",
    "        vocab = pickle.load(fp)\n",
    "    print(f\"vocab len: {len(vocab)}\")  # vocab + padding + unknown\n",
    "\n",
    "    with open(OUT_TRAIN_DATA_PATH, \"rb\") as fp:\n",
    "        train_dataset = pickle.load(fp)\n",
    "\n",
    "    with open(OUT_VALID_DATA_PATH, \"rb\") as fp:\n",
    "        valid_dataset = pickle.load(fp)\n",
    "\n",
    "    with open(OUT_TEST_DATA_PATH, \"rb\") as fp:\n",
    "        test_dataset = pickle.load(fp)\n",
    "\n",
    "    with open(OUT_SELECTED_EXAMPLES_PATH, \"rb\") as fp:\n",
    "        example_dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab: 685\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Vocab: {len(vocab._vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GT codes original: 134\n",
      "Total vocab: 685\n",
      "Total GT Available: 12\n"
     ]
    }
   ],
   "source": [
    "gt_codes, _ = get_ground_truth_codes(GT_CODES_PATH, vocab, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and load LRP LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # LOAD Model Parameters\n",
    "    with open(PARAMS_PATH, \"r\") as fp:\n",
    "        MODEL_PARAMS = json.load(fp)\n",
    "\n",
    "lstm_model = AttNoHtLSTM(\n",
    "    MODEL_PARAMS[\"embedding_dim\"],\n",
    "    MODEL_PARAMS[\"hidden_dim\"],\n",
    "    vocab,\n",
    "    model_device,\n",
    "    bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "    nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "    dropout=MODEL_PARAMS[\"dropout\"],\n",
    "    init_type=MODEL_PARAMS[\"init_type\"],\n",
    "    linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttNoHtLSTM(\n",
       "  (emb_layer): Embedding(685, 30, padding_idx=0)\n",
       "  (lstm): LSTM(30, 30, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=60, out_features=1, bias=False)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lstm_model.parameters(), lr=MODEL_PARAMS[\"learning_rate\"], weight_decay=0.03\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, MODEL_PARAMS[\"scheduler_step\"], gamma=0.9\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(lstm_model.parameters(), lr=0.0001, weight_decay=0.02)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 11, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm_model.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = {}\n",
    "test_results = {}\n",
    "\n",
    "rbo_p = 0.8\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_auc_lst = []\n",
    "    train_loss_lst = []\n",
    "\n",
    "    val_auc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_lrp_sim_lst = []\n",
    "    val_shap_sim_lst = []\n",
    "    val_lrp_shap_rbo_lst = []\n",
    "    val_lrp_shap_tau_lst = []\n",
    "\n",
    "    test_auc_lst = []\n",
    "    test_loss_lst = []\n",
    "    test_lrp_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "    test_lrp_shap_rbo_lst = []\n",
    "    test_lrp_shap_tau_lst = []\n",
    "\n",
    "    val_patient_ids, val_labels, val_idxed_text = next(iter(valid_dataloader))\n",
    "    test_patient_ids, test_labels, test_idxed_text = next(iter(test_dataloader))\n",
    "\n",
    "    # patient_ids, labels, idxed_text = get_sub_valid_data(N_VALID_EXAMPLES, MODEL_PARAMS['batch_size'], valid_dataloader)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        lstm_model.train()\n",
    "        # model training & perf evaluation\n",
    "        train_loss, train_auc = epoch_train_lstm(\n",
    "            lstm_model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            clip=MODEL_PARAMS[\"clip\"],\n",
    "            device=model_device,\n",
    "        )\n",
    "        train_auc_lst.append(train_auc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "\n",
    "        valid_loss, valid_auc = epoch_val_lstm(\n",
    "            lstm_model, valid_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        val_auc_lst.append(valid_auc)\n",
    "        val_loss_lst.append(valid_loss)\n",
    "\n",
    "        test_loss, test_auc = epoch_val_lstm(\n",
    "            lstm_model, test_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        test_auc_lst.append(test_auc)\n",
    "        test_loss_lst.append(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # save model\n",
    "        save_path = MODEL_SAVE_PATH_PATTERN.format(str(epoch).zfill(2))\n",
    "        torch.save(lstm_model.state_dict(), save_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        #         print(\n",
    "        #             f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "        #             + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "        #             + f\"\\t Test. Loss: {test_loss:.4f} | Test. AUC: {test_auc:.4f} \"\n",
    "        #         )\n",
    "        #         continue\n",
    "\n",
    "        # calculate relevancy and SHAP\n",
    "        lstm_model.eval()\n",
    "        lrp_model = LSTM_LRP_MultiLayer(lstm_model.cpu())\n",
    "\n",
    "        # Save valid/test results\n",
    "        valid_results[epoch] = {}\n",
    "        test_results[epoch] = {}\n",
    "\n",
    "        for sel_idx in range(len(val_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in val_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if val_patient_ids[sel_idx] not in valid_results[epoch]:\n",
    "                valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"label\"] = val_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        for sel_idx in range(len(test_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in test_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if test_patient_ids[sel_idx] not in test_results[epoch]:\n",
    "                test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"label\"] = test_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        shap_start_time = time.time()\n",
    "#         (\n",
    "#             val_features,\n",
    "#             val_scores,\n",
    "#             val_patients,\n",
    "#         ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "#             lstm_model.cuda(),\n",
    "#             train_dataloader,\n",
    "#             valid_dataloader,\n",
    "#             SEQ_LEN,\n",
    "#             \"\",\n",
    "#             save_output=False,\n",
    "#             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "#             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "#             n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "#             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "#             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "#         )\n",
    "\n",
    "#         (\n",
    "#             test_features,\n",
    "#             test_scores,\n",
    "#             test_patients,\n",
    "#         ) = sj_utils.get_lstm_features_and_shap_scores(\n",
    "#             lstm_model.cuda(),\n",
    "#             train_dataloader,\n",
    "#             test_dataloader,\n",
    "#             SEQ_LEN,\n",
    "#             \"\",\n",
    "#             save_output=False,\n",
    "#             n_background=MODEL_PARAMS[\"n_background\"],\n",
    "#             background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "#             n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "#             test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "#             is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "#         )\n",
    "\n",
    "        (\n",
    "            val_features,\n",
    "            val_scores,\n",
    "            val_patients,\n",
    "        ) = get_lstm_features_and_shap_scores_mp(\n",
    "            lstm_model.cpu(),\n",
    "            train_dataloader,\n",
    "            (val_patient_ids, val_labels, val_idxed_text),\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            multigpu_lst=MULTIGPU_LST, #[\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        )\n",
    "\n",
    "        (\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_patients,\n",
    "        ) = get_lstm_features_and_shap_scores_mp(\n",
    "            lstm_model.cpu(),\n",
    "            train_dataloader,\n",
    "            (test_patient_ids, test_labels, test_idxed_text),\n",
    "            SEQ_LEN,\n",
    "            \"\",\n",
    "            save_output=False,\n",
    "            n_background=MODEL_PARAMS[\"n_background\"],\n",
    "            background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "            test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "            is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            multigpu_lst=MULTIGPU_LST, #[\"cuda:2\", \"cuda:3\", \"cuda:1\"],\n",
    "        )\n",
    "\n",
    "        shap_end_time = time.time()\n",
    "        shap_mins, shap_secs = epoch_time(shap_start_time, shap_end_time)\n",
    "\n",
    "        for idx, pid in enumerate(val_patients):\n",
    "            df = valid_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(val_scores[idx])\n",
    "            df[\"shap_scores\"] = val_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        for idx, pid in enumerate(test_patients):\n",
    "            df = test_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(test_scores[idx])\n",
    "            df[\"shap_scores\"] = test_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            test_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        # calculate similarity indexes for val\n",
    "        epoch_val_lrp_shap_t_corr = []\n",
    "        epoch_val_lrp_shap_rbo = []\n",
    "        epoch_val_lrp_sim = []\n",
    "        epoch_val_shap_sim = []\n",
    "\n",
    "        for pid in valid_results[epoch].keys():\n",
    "            imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            valid_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_val_lrp_shap_t_corr.append(\n",
    "                valid_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_val_lrp_shap_rbo.append(valid_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            # gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            gt_idx = [x for x, tok in enumerate(imp_df.u_token) if tok in gt_codes]\n",
    "            n_gt = len(gt_idx)\n",
    "            if n_gt > 0:\n",
    "                lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt + 1]\n",
    "                shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "                    : n_gt + 1\n",
    "                ]\n",
    "                lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "                shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "                epoch_val_lrp_sim.append(lrp_sim)\n",
    "                epoch_val_shap_sim.append(shap_sim)\n",
    "            else:\n",
    "                lrp_sim = -1\n",
    "                shap_sim = -1\n",
    "            valid_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            valid_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(valid_results[epoch], fp)\n",
    "\n",
    "        val_lrp_shap_rbo_lst.append(np.mean(epoch_val_lrp_shap_rbo))\n",
    "        val_lrp_shap_tau_lst.append(np.mean(epoch_val_lrp_shap_t_corr))\n",
    "        val_lrp_sim_lst.append(np.mean(epoch_val_lrp_sim))\n",
    "        val_shap_sim_lst.append(np.mean(epoch_val_shap_sim))\n",
    "\n",
    "        # calculate similarity indexes for test\n",
    "        epoch_test_lrp_shap_t_corr = []\n",
    "        epoch_test_lrp_shap_rbo = []\n",
    "        epoch_test_lrp_sim = []\n",
    "        epoch_test_shap_sim = []\n",
    "\n",
    "        for pid in test_results[epoch].keys():\n",
    "            imp_df = test_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            test_results[epoch][pid][\"lrp_shap_t_corr\"] = get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            test_results[epoch][pid][\"lrp_shap_rbo\"] = get_rbo(\n",
    "                imp_df[\"lrp_scores\"],\n",
    "                imp_df[\"shap_scores\"],\n",
    "                imp_df[\"u_token\"].tolist(),\n",
    "                p=rbo_p,\n",
    "            )\n",
    "\n",
    "            epoch_test_lrp_shap_t_corr.append(\n",
    "                test_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "            epoch_test_lrp_shap_rbo.append(test_results[epoch][pid][\"lrp_shap_rbo\"])\n",
    "\n",
    "            # gt similarity\n",
    "            gt_idx = [x for x, tok in enumerate(imp_df.u_token) if is_value(tok)]\n",
    "            n_gt = len(gt_idx)\n",
    "            if n_gt > 0:\n",
    "                lrp_idx = np.argsort(np.abs(imp_df.lrp_scores.values))[::-1][: n_gt + 1]\n",
    "                shap_idx = np.argsort(np.abs(imp_df.shap_scores.values))[::-1][\n",
    "                    : n_gt + 1\n",
    "                ]\n",
    "                lrp_sim = len(set(lrp_idx).intersection(gt_idx)) / n_gt\n",
    "                shap_sim = len(set(shap_idx).intersection(gt_idx)) / n_gt\n",
    "                epoch_test_lrp_sim.append(lrp_sim)\n",
    "                epoch_test_shap_sim.append(shap_sim)\n",
    "            else:\n",
    "                lrp_sim = -1\n",
    "                shap_sim = -1\n",
    "            test_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            test_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(test_results[epoch], fp)\n",
    "\n",
    "        test_lrp_shap_rbo_lst.append(np.mean(epoch_test_lrp_shap_rbo))\n",
    "        test_lrp_shap_tau_lst.append(np.mean(epoch_test_lrp_shap_t_corr))\n",
    "        test_lrp_sim_lst.append(np.mean(epoch_test_lrp_sim))\n",
    "        test_shap_sim_lst.append(np.mean(epoch_test_shap_sim))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | \"\n",
    "            + f\"SHAP Time: {shap_mins}m {shap_secs}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "            + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "            + f\"| Val LRP Sim: {np.mean(epoch_val_lrp_sim):.4f} | Val SHAP Sim: {np.mean(epoch_val_shap_sim):.4f}\"\n",
    "        )\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results[\"epoch\"] = [x for x in range(N_EPOCHS)]\n",
    "    df_results[\"train_AUC\"] = train_auc_lst\n",
    "    df_results[\"train_Loss\"] = train_loss_lst\n",
    "    df_results[\"val_AUC\"] = val_auc_lst\n",
    "    df_results[\"val_Loss\"] = val_loss_lst\n",
    "    df_results[\"test_AUC\"] = test_auc_lst\n",
    "    df_results[\"test_Loss\"] = test_loss_lst\n",
    "    df_results[\"val_lrp_shap_rbo\"] = val_lrp_shap_rbo_lst\n",
    "    df_results[\"val_lrp_shap_tau\"] = val_lrp_shap_tau_lst\n",
    "    df_results[\"test_lrp_shap_rbo\"] = test_lrp_shap_rbo_lst\n",
    "    df_results[\"test_lrp_shap_tau\"] = test_lrp_shap_tau_lst\n",
    "    df_results[\"val_GT_lrp_sim\"] = val_lrp_sim_lst\n",
    "    df_results[\"val_GT_shap_sim\"] = val_shap_sim_lst\n",
    "    df_results[\"test_GT_lrp_sim\"] = test_lrp_sim_lst\n",
    "    df_results[\"test_GT_shap_sim\"] = test_shap_sim_lst\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    df_results.to_csv(OUTPUT_RESULTS_PATH)\n",
    "\n",
    "    # Save Model Parameters\n",
    "    with open(PARAMS_PATH, \"w\") as fp:\n",
    "        json.dump(MODEL_PARAMS, fp)\n",
    "\n",
    "else:\n",
    "    print(\"Loading Training results....\")\n",
    "    df_results = pd.read_csv(OUTPUT_RESULTS_PATH)\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Load valid results.\n",
    "        valid_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"rb\") as fp:\n",
    "            valid_results[epoch] = pickle.load(fp)\n",
    "        # Load test results.\n",
    "        test_shap_path = SHAP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"rb\") as fp:\n",
    "            test_results[epoch] = pickle.load(fp)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.shape)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"SHAP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_lrp_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs GT Similarity on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_rbo\", \"val_lrp_shap_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"LRP vs SHAP with RBO/Kendall-T on {MODEL_PARAMS['n_valid_examples']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_rbo\", \"val_lrp_shap_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"LRP/SHAP with RBO/Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 2\n",
    "# selected_epochs = [0, 1, 2]\n",
    "selected_epochs = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patients = pd.read_csv(SELECTED_EXAMPLES_PATH, sep=\" \", header=None)\n",
    "selected_patients = selected_patients.values.flatten().tolist()\n",
    "selected_patients\n",
    "# selected_patients = [\"1OD472J277\", \"XEK4OM00KJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = {}\n",
    "for pat_id in selected_patients:\n",
    "    example_results[pat_id] = {}\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if pat_id in test_results[epoch].keys():\n",
    "            example_results[pat_id][epoch] = test_results[epoch][pat_id]\n",
    "example_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"LRP for Epoch: \" + str(epoch))\n",
    "    sj_utils.plot_global_feature_importance(global_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_shap_scores\n",
    "n_jobs = len(valid_results.keys())\n",
    "step = int(n_jobs / 2) - 1\n",
    "for epoch, val_scores in valid_results.items():\n",
    "    if epoch != best_epoch:\n",
    "        continue\n",
    "    all_features = []\n",
    "    all_scores = []\n",
    "    for pat_id, scores in val_scores.items():\n",
    "        all_scores.append(valid_results[epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "        all_features.append(valid_results[epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "        # print(valid_results[epoch][pat_id][\"imp\"][\"token\"])\n",
    "    global_scores = sj_utils.get_global_feature_importance(\n",
    "        all_features, all_scores, absolute=True\n",
    "    )\n",
    "    print(\"SHAP for Epoch: \" + str(epoch))\n",
    "    sj_utils.plot_global_feature_importance(global_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 30\n",
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[selected_epochs[-1]].values))[::-1][\n",
    "            :max_tokens\n",
    "        ].tolist()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"SHAP SCORES for {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.axisbelow\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[selected_epochs[-1]].values))[::-1][\n",
    "            :max_tokens\n",
    "        ].tolist()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    #     # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    #     df[selected_epochs].plot.bar(\n",
    "    #         align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    #     )\n",
    "    #     plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT LSTM LRP & SHAP, LRP & SHAP & Attention scores separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[\"lrp_scores\"].values))[::-1][:max_tokens].tolist()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[[\"lrp_scores\", \"shap_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    # plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "\n",
    "    sorted = False\n",
    "    if df.shape[0] >= max_tokens:\n",
    "        idx = np.argsort(np.abs(df[\"lrp_scores\"].values))[::-1][:max_tokens].tolist()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        df = df.iloc[idx]\n",
    "        # df = df.sort_values(selected_epochs[-1], ascending=False)\n",
    "        df[\"token\"] = df[\"seq_idx\"].astype(\"str\") + \"_\" + df[\"token\"]\n",
    "        df = df.iloc[:max_tokens]\n",
    "        sorted = True\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[[\"lrp_scores\", \"shap_scores\", \"att_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    if sorted:\n",
    "        plt.xticks(range(len(df[\"seq_idx\"])), df.token.values.tolist(), rotation=90)\n",
    "    else:\n",
    "        plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    # plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP/SHAP/Attn scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which right RBO p to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbo_results = {}\n",
    "# for epoch in valid_results.keys():\n",
    "#     rbo_results[epoch] = []\n",
    "\n",
    "#     for p in range(1, 20, 1):\n",
    "#         p = p / 20.0\n",
    "\n",
    "#         rbo_val = []\n",
    "#         for pid in valid_results[epoch].keys():\n",
    "#             imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "\n",
    "#             rbo_val.append(\n",
    "#                 get_rbo(\n",
    "#                     imp_df[\"lrp_scores\"],\n",
    "#                     imp_df[\"shap_scores\"],\n",
    "#                     imp_df[\"u_token\"].tolist(),\n",
    "#                     p=p,\n",
    "#                 )\n",
    "#             )\n",
    "#         rbo_results[epoch].append((p, np.mean(rbo_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(17, 10))\n",
    "# for epoch in rbo_results.keys():\n",
    "#     results = rbo_results[epoch]\n",
    "#     plt.plot(\n",
    "#         [r[0] for r in results],\n",
    "#         [r[1] for r in results],\n",
    "#         label=\"epoch_\" + str(epoch),\n",
    "#         marker=\"o\",\n",
    "#     )\n",
    "\n",
    "# plt.ylabel(\"RBO\")\n",
    "# plt.xlabel(\"p\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.xlim([0, 1.05])\n",
    "# plt.title(\"RBO vs epoch and p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reformat the data\n",
    "\n",
    "# plt.figure(figsize=(17, 10))\n",
    "# new_keys = [x[0] for x in rbo_results[list(rbo_results.keys())[0]]]\n",
    "\n",
    "# melted_results = {}\n",
    "# for key in new_keys:\n",
    "#     melted_results[key] = []\n",
    "\n",
    "# for epoch in rbo_results.keys():\n",
    "#     results = rbo_results[epoch]\n",
    "\n",
    "#     for key, val in results:\n",
    "#         melted_results[key].append((epoch, val))\n",
    "\n",
    "# for p in melted_results.keys():\n",
    "#     results = melted_results[p]\n",
    "#     plt.plot(\n",
    "#         [str(r[0] + 1) for r in results],\n",
    "#         [r[1] for r in results],\n",
    "#         label=\"p=\" + str(p),\n",
    "#         marker=\"o\",\n",
    "#     )\n",
    "\n",
    "# plt.ylabel(\"RBO\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.xlim((0, 17))\n",
    "# plt.legend()\n",
    "# plt.grid(linestyle=\"--\")\n",
    "# plt.title(\"RBO vs epoch and p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.val_lrp_shap_tau.plot(marker=\"o\", figsize=(17, 10))\n",
    "# plt.grid(linestyle=\"--\")\n",
    "# plt.ylabel(\"correlation\")\n",
    "# plt.title(\"Weighted Kendall Tau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
