{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model Training and SHAP computation using Sequence-based Synthetic Dataset (Seq-len=300)\n",
    "\n",
    "**Author: Tesfagabir Meharizghi<br>Last Updated: 02/03/2021**\n",
    "\n",
    "In this jupyter notebook, we trained XGB models by manually tuning their parameters to get the best performance and explainability scores from the different datasets.\n",
    "\n",
    "Tasks done here are:\n",
    "- Training XGB models with the specified parameters and datasets\n",
    "- Computing different model performance measures such as AUCs, intersection similarity, RBOs, etc.\n",
    "- Visualizing global and local SHAP scores\n",
    "- And finally copying the well trained models to S3 if needed\n",
    "\n",
    "Outputs:\n",
    "- The following artifacts are saved:\n",
    "    * Model artifacts\n",
    "    * SHAP values and their corresponding scores for the specified number of val/test examples\n",
    "\n",
    "Model Architecture Used:\n",
    "- XGB\n",
    "\n",
    "Dataset:\n",
    "- Synthetic-events (Toy Dataset) - [seq-len=30]\n",
    "\n",
    "Requirements:\n",
    "- Make sure that you have already generated sequence-based dataset (train/val/test splits) using [Create_toy_dataset_sequence.ipynb](../../../data/toy_dataset/Create_toy_dataset_sequence.ipynb).\n",
    "\n",
    "Discussions/observations:\n",
    "- Since the models' parameters are manually specified with the best ones, we could get almost the best performance in terms of its AUCs and explainability scores.\n",
    "- In addition, we could also get almost perfect global SHAP importance scores where all the important events are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.inspection import permutation_importance\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "#!pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo\n",
    "import rbo\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import xgboost_utils as xgb_utils\n",
    "import shap_jacc_utils as sj_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature suffices that help predict the positive or negative class\n",
    "HELPING_FEATURES_SUFFICES = [\"_A\", \"_H\", \"_U\"]  # Noises Removed\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "\n",
    "# Whether to preprocess data\n",
    "PREPROCESS_DATA = False\n",
    "\n",
    "# Whether to save SHAP scores\n",
    "SAVE_SHAP_OUTPUT = True\n",
    "\n",
    "# Whether to ouput SHAP explainer expected value\n",
    "OUTPUT_SHAP_EXPLAINER = True\n",
    "\n",
    "# Whether to shuffle val & test dataset for shap visualization purposes\n",
    "SHUFFLE = False\n",
    "\n",
    "seq_len = 300\n",
    "\n",
    "USE_FREQ = True  # Whether to use feature frequencies or one-hot for data preprocessing\n",
    "\n",
    "nrows = 1e9\n",
    "\n",
    "target_colname = \"label\"\n",
    "uid_colname = \"patient_id\"\n",
    "target_value = \"1\"\n",
    "\n",
    "rev = False\n",
    "\n",
    "model_name = \"xgb\"\n",
    "dataset = \"Synthetic-seq-based\"\n",
    "\n",
    "# For model early stopping criteria\n",
    "EARLY_STOPPING = \"intersection_similarity\"  # Values are any of these: ['intersection_similarity', 'loss']\n",
    "\n",
    "# SHAP related constants\n",
    "N_BACKGROUND = None  # Number of background examples\n",
    "BACKGROUND_NEGATIVE_ONLY = False  # If negative examples are used as background\n",
    "BACKGROUND_POSITIVE_ONLY = False  # If positive examples are used as background\n",
    "TEST_POSITIVE_ONLY = False  # If only positive examples are selected\n",
    "IS_TEST_RANDOM = (\n",
    "    False  # If random test/val examples are selected for shap value computation\n",
    ")\n",
    "SORT_SHAP_VALUES = False  # Whether to sort per-patient shap values for visualization\n",
    "\n",
    "SHAP_SCORE_ABSOLUTE = True  # Whether to consider the absolute value of a shap score #TODO: Check this before running.\n",
    "\n",
    "train_data_path = f\"../../../data/toy_dataset/data/seq/{seq_len}/seq_aaa_3/train.csv\"\n",
    "valid_data_path = f\"../../../data/toy_dataset/data/seq/{seq_len}/seq_aaa_3/val.csv\"\n",
    "test_data_path = f\"../../../data/toy_dataset/data/seq/{seq_len}/seq_aaa_3/test.csv\"\n",
    "\n",
    "model_save_path = (\n",
    "    f\"./output/Final_Manually_Tuned/{seq_len}/{model_name}/models/model_{'{}'}.pkl\"\n",
    ")\n",
    "shap_save_path_pattern = f\"./output/Final_Manually_Tuned/{seq_len}/{model_name}/shap/{'{}'}_shap_{'{}'}.pkl\"  # SHAP values path for a given dataset split (train/val/test) (data format (features, scores, patient_ids))\n",
    "\n",
    "# Dataset preprocessing\n",
    "x_train_one_hot_path = f\"./output/{seq_len}/{model_name}/data/train_one_hot.csv\"\n",
    "x_valid_one_hot_path = f\"./output/{seq_len}/{model_name}/data/val_one_hot.csv\"\n",
    "x_test_one_hot_path = f\"./output/{seq_len}/{model_name}/data/test_one_hot.csv\"\n",
    "\n",
    "x_train_data_path = f\"./output/{seq_len}/{model_name}/data/train.csv\"\n",
    "x_valid_data_path = f\"./output/{seq_len}/{model_name}/data/val.csv\"\n",
    "x_test_data_path = f\"./output/{seq_len}/{model_name}/data/test.csv\"\n",
    "\n",
    "s3_output_data_dir = (\n",
    "    f\"s3://merck-paper-bucket/{dataset}/Final_Manually_Tuned/{seq_len}/data\"\n",
    ")\n",
    "\n",
    "# Model training\n",
    "BUCKET = \"merck-paper-bucket\"\n",
    "DATA_PREFIX = f\"{dataset}/Final_Manually_Tuned/{seq_len}/data\"\n",
    "MODEL_PREFIX = f\"{dataset}/Final_Manually_Tuned/{seq_len}/{model_name}\".format(seq_len)\n",
    "label = \"label\"\n",
    "\n",
    "output_results_path = (\n",
    "    f\"output/Final_Manually_Tuned/{seq_len}/{model_name}/train_results/results.csv\"\n",
    ")\n",
    "\n",
    "local_model_dir = f\"output/Final_Manually_Tuned/{seq_len}/{model_name}/models/\"\n",
    "s3_output_path = f\"s3://{BUCKET}/{MODEL_PREFIX}/output\"\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = \"xgboost\"\n",
    "REPO_VERSION = \"1.2-1\"\n",
    "\n",
    "EARLY_STOPPING_ROUNDS = 5\n",
    "# EARLY_STOPPING_ROUNDS = 2\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"n_jobs\": 25,\n",
    "    \"random_state\": 10,\n",
    "    \"max_depth\": 2,\n",
    "    \"n_estimators\": 500,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"reg_alpha\": 0.2,\n",
    "    \"reg_lambda\": 0.3,\n",
    "    \"colsample_bytree\": 0.35,\n",
    "    \"use_label_encoder\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Model Output Directory\n",
    "    model_save_dir = os.path.dirname(model_save_path)\n",
    "    shap_save_dir = os.path.dirname(shap_save_path_pattern)\n",
    "\n",
    "    if os.path.exists(model_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(model_save_dir)\n",
    "    if os.path.exists(shap_save_dir):\n",
    "        # Remove model save directory if exists\n",
    "        shutil.rmtree(shap_save_dir)\n",
    "    os.makedirs(model_save_dir)\n",
    "    os.makedirs(shap_save_dir)\n",
    "    print(f\"New directory created: {model_save_dir}\")\n",
    "    print(f\"New directory created: {shap_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22440, 304)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>299</th>\n",
       "      <th>298</th>\n",
       "      <th>297</th>\n",
       "      <th>296</th>\n",
       "      <th>295</th>\n",
       "      <th>294</th>\n",
       "      <th>293</th>\n",
       "      <th>292</th>\n",
       "      <th>291</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seq_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>349</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>0</td>\n",
       "      <td>JVTRR68RQR</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2276</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>helper4_H</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>1</td>\n",
       "      <td>ZHQ7JY7FIJ</td>\n",
       "      <td>ahh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2518</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>1</td>\n",
       "      <td>3I9E8PY74H</td>\n",
       "      <td>ahh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>0</td>\n",
       "      <td>XNPFZTXZPA</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1748</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>0</td>\n",
       "      <td>110U8UQZH9</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    299    298    297    296    295    294    293    292    291  ...  \\\n",
       "0    349  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1   2276  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2   2518  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3     13  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4   1748  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "               6                 5                  4                 3  \\\n",
       "0  dental_exam_N     quad_injury_N        foot_pain_N    ingrown_nail_N   \n",
       "1     backache_N  peanut_allergy_N          helper4_H     dental_exam_N   \n",
       "2     backache_N      cut_finger_N  annual_physical_N  peanut_allergy_N   \n",
       "3  quad_injury_N        headache_N       cut_finger_N    ankle_sprain_N   \n",
       "4       myopia_N        eye_exam_N         headache_N       cold_sore_N   \n",
       "\n",
       "               2              1                  0 label  patient_id seq_event  \n",
       "0     headache_N     eye_exam_N  annual_physical_N     0  JVTRR68RQR        aa  \n",
       "1  quad_injury_N    hay_fever_N           myopia_N     1  ZHQ7JY7FIJ       ahh  \n",
       "2     headache_N  dental_exam_N        cold_sore_N     1  3I9E8PY74H       ahh  \n",
       "3     ACL_tear_N       myopia_N  annual_physical_N     0  XNPFZTXZPA         a  \n",
       "4    hay_fever_N     ACL_tear_N         backache_N     0  110U8UQZH9     noise  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = xgb_utils.get_valid_tokens(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    xgb_utils.prepare_data(\n",
    "        train_data_path,\n",
    "        x_train_one_hot_path,\n",
    "        x_train_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        valid_data_path,\n",
    "        x_valid_one_hot_path,\n",
    "        x_valid_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "    xgb_utils.prepare_data(\n",
    "        test_data_path,\n",
    "        x_test_one_hot_path,\n",
    "        x_test_data_path,\n",
    "        seq_len,\n",
    "        target_colname,\n",
    "        tokens,\n",
    "        s3_output_data_dir,\n",
    "        use_freq=USE_FREQ,\n",
    "    )\n",
    "# else:\n",
    "#     local_dir = os.path.dirname(x_train_data_path)\n",
    "#     xgb_utils.copy_data_to_s3(local_dir, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xgb_classifier(model_path, model_params):\n",
    "    \"\"\"Load XGBClassifier model from saved Booster model.\"\"\"\n",
    "    xgb_model = xgb.XGBClassifier(**model_params)\n",
    "    # xgb_model.set_params()\n",
    "    xgb_model._Booster = sj_utils.load_pickle(model_path)\n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "def get_model_paths(model_save_dir, sort=True):\n",
    "    \"\"\"Get list models paths in sorted order if needed.\"\"\"\n",
    "    fnames = os.listdir(model_save_dir)\n",
    "    if sort:\n",
    "        fnames.sort(key=lambda f: int(re.sub(\"\\D\", \"\", f)))\n",
    "    model_paths = [os.path.join(model_save_dir, fname) for fname in fnames]\n",
    "    return model_paths\n",
    "\n",
    "\n",
    "def get_wtau(x, y):\n",
    "    return stats.weightedtau(x, y, rank=None)[0]\n",
    "\n",
    "\n",
    "# calculate ground truth scores\n",
    "def is_value(x):\n",
    "    if x.endswith(\"_N\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_model_intersection_similarity_v2(features_scores, absolute=True):\n",
    "    # gt similarity\n",
    "    all_features, all_scores = features_scores[0], features_scores[1]\n",
    "    sims = []\n",
    "    for i, features in enumerate(all_features):\n",
    "        scores = all_scores[i]\n",
    "        gt_features = [feature for feature in features if is_value(feature)]\n",
    "        n_gt = len(gt_features)\n",
    "        if n_gt > 0:\n",
    "            dict_features_scores = sj_utils.create_dict_features_scores(\n",
    "                features, scores, absolute\n",
    "            )\n",
    "            top_features_scores = sj_utils.top_k(dict_features_scores, len(gt_features))\n",
    "            top_features = top_features_scores[0]\n",
    "            pred_features = [feature for feature in top_features if is_value(feature)]\n",
    "            sim = len(set(pred_features).intersection(gt_features)) / float(n_gt)\n",
    "        else:\n",
    "            sim = -1\n",
    "        sims.append(sim)\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    return avg_sim, sims\n",
    "\n",
    "\n",
    "def compute_shap(xgb_model, df_train0, df_test0, explainer=None):\n",
    "    # Load the copied model\n",
    "    # xgb_model = sj_utils.load_pickle(model_path)\n",
    "\n",
    "    df_train = df_train0.copy()\n",
    "    df_test = df_test0.copy()\n",
    "\n",
    "    feature_names = [\n",
    "        col for col in df_train.columns.tolist() if col not in [\"patient_id\", \"label\"]\n",
    "    ]\n",
    "    X_train = df_train[feature_names]\n",
    "    X_test = df_test[feature_names]\n",
    "\n",
    "    if explainer is None:\n",
    "        explainer = shap.TreeExplainer(xgb_model, X_train)\n",
    "    shap_scores = explainer.shap_values(X_test).tolist()\n",
    "    features = [feature_names[:]] * X_test.shape[0]\n",
    "    patients = df_test.patient_id.tolist()\n",
    "\n",
    "    return ((features, shap_scores, patients), explainer)\n",
    "\n",
    "\n",
    "def save_results(patients, features, shap_scores, y_true, y_pred, output_path):\n",
    "    \"\"\"Save all model training results to file.\"\"\"\n",
    "    results = {}\n",
    "    for i, patient_id in enumerate(patients):\n",
    "        results[patient_id] = {}\n",
    "        results[patient_id][\"features_xgb\"] = features[i]\n",
    "        results[patient_id][\"label\"] = y_true[i]\n",
    "        results[patient_id][\"xgb_pred\"] = y_pred[i]\n",
    "        results[patient_id][\"xgb_shap\"] = shap_scores[i]\n",
    "\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    sj_utils.save_pickle(results, output_path, verbose=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_features_and_global_shap(results, exp_num=1):\n",
    "    \"\"\"Get a dataframe of features and global shap values.\"\"\"\n",
    "    features = None\n",
    "    ##Get features and global shap\n",
    "    all_shap = []\n",
    "    for patient_id, result in results.items():\n",
    "        if features is None:\n",
    "            features = results[patient_id][\"features_xgb\"]\n",
    "        all_shap.append(results[patient_id][\"xgb_shap\"])\n",
    "    all_shap = np.absolute(np.array(all_shap)).mean(axis=0)\n",
    "    df_shap = pd.DataFrame()\n",
    "    df_shap[\"features\"] = features\n",
    "    df_shap[\"scores\"] = all_shap\n",
    "    df_shap[\"exp_num\"] = exp_num\n",
    "    return df_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./output/300/xgb/data/train_one_hot.csv does not exist: './output/300/xgb/data/train_one_hot.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9f79cb2f5577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_one_hot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid_one_hot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_one_hot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./output/300/xgb/data/train_one_hot.csv does not exist: './output/300/xgb/data/train_one_hot.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(x_train_one_hot_path)\n",
    "df_val = pd.read_csv(x_valid_one_hot_path)\n",
    "df_test = pd.read_csv(x_test_one_hot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0 = df_train.label.value_counts()[0]\n",
    "count_1 = df_train.label.value_counts()[1]\n",
    "print(\n",
    "    f\"Train Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "count_0 = df_val.label.value_counts()[0]\n",
    "count_1 = df_val.label.value_counts()[1]\n",
    "print(\n",
    "    f\"Val Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "count_0 = df_test.label.value_counts()[0]\n",
    "count_1 = df_test.label.value_counts()[1]\n",
    "print(\n",
    "    f\"Test Classes Count: 0-->{count_0}, 1-->{count_1}, Total-->{count_0+count_1}\",\n",
    ")\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = [\n",
    "    col for col in df_train.columns.tolist() if col not in [\"patient_id\", \"label\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.dirname(model_save_path)\n",
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_save_dir):\n",
    "        shutil.rmtree(model_save_dir)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    rounds = 1  # Saves model every one-epoch\n",
    "\n",
    "    check_point = xgb.callback.TrainingCheckPoint(\n",
    "        directory=model_save_dir,\n",
    "        iterations=rounds,\n",
    "        name=\"model\",\n",
    "        as_pickle=True,\n",
    "    )\n",
    "\n",
    "    X_train, y_train = df_train[feature_names], df_train[target_colname]\n",
    "    X_val, y_val = df_val[feature_names], df_val[target_colname]\n",
    "\n",
    "    print(\"Training XGB Classifier...\")\n",
    "    model = XGBClassifier(**MODEL_PARAMS)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose=True,\n",
    "        callbacks=[check_point],\n",
    "    )\n",
    "    print(\"Successfully Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of models paths\n",
    "model_paths = get_model_paths(model_save_dir, sort=True)\n",
    "# model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    p_rbo = 0.9\n",
    "\n",
    "    train_aucs = []\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    gain_permute_rbo_scores = []\n",
    "    gain_permute_tau_scores = []\n",
    "\n",
    "    val_shap_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "\n",
    "    val_gain_shap_rbo_lst = []\n",
    "    val_gain_shap_tau_lst = []\n",
    "\n",
    "    epochs = []\n",
    "    for model_path in model_paths:\n",
    "        # Get epoch number from the model path\n",
    "        model_fname = model_path\n",
    "        model_fname = os.path.basename(model_path)\n",
    "        epoch = int(re.sub(\"\\D\", \"\", model_fname))\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        xgb_model = load_xgb_classifier(model_path, MODEL_PARAMS)\n",
    "\n",
    "        # feature_importances = xgb_model.feature_importances_\n",
    "        feature_names = xgb_model.get_booster().feature_names\n",
    "\n",
    "        # Compute AUCs\n",
    "        train_y_true = df_train.label\n",
    "        train_y_pred = xgb_model.predict_proba(df_train[feature_names])[:, 1]\n",
    "        train_auc = roc_auc_score(train_y_true, train_y_pred)\n",
    "        train_aucs.append(train_auc)\n",
    "\n",
    "        val_y_true = df_val.label\n",
    "        val_y_pred = xgb_model.predict_proba(df_val[feature_names])[:, 1]\n",
    "        val_auc = roc_auc_score(val_y_true, val_y_pred)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        test_y_true = df_test.label\n",
    "        test_y_pred = xgb_model.predict_proba(df_test[feature_names])[:, 1]\n",
    "        test_auc = roc_auc_score(test_y_true, test_y_pred)\n",
    "        test_aucs.append(test_auc)\n",
    "\n",
    "        # Losses\n",
    "        train_loss = log_loss(train_y_true, train_y_pred)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss = log_loss(val_y_true, val_y_pred)\n",
    "        val_losses.append(val_loss)\n",
    "        test_loss = log_loss(test_y_true, test_y_pred)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        xgb_gain = pd.DataFrame(\n",
    "            (xgb_model.get_booster().get_score(importance_type=\"gain\")).items()\n",
    "        )\n",
    "        xgb_gain.columns = [\"features\", \"xgb_gain\"]\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"sk_gain\"]\n",
    "\n",
    "        df_gains = xgb_gain.merge(feat_imp, how=\"inner\", on=[\"features\"])\n",
    "\n",
    "        df_gains[\"xgb_rank\"] = df_gains[\"xgb_gain\"].rank(ascending=False)\n",
    "        df_gains[\"sk_rank\"] = df_gains[\"sk_gain\"].rank(ascending=False)\n",
    "        df_gains.sort_values(\"xgb_gain\", ascending=False)\n",
    "        # print(df_gains)\n",
    "\n",
    "        feat_imp = pd.DataFrame(\n",
    "            list(zip(feature_names, xgb_model.feature_importances_))\n",
    "        )\n",
    "        feat_imp.columns = [\"features\", \"scores\"]\n",
    "        feat_imp.set_index(\"features\", inplace=True)\n",
    "        feat_imp.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        # feat_imp.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Model Feature Importance Scores\")\n",
    "\n",
    "        # Check permutation\n",
    "        perm_imp = permutation_importance(\n",
    "            xgb_model, df_val[feature_names], df_val.label\n",
    "        )\n",
    "\n",
    "        perm_df = pd.DataFrame()\n",
    "        perm_df[\"features\"] = feature_names\n",
    "        perm_df[\"scores\"] = perm_imp.importances_mean\n",
    "        perm_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "        perm_df.set_index(\"features\", inplace=True)\n",
    "        # perm_df.plot.bar(figsize=(10, 5))\n",
    "        # plt.title(\"Permutation Importance Scores\")\n",
    "\n",
    "        vals = feat_imp.merge(perm_df, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"permute_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        permute_rank = vals.sort_values(\"permute_rank\").index\n",
    "\n",
    "        # Plot for different p values\n",
    "        #     rbo_scores = []\n",
    "        #     p_vals = []\n",
    "        #     p_range = range(1, 10)\n",
    "        #     for p in p_range:\n",
    "        #         rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "        #             p=1.0 / p\n",
    "        #         )\n",
    "        #         rbo_scores.append(rbo_score)\n",
    "        #         p_vals.append(1.0 / p)\n",
    "\n",
    "        #     # plt.figure(figsize=(10, 5))\n",
    "        #     plt.plot(p_vals, rbo_scores, marker=\"x\")\n",
    "        #     plt.title(\"RBO Rankings for different p values.\")\n",
    "        #     plt.xlabel(\"p\")\n",
    "        #     plt.ylabel(\"RBO\")\n",
    "\n",
    "        rbo_score = rbo.RankingSimilarity(gain_rank.values, permute_rank.values).rbo(\n",
    "            p=p_rbo\n",
    "        )\n",
    "        gain_permute_rbo_scores.append(rbo_score)\n",
    "        tau_score = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        gain_permute_tau_scores.append(tau_score)\n",
    "\n",
    "        val_shap_results, explainer = compute_shap(\n",
    "            xgb_model, df_train, df_val, explainer=None\n",
    "        )\n",
    "        (val_features, val_scores, val_patients) = val_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (val_features, val_scores), absolute=True\n",
    "        )\n",
    "        val_shap_sim_lst.append(avg_sim)\n",
    "\n",
    "        # Compute the shap global feature importance\n",
    "        shap_global = np.absolute(np.array(val_scores)).mean(axis=0)\n",
    "        df_shap = pd.DataFrame()\n",
    "        df_shap[\"features\"] = feature_names\n",
    "        df_shap[\"scores\"] = shap_global.tolist()\n",
    "        df_shap.set_index(\"features\", inplace=True)\n",
    "        # df_shap[\"shap_rank\"] = df_shap.scores.rank(ascending=False)\n",
    "        # df_shap.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "        vals = feat_imp.merge(df_shap, left_index=True, right_index=True)\n",
    "        vals[\"gain_rank\"] = vals.scores_x.rank(ascending=False)\n",
    "        vals[\"shap_rank\"] = vals.scores_y.rank(ascending=False)\n",
    "\n",
    "        gain_rank = vals.sort_values(\"gain_rank\").index\n",
    "        shap_rank = vals.sort_values(\"shap_rank\").index\n",
    "\n",
    "        rbo_score_shap = rbo.RankingSimilarity(gain_rank.values, shap_rank.values).rbo(\n",
    "            p=p_rbo\n",
    "        )\n",
    "        val_gain_shap_rbo_lst.append(rbo_score_shap)\n",
    "\n",
    "        tau_score_shap = get_wtau(vals.scores_x, vals.scores_y)\n",
    "        val_gain_shap_tau_lst.append(tau_score_shap)\n",
    "\n",
    "        test_shap_results, _ = compute_shap(xgb_model, df_train, df_test, explainer)\n",
    "        (test_features, test_scores, test_patients) = test_shap_results\n",
    "        avg_sim, _ = get_model_intersection_similarity_v2(\n",
    "            (test_features, test_scores), absolute=True\n",
    "        )\n",
    "        test_shap_sim_lst.append(avg_sim)\n",
    "\n",
    "        # Save training results to file.\n",
    "        shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "        results = save_results(\n",
    "            val_patients, val_features, val_scores, val_y_true, val_y_pred, shap_path\n",
    "        )\n",
    "\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        results = save_results(\n",
    "            test_patients,\n",
    "            test_features,\n",
    "            test_scores,\n",
    "            test_y_true,\n",
    "            test_y_pred,\n",
    "            shap_path,\n",
    "        )\n",
    "        del xgb_model\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02} | Train Loss={train_loss:.4} | Train AUC={train_auc:.4} | Val Loss={val_loss:.4} | Val AUC={val_auc:.4} | RBO(p={p_rbo})={rbo_score:.4} | TAU={tau_score:.4}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Aggregate results\n",
    "    columns = [\n",
    "        \"train_AUC\",\n",
    "        \"val_AUC\",\n",
    "        \"test_AUC\",\n",
    "        \"train_Loss\",\n",
    "        \"val_Loss\",\n",
    "        \"test_Loss\",\n",
    "        \"gain_permute_rbo\",\n",
    "        \"gain_permute_tau\",\n",
    "        \"val_GT_shap_sim\",\n",
    "        \"test_GT_shap_sim\",\n",
    "        \"val_gain_shap_rbo\",\n",
    "        \"val_gain_shap_tau\",\n",
    "    ]\n",
    "\n",
    "    df_results = pd.DataFrame(\n",
    "        np.array(\n",
    "            [\n",
    "                train_aucs,\n",
    "                val_aucs,\n",
    "                test_aucs,\n",
    "                train_losses,\n",
    "                val_losses,\n",
    "                test_losses,\n",
    "                gain_permute_rbo_scores,\n",
    "                gain_permute_tau_scores,\n",
    "                val_shap_sim_lst,\n",
    "                test_shap_sim_lst,\n",
    "                val_gain_shap_rbo_lst,\n",
    "                val_gain_shap_tau_lst,\n",
    "            ]\n",
    "        ).T,\n",
    "        columns=columns,\n",
    "    )\n",
    "    df_results[\"epoch\"] = epochs\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    output_dir = os.path.dirname(output_results_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_results.to_csv(output_results_path)\n",
    "else:\n",
    "    # save results summary\n",
    "    df_results = pd.read_csv(output_results_path)\n",
    "    df_results.set_index(\"epoch\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\", \"test_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for XGB\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"SHAP vs GT Similarity on All Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_gain_shap_rbo\", \"val_gain_shap_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs SHAP with RBO/Kendall-T on Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_rbo\", \"gain_permute_tau\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Information Gain vs Permutation Importance with RBO/Kendall-T\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"gain_permute_rbo\", \"gain_permute_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Gain/Permutation with RBO/Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(model_paths) + 1)\n",
    "figsize = (15, 10)\n",
    "for epoch in epochs:\n",
    "    shap_path = shap_save_path_pattern.format(\"val\", epoch)\n",
    "    print(shap_path)\n",
    "    val_results = sj_utils.load_pickle(shap_path)\n",
    "    df_shap = get_features_and_global_shap(val_results)\n",
    "    df_shap = df_shap.sort_values(\"scores\", ascending=True)\n",
    "    df_shap.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=\"scores\",\n",
    "        kind=\"barh\",\n",
    "    )\n",
    "    plt.title(f\"Global Feature Importance (epoch={epoch})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.dirname(train_data_path)\n",
    "patients_path = os.path.join(data_dir, \"visualized_test_patients.txt\")\n",
    "\n",
    "with open(patients_path, \"r\") as fp:\n",
    "    selected_patients = fp.readlines()\n",
    "    selected_patients = [pat.strip() for pat in selected_patients]\n",
    "\n",
    "selected_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = len(model_paths)\n",
    "step = int(n_jobs / 2)\n",
    "# epochs = range(1, len(model_paths) + 1, 2)\n",
    "epochs = range(1, len(model_paths) + 1, step)\n",
    "for patient_id in selected_patients:\n",
    "    df = pd.DataFrame()\n",
    "    label = None\n",
    "    pred_prob = None\n",
    "    for epoch in epochs:\n",
    "        shap_path = shap_save_path_pattern.format(\"test\", epoch)\n",
    "        test_results = sj_utils.load_pickle(shap_path)\n",
    "        if not len(df):\n",
    "            features = test_results[patient_id][\"features_xgb\"]\n",
    "            df[\"features\"] = features\n",
    "            df[\"values\"] = df_test[features][\n",
    "                df_test[\"patient_id\"] == patient_id\n",
    "            ].values[0]\n",
    "            df[\"features\"] = df[\"values\"].astype(str) + \"_\" + df[\"features\"]\n",
    "            label = test_results[patient_id][\"label\"]\n",
    "        pred_prob = test_results[patient_id][\"xgb_pred\"]\n",
    "        df[f\"epoch_{epoch}\"] = test_results[patient_id][\"xgb_shap\"]\n",
    "    figsize = (15, 10)\n",
    "    epoch_cols = [f\"epoch_{epoch}\" for epoch in epochs]\n",
    "    df.plot(\n",
    "        figsize=figsize,\n",
    "        x=\"features\",\n",
    "        y=epoch_cols,\n",
    "        kind=\"bar\",\n",
    "        width=0.8,\n",
    "    )\n",
    "    plt.title(f\"SHAP Scores for {patient_id}: label={label}, pred={pred_prob:.4}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_exps = 10\n",
    "# df = pd.DataFrame()\n",
    "# for n_exp in range(1, n_exps + 1):\n",
    "#     summary_path = f\"output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/train_results/results.csv\"\n",
    "#     df_summary = pd.read_csv(summary_path)\n",
    "#     df_summary.set_index(\"epoch\", inplace=True)\n",
    "#     best_epoch = df_summary[\"val_GT_shap_sim\"].idxmax()\n",
    "\n",
    "#     best_results_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/test_shap_{best_epoch}.pkl\"\n",
    "#     dict_results = sj_utils.load_pickle(best_results_path)\n",
    "#     df_shap = get_features_and_global_shap(dict_results, n_exp)\n",
    "\n",
    "#     df = pd.concat([df, df_shap], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 10))\n",
    "# df_mean = df.groupby(\"features\")[\"scores\"].mean()\n",
    "# df_mean.sort_values(ascending=False, inplace=True)\n",
    "# df_mean_cols = df_mean.index\n",
    "# ax = sns.boxplot(x=\"features\", y=\"scores\", data=df, order=df_mean_cols)\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best models results to S3\n",
    "# n_exps = 10\n",
    "# for n_exp in range(1, n_exps + 1):\n",
    "#     print(f\"Copying best model data to s3 for exp={n_exp}...\")\n",
    "#     summary_path = f\"output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/train_results/results.csv\"\n",
    "#     df_summary = pd.read_csv(summary_path)\n",
    "#     df_summary.set_index(\"epoch\", inplace=True)\n",
    "#     best_epoch = df_summary[\"val_GT_shap_sim\"].idxmax()\n",
    "\n",
    "#     model_path = f\"./output/Final_Manually_Tuned/30/10/xgb/models/model_{best_epoch}.pkl\"\n",
    "#     val_best_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/val_shap_{best_epoch}.pkl\"\n",
    "#     test_best_path = f\"./output/Final_Manually_Tuned/{seq_len}/{n_exp:02}/{model_name}/shap/test_shap_{best_epoch}.pkl\"\n",
    "\n",
    "#     s3_dir = f\"s3://merck-paper-bucket/Synthetic-events/final_event_30_xgb/{n_exp}/\"\n",
    "\n",
    "#     # copy files to s3\n",
    "#     command_pattern = f\"aws s3 cp {'{}'} {s3_dir}\"\n",
    "\n",
    "#     command = command_pattern.format(summary_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(model_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(val_best_path)\n",
    "#     os.system(command)\n",
    "\n",
    "#     command = command_pattern.format(test_best_path)\n",
    "#     os.system(command)\n",
    "# print(\"Models data successfully copied to S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
