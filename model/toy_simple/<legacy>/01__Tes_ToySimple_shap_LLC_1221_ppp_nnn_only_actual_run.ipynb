{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Case: PPP and NNN cases only\n",
    "Author: Lin Lee Cheong <br>\n",
    "Date: 12/21/2020 <br> <br>\n",
    "\n",
    "PPP event is defined as one adverse token and one adverse helper token. \n",
    "- adverse_tokens:\n",
    "    - AMI\n",
    "    - PH\n",
    "    - ARR\n",
    "    - CHF\n",
    "\n",
    "- adverse_helper_tokens:\n",
    "    - apnea\n",
    "    - furosemide\n",
    "    - pneumonia\n",
    "    - high_creatinine\n",
    "    - tachycardia\n",
    "    - resistent_hyp\n",
    "    \n",
    "Findings:\n",
    "1. Main source of variations between SHAP values originals from model overtraining. Models with large feature space ends up with strange attributions of features. The sweet spot seems to be around 1:10 param:obs, slower learning rate, aggressive stopping. <If the feature attributes don't make sense, it means you've overtrained>.\n",
    "2. For SHAP: use negative background to backfill missingness that calculates the effects of the given feature. About 500 is sufficient. \n",
    "\n",
    "TODO:\n",
    "1. First event still has plenty of high attributes. Need to look into trainable init hidden states.\n",
    "2. Start reducing probabilities (performance drop) and see how the 'latching on' of important features change\n",
    "3. Look into how to do feature comparisons\n",
    "4. Define attention models and see how they help in such cases.\n",
    "5. Retry this on the original toy dataset\n",
    "6. Multiprocessing explainer otherwise it will take forever!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pathos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pathos.multiprocessing as multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import deep_id_pytorch_debug2 as deep_id_pytorch_debug\n",
    "\n",
    "#from lstm_models import *\n",
    "from lstm_models_cpu import *\n",
    "#from lstm_utils import *\n",
    "from lstm_utils_cpu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 1\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/30/train.csv\"\n",
    "valid_data_path = \"../../data/toy_dataset/data/30/val.csv\"\n",
    "test_data_path = \"../../data/toy_dataset/data/30/test.csv\"\n",
    "\n",
    "model_save_path = './models/lstm/lstm_seq_{}'.format(seq_len)\n",
    "results_save_path = \"./model_results/lstm/lstm_seq_{}\".format(seq_len)\n",
    "batch_size = 8\n",
    "\n",
    "n_epochs = 1\n",
    "stop_num = 2\n",
    "\n",
    "embedding_dim = 5\n",
    "hidden_dim = 8\n",
    "bidirectional = True\n",
    "dropout = 0.4\n",
    "n_layers = 3\n",
    "\n",
    "target_colname = 'label'\n",
    "uid_colname = 'patient_id'\n",
    "x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = '1'\n",
    "\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "for fp in [model_save_path, results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "#model_device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model_device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../data/toy_dataset/data/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "                                train_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=None,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "                                valid_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "                                test_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_tokens(dataloader, del_pad=True):\n",
    "    counter = None\n",
    "    for _, _, idxes in dataloader:\n",
    "        idxes = idxes.flatten().tolist()\n",
    "        if counter is None:\n",
    "            counter = Counter(idxes)\n",
    "        else:\n",
    "            counter.update(idxes)\n",
    "    counter = dict(counter)\n",
    "    if del_pad:\n",
    "        del counter[0]\n",
    "    return counter\n",
    "\n",
    "\n",
    "def plot_data(counter, title):\n",
    "    plt.bar(counter.keys(), counter.values(), width=1.0, color='g')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Token IDs')\n",
    "    plt.ylabel('Freqs')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_counter = count_tokens(train_dataloader, del_pad=True)\n",
    "# plot_data(train_counter, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_counter = count_tokens(valid_dataloader, del_pad=True)\n",
    "# plot_data(val_counter, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_counter2 = count_tokens(test_dataloader2, del_pad=True)\n",
    "# plot_data(test_counter2, 'Test All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_counter = count_tokens(test_dataloader, del_pad=True)\n",
    "# plot_data(test_counter, 'Test PPP Only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3,4,5\"\n",
    "#! echo ${CUDA_VISIBLE_DEVICES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=n_layers, dropout=dropout)\n",
    "#model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4465"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 48s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.493 | Train AUC: 0.84 \t Val. Loss: 0.336 |  Val. AUC: 0.8955\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "        model, valid_dataloader, loss_function\n",
    "    )  # , return_preds=False\n",
    "    # )\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.336 | Val AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_auc, val_labs, val_pred_scores = epoch_val_lstm(\n",
    "    model, valid_dataloader, loss_function, return_preds=True\n",
    ")\n",
    "print(f\"Val Loss: {val_loss:.3f} | Val AUC: {val_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (PPP only): 0.334 | Test AUC (PPP only): 0.90\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   model, test_dataloader, loss_function)#, return_preds=False\n",
    "print(f\"Test Loss (PPP only): {test_loss:.3f} | Test AUC (PPP only): {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48966666666666664"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1 if x > 0.5 else 0 for x in val_pred_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# sns.distplot(val_pred_scores, kde=False, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_auc, labs, pred_scores = epoch_val_lstm(\n",
    "    model, test_dataloader, loss_function, return_preds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49183333333333334"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1 if x > 0.5 else 0 for x in pred_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# sns.distplot(pred_scores, kde=False, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the negative cases as background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select non-AE cases only as background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_background = 500\n",
    "sel_set = []\n",
    "for batch in valid_dataloader:\n",
    "    sel_set.extend(\n",
    "        [\n",
    "            (uid, lab, idxes)\n",
    "            for (uid, lab, idxes) in zip(batch[0], batch[1], batch[2])\n",
    "            if lab == 0\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if len(sel_set) > n_background:\n",
    "        break\n",
    "        \n",
    "sel_set = sel_set[:n_background]\n",
    "    \n",
    "background_ids = [x[0] for x in sel_set]\n",
    "background_labels = [x[1] for x in sel_set]\n",
    "background_idxes = [x[2] for x in sel_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 30])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_idxes = torch.stack(background_idxes)\n",
    "background_idxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data, bg_masks = model.get_all_ids_masks(background_idxes, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 30, 32])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Model type not yet supported by TreeExplainer: <class 'lstm_models_cpu.SimpleLSTM'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f936cca688a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, **deprecated_options)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_perturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model type not yet supported by TreeExplainer: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;31m# build a dense numpy version of all the tree objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Model type not yet supported by TreeExplainer: <class 'lstm_models_cpu.SimpleLSTM'>"
     ]
    }
   ],
   "source": [
    "shap.shap_values(model, approximate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:03, 126.03it/s]\n"
     ]
    }
   ],
   "source": [
    "explainer = deep_id_pytorch_debug.CustomPyTorchDeepIDExplainer(\n",
    "    model, bg_data, bg_masks, gpu_memory_efficient=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train() # in case that shap complains that autograd cannot be called\n",
    "lstm_values = []\n",
    "features = []\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AE events to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "sel_set = []\n",
    "for batch in test_dataloader:\n",
    "    sel_set.extend(\n",
    "        [\n",
    "            (uid, lab, idxes)\n",
    "            for (uid, lab, idxes) in zip(batch[0], batch[1], batch[2])\n",
    "            if lab == 1\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if len(sel_set) > n_test:\n",
    "        break\n",
    "        \n",
    "sel_set = sel_set[:n_test]\n",
    "    \n",
    "test_ids = [x[0] for x in sel_set]\n",
    "test_labels = [x[1] for x in sel_set]\n",
    "test_idxes = [x[2] for x in sel_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idxes = torch.stack(test_idxes)\n",
    "test_idxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_masks = model.get_all_ids_masks(test_idxes, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30, 32])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 32])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (emb_layer): Embedding(32, 5, padding_idx=0)\n",
       "  (lstm): LSTM(5, 8, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n",
      "I am here 0\n",
      "I am here 0\n",
      "I am here 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n",
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 55.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n",
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 58.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 58.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:18, 54.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n",
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:00, 56.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:00, 56.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:00, 55.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here 0\n",
      "I am here 1\n",
      "I am here 2\n",
      "gradient\n",
      "gradient 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:18, 55.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:17, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient 2.2\n",
      "gradient else\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-359:\n",
      "Process ForkPoolWorker-360:\n",
      "Process ForkPoolWorker-361:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-af11576106e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_shap_values_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total time: {:.2f}mins'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/deep_id_pytorch_debug2.py\u001b[0m in \u001b[0;36mshap_values_parallel\u001b[0;34m(self, X, masks, ranked_outputs, output_rank_order)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m#pool = Pool(os.cpu_count())      # Create a multiprocessing Pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0moutput_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_1_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/queues.py\", line 337, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/queues.py\", line 338, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/queues.py\", line 337, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/connection.py\", line 219, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/connection.py\", line 410, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/multiprocess/connection.py\", line 382, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lstm_shap_values_p = explainer.shap_values_parallel(test_data, test_masks)\n",
    "print('Total time: {:.2f}mins'.format((time.time()-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lstm_shap_values = explainer.shap_values(test_data, test_masks)\n",
    "print('Total time: {:.2f}mins'.format((time.time()-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_values, features = [], []\n",
    "for idx, label in zip(range(test_idxes.shape[0]), test_labels):\n",
    "    features.append([model.vocab.itos(x) for x in test_idxes[idx].numpy()])\n",
    "    w = []\n",
    "    for seq_idx, tok_idx in enumerate(test_idxes[idx].numpy()):\n",
    "        w.append(lstm_shap_values[idx, seq_idx, tok_idx])\n",
    "    lstm_values.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix background + dropout in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH NEG AS BACKGROUND + EMBEDDING DROPOUT + 30000 observation in training + \n",
    "# reduced dim space, 3 layers, bidirectional\n",
    "# raise dropout, single epoch training, drop training rate\n",
    "\n",
    "#for idx in range(n_test):\n",
    "for idx in range(10):\n",
    "    df = pd.DataFrame()\n",
    "    df['events'] = features[idx]\n",
    "    df['shap_vals'] = lstm_values[idx]\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = sns.barplot(x=df.index, y=df.shap_vals, orient='v')\n",
    "    z = ax.set_xticklabels(df.events, rotation=90)\n",
    "    plt.title(f\"idx: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find overall contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for idx in range(n_test):\n",
    "    for token, value in zip(features[idx], lstm_values[idx]):\n",
    "        if token not in results:\n",
    "            results[token] = {}\n",
    "            results[token]['val'] = 0\n",
    "            results[token]['count'] = 0\n",
    "        results[token]['val'] += value\n",
    "        results[token]['count'] += 1\n",
    "        \n",
    "for token in results.keys():\n",
    "    results[token]['ave'] = results[token]['val'] / results[token]['count']\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "res = [(k, results[k]['ave'], results[k]['val']) for k in results.keys()]\n",
    "results_df['token'] = [i[0] for i in res]\n",
    "results_df['ave'] = [i[1] for i in res]\n",
    "results_df['tot'] = [i[2] for i in res]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('ave', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['ave_abs'] = results_df.ave.abs()\n",
    "results_df.sort_values('ave_abs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
