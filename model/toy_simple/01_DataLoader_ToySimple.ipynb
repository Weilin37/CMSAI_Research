{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1000\n",
    "min_freq = 1\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/train.csv\"\n",
    "valid_data_path = \"../../data/toy_dataset/data/val.csv\"\n",
    "\n",
    "model_save_path = './sample_model/simple_lstm_201215'\n",
    "results_save_path = \"./sample_results/simple_results_201215\"\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "target_colname = 'label'\n",
    "uid_colname = 'patient_id'\n",
    "x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "for fp in [model_save_path, results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocab\n",
    "\n",
    "equivalent to model.create_word_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = train_data_path\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_token(t):\n",
    "    if len(t) == 0 or t.lower() == \"<pad>\":\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "with open(datapath, \"r\") as f:\n",
    "    # determine column mapping\n",
    "    header = f.readline()\n",
    "    header = [h.replace(\" \", \"\").replace(\"\\n\", \"\") for h in header.split(\",\")]\n",
    "\n",
    "    target_index = header.index(target_colname)\n",
    "\n",
    "    uid_index = header.index(uid_colname)\n",
    "\n",
    "    x_idxes = []\n",
    "    for colname in x_inputs:\n",
    "        x_idxes.append(header.index(colname))\n",
    "\n",
    "    # start processing \n",
    "    line = f.readline()\n",
    "    invalid_uid = 0\n",
    "    deaths = 0\n",
    "    while line:\n",
    "        if 'death' in line:\n",
    "            deaths += 1\n",
    "            pass\n",
    "\n",
    "        tokens = line.split(\",\")\n",
    "\n",
    "        if len(tokens[uid_index]) == 0:\n",
    "            invalid_uid += 1 # some UIDS are missing\n",
    "            pass\n",
    "        else:\n",
    "            uid = tokens[uid_index].replace('\\n', '')\n",
    "\n",
    "        ## CHANGE: integer\n",
    "        if isinstance(tokens[target_index], str):\n",
    "            label = 1 if tokens[target_index].startswith(target_value) else 0\n",
    "        \n",
    "        if isinstance(tokens[target_index], int):\n",
    "            label = tokens[target_index]\n",
    "\n",
    "        tokens = [tokens[idx] for idx in x_idxes]\n",
    "        tokens = [t.strip().replace('\\n', '') for t in tokens if valid_token(t)]\n",
    "\n",
    "        if rev:\n",
    "            tokens = tokens[::-1]\n",
    "\n",
    "        token_list.append((uid, label, tokens))\n",
    "\n",
    "        line = f.readline()\n",
    "\n",
    "        if len(token_list) == nrows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define vocabulary without torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "rev_vocab = {}\n",
    "rev_vocab[0] = '<pad>'\n",
    "rev_vocab[1] = '<unk>'\n",
    "\n",
    "counter = Counter()\n",
    "for uid, label, tokens in token_list:\n",
    "    for token in tokens:\n",
    "        counter[token] += 1\n",
    "\n",
    "for token in counter:\n",
    "    if counter[token] < min_freq:\n",
    "        continue\n",
    "        \n",
    "    idx = len(vocab)\n",
    "    vocab[token] = idx\n",
    "    rev_vocab[idx] = token\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab, rev_vocab):\n",
    "        self._vocab = vocab\n",
    "        self._rev_vocab = rev_vocab\n",
    "        self._unk_idx = vocab['<unk>']\n",
    "        self._pad_idx = vocab['<pad>']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._vocab)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        if token in self._vocab:\n",
    "            return self._vocab[token]\n",
    "        return self._vocab['<unk>']\n",
    "    \n",
    "    def itos(self, idx):\n",
    "        if idx in self._rev_vocab:\n",
    "            return self._rev_vocab[idx]\n",
    "        return self._unk_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'annual_physical': 2,\n",
       " 'foot_pain': 3,\n",
       " 'eye_exam': 4,\n",
       " 'hay_fever': 5,\n",
       " 'ACE_inhibitors': 6,\n",
       " 'ingrown_nail': 7,\n",
       " 'cold_sore': 8,\n",
       " 'dental_exam': 9,\n",
       " 'apnea': 10,\n",
       " 'PCI': 11,\n",
       " 'backache': 12,\n",
       " 'cut_finger': 13,\n",
       " 'ankle_sprain': 14,\n",
       " 'quad_injury': 15,\n",
       " 'high_creatinine': 16,\n",
       " 'resistent_hyp': 17,\n",
       " 'tachycardia': 18,\n",
       " 'myopia': 19,\n",
       " 'CHF': 20,\n",
       " 'headache': 21,\n",
       " 'normal_bmi': 22,\n",
       " 'cardiac_rehab': 23,\n",
       " 'low_salt_diet': 24,\n",
       " 'ACL_tear': 25,\n",
       " 'peanut_allergy': 26,\n",
       " 'pneumonia': 27,\n",
       " 'AMI': 28,\n",
       " 'PH': 29,\n",
       " 'furosemide': 30,\n",
       " 'ARR': 31}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_class = Vocab(vocab, rev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (pid, [label], [vocab_class.stoi(t) for t in tokens])\n",
    "    for pid, label, tokens in token_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DU5LE6OJSR', [0], [21, 22, 23, 24])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    '''\n",
    "    Similar to transformer's, adapted to this dataset with Marc's line by line reading output\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, max_len=35, pad_idx=0):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        idx_lst = data[idx][2]\n",
    "        if len(idx_lst) > self.max_len:\n",
    "            idx_lst = idx_lst[:self.max_len]\n",
    "        idx_lst = idx_lst + [self.pad_idx] * (self.max_len - len(idx_lst))\n",
    "            \n",
    "        return (\n",
    "            data[idx][0],\n",
    "            torch.tensor(data[idx][1]),\n",
    "            torch.tensor(idx_lst) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = ToyDataset(data, max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    toy_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('KWPSWXBCOO',\n",
       "  'OAB32HBRGF',\n",
       "  'GVKVDO2JQP',\n",
       "  'DU5LE6OJSR',\n",
       "  'EYL9OAO7UU',\n",
       "  'EA3Y23B5G4',\n",
       "  'HA9XRJBP80',\n",
       "  'GSD914SV90',\n",
       "  'AI58JP94TT',\n",
       "  '1BFRROCHC9',\n",
       "  '7XFPYN3G6K',\n",
       "  'AIRXLC21GS',\n",
       "  'BKD87ZKI6X',\n",
       "  'P4QJBL4D9U',\n",
       "  'UPOO9PS06M',\n",
       "  'RNW0E98SKN',\n",
       "  'CQ4UYADG1U',\n",
       "  'AIXRRUPL62',\n",
       "  'F2UBVPAOL2',\n",
       "  'OXL8BE7K0X',\n",
       "  '94NSP0QGZL',\n",
       "  'L7VFUQT4AP',\n",
       "  '3LMWS2U7KE',\n",
       "  'NGOXR1IT77',\n",
       "  'WGIIXYGPEQ',\n",
       "  'VH5QT1BR9C',\n",
       "  '1R4IP7CEUB',\n",
       "  '7A1OQ4W91A',\n",
       "  '4TMHC604U5',\n",
       "  'O2XPIZBL96',\n",
       "  '9UX6ZVXL1Q',\n",
       "  '7L1IZ18Z74',\n",
       "  '9SJY4TSXJR',\n",
       "  '302YJ2ZEMT',\n",
       "  'BMKYCDP4LN',\n",
       "  'W842GUPG7Q',\n",
       "  '21AW4N8JKC',\n",
       "  'ZGWVOTQVQZ',\n",
       "  '4I1RJPR5A7',\n",
       "  'DQ6SIIE0I8',\n",
       "  '4ERS8UHLXC',\n",
       "  'E6R21HM4YQ',\n",
       "  'LQSUVM7RPK',\n",
       "  'E67ZJ93TOR',\n",
       "  'D46MWQ6RSJ',\n",
       "  'SDUUKOGA8R',\n",
       "  '62YBG3WVFS',\n",
       "  '0B2MCYWFG7',\n",
       "  'FLKD8Y953C',\n",
       "  '6YZJHL5S9J',\n",
       "  'YL3NRAWKGK',\n",
       "  'KBMKJ9XHFW',\n",
       "  'ZZT9Y30HP2',\n",
       "  'B2HSUU7ALJ',\n",
       "  'GQCSIL3P1O',\n",
       "  'U2OKF5O8IE',\n",
       "  '2IEBXML89S',\n",
       "  'R1RB6W9KL7',\n",
       "  'OUEY62N32C',\n",
       "  'YU03WA1QGR',\n",
       "  '4Q428PHYFQ',\n",
       "  'H468KCYCTX',\n",
       "  'GF60340DPQ',\n",
       "  '4FE4FQGL7M'),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]),\n",
       " tensor([[ 2,  3,  4,  ...,  0,  0,  0],\n",
       "         [ 3, 12, 13,  ...,  0,  0,  0],\n",
       "         [18,  7,  9,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [14,  4,  9,  ...,  0,  0,  0],\n",
       "         [ 8, 19, 12,  ...,  0,  0,  0],\n",
       "         [19, 28, 26,  ...,  0,  0,  0]])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, vocab, device, nlayers=1, bidi=True, use_gpu=True,\n",
    "                 pad_idx=0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.input_dim = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.emb_layer = nn.Embedding(self.input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidi = bidi\n",
    "        self.nlayers = nlayers\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=nlayers,\n",
    "                            bidirectional=bidi,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.pred_layer = nn.Linear(hidden_dim * 2, 2) if bidi else nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.3\n",
    "        self.emb_layer.weight.data.uniform_(-initrange, initrange) \n",
    "        \n",
    "        self.pred_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        self.pred_layer.bias.data.zero_()   \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        count = self.nlayers * 2 if self.bidi else self.nlayers\n",
    "        \n",
    "        weight = next(self.parameters())\n",
    "        \n",
    "        directionality = 2 if self.bidi else 1\n",
    "        weights = (\n",
    "            weight.new_zeros(count * directionality, batch_size, self.hidden_dim),\n",
    "            weight.new_zeros(count * directionality, batch_size, self.hidden_dim)\n",
    "        )\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            return (weights[0].cuda(), weights[1].cuda())\n",
    "        \n",
    "        return weights\n",
    "        \n",
    "    def repackage_hidden(h):\n",
    "        \"\"\"\n",
    "        Wraps hidden states in new Tensors, to detach them from their history.\n",
    "        Needed to prevent RNN+Attention backpropagating between batches.\n",
    "        \"\"\"\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        \n",
    "        if isinstance(h, tuple):\n",
    "            return (v.detach() for v in h)\n",
    "    \n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        embedded = self.emb_layer(tokens)\n",
    "        \n",
    "        hidden = self.init_hidden(tokens.shape[0])\n",
    "        hidden = self.repackage_hidden()\n",
    "        \n",
    "        text_lengths = torch.sum(tokens != self.pad_idx, dim=1)\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded, hidden)\n",
    "        \n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        output = output.permute(1, 0, 2) # [batch, text_length, hidden_dim]\n",
    "        \n",
    "        if self.bidi:\n",
    "            out = torch.cat([output[:, -1, :self.hidden_dim], output[:, 0, self.hidden_dim:]],\n",
    "                           dim=1)\n",
    "        else:\n",
    "            out = output[:, -1, :]\n",
    "        \n",
    "        pred = self.pred_layer(out)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        if not os.path.isdir(os.path.split(filepath)[0]):\n",
    "            os.makedirs(os.path.split(filepath)[0])\n",
    "        torch.save({self.state_dict()}, filepath)\n",
    "                   \n",
    "    def load_model(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))\n",
    "        \n",
    "    def get_ids_masks(self, token_list, padding_length):\n",
    "        '''Only enabled for one example at a time'''\n",
    "        ids, masks = np.zeros((padding_length, len(self.vocab))), [0] * padding_length\n",
    "        \n",
    "        for (i, idx_token) in enumerate(token_list):\n",
    "            if i >= padding_length or idx_token == self.pad_idx:\n",
    "                break\n",
    "            ids[i, idx_token] = 1\n",
    "            masks[i] = 1\n",
    "\n",
    "        return torch.FloatTensor(ids), masks\n",
    "    \n",
    "    def get_all_ids_masks(self, data, padding_length):\n",
    "        ids,  masks = [], []\n",
    "        for obs in data:\n",
    "            id_vals, mask_vals = self.get_ids_masks(obs, padding_length)\n",
    "            ids.append(id_vals)\n",
    "            masks.append(mask_vals)\n",
    "            \n",
    "        return torch.stack(ids), masks\n",
    "    \n",
    "    def forward_shap(self, token_ids, mask, full_id_matrix=False):\n",
    "        '''Only enabled for one example at a time'''\n",
    "        token_ids = token_ids if token_ids.is_cuda else token_ids.cuda()\n",
    "        \n",
    "        hidden = self.init_hidden(1)\n",
    "        hidden = self.repackage_hidden()\n",
    "        \n",
    "        emb = torch.matmul(token_ids, self.emb_layer.weight).unsqueeze(0)\n",
    "        \n",
    "        out, _ = self.lstm(emb, hidden)\n",
    "        \n",
    "        return self.pred_layer(out).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3,  4,  ...,  0,  0,  0],\n",
       "        [ 3, 12, 13,  ...,  0,  0,  0],\n",
       "        [18,  7,  9,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [14,  4,  9,  ...,  0,  0,  0],\n",
       "        [ 8, 19, 12,  ...,  0,  0,  0],\n",
       "        [19, 28, 26,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 12, 10,  4, 18, 24, 28,  6, 14, 24, 18,  9, 10, 26, 15,  4, 26, 26,\n",
       "        25, 20,  9, 26, 21,  3,  7,  9, 12,  6, 28, 11, 28, 23, 27,  7,  6, 19,\n",
       "        27, 17, 21,  4,  4, 25, 26, 10,  5, 16, 14,  4, 29, 15, 27, 26, 27,  8,\n",
       "        25, 27, 14, 11, 12, 22,  5, 19, 12, 18])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(single[2] != 0, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18,  7,  9,  9, 19,  7,  2, 12,  5, 20,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(single[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single[2].is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(embedding_dim, hidden_dim, vocab_class, model_device)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.emb_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-aa8380a92dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-29f9517f87c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         packed_embedded = nn.utils.rnn.pack_padded_sequence(\n\u001b[0;32m---> 72\u001b[0;31m             embedded, text_lengths, enforce_sorted=False, batch_first=True)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor"
     ]
    }
   ],
   "source": [
    "preds = model(single[2].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('YY7IC1FOD3',\n",
       " tensor([0]),\n",
       " tensor([ 2,  3,  4,  5,  6,  7,  8,  4,  2,  2,  9,  8,  4, 10, 11,  8,  4,  7,\n",
       "         12,  7, 13, 14,  2, 15,  9,  7, 16, 17,  0,  0]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('YXRO57K7EL',\n",
       " 1,\n",
       " ['annual_physical',\n",
       "  'myopia',\n",
       "  'backache',\n",
       "  'annual_physical',\n",
       "  'cut_finger',\n",
       "  'ingrown_nail',\n",
       "  'hay_fever',\n",
       "  'peanut_allergy',\n",
       "  'CHF'])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_ids_masks(toy_dataset[3][2], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, mask = model.get_ids_masks(toy_dataset[3][2], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_shap(ids, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd83e4f0860>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "seaborn.heatmap(model.get_ids_masks(toy_dataset[3][2], 15)[0],\n",
    "                linewidths=0.4, cmap=\"YlGnBu\", linecolor='gray', \n",
    "                vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train_lstm(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    test=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model for an epoch, called by ModelProcess function\n",
    "    detach_hidden is used to detach hidden state between batches,\n",
    "    and will add a new hidden state to model. Model must have .init_hidden function defined\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        model (nn.Module): lstm general attention model\n",
    "        dataloader : iterator for dataset, yields (ids, sequence, seq length, labels)\n",
    "        criterion : loss function\n",
    "        batch_size : int default 0\n",
    "                     used when detach_hidden is enabled\n",
    "                     to create the correct hidden sizes during initialization\n",
    "        \n",
    "    Returns:\n",
    "    ----------\n",
    "        tuple containing:\n",
    "            average loss for whole epoch,\n",
    "            average AUC for whole epoch\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "           \n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # initialize lists to compare predictions & ground truth labels for metric calculation\n",
    "    order_labels = []\n",
    "    prediction_scores = []\n",
    "    if test: # test function on small number of batches\n",
    "        counter = 0\n",
    "        \n",
    "    for idx, (ids, labels, idxed_text) in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "          \n",
    "        labels = labels.type(torch.long)\n",
    "        idxed_text, labels = idxed_text.cuda(), labels.cuda()\n",
    "\n",
    "        predictions = model(idxed_text)        \n",
    "        #predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        \n",
    "        loss = criterion(predictions, labels.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # prevent internal pytorch timeout due to too many file opens by multiprocessing\n",
    "        copied_labels = copy.deepcopy(labels.detach().cpu().numpy())\n",
    "        del labels\n",
    "        order_labels.extend(copied_labels)\n",
    "        \n",
    "        copied_preds = copy.deepcopy(predictions.detach().cpu().numpy())\n",
    "        del predictions\n",
    "        prediction_scores.extend(copied_preds)\n",
    "        \n",
    "        epoch_loss += loss.item()        \n",
    "    \n",
    "        if test:\n",
    "            if counter >= test:\n",
    "                break\n",
    "            counter += 1\n",
    "    \n",
    "    epoch_metric = roc_auc_score(order_labels, torch.sigmoid(torch.Tensor(prediction_scores)[:, 1]))\n",
    "        \n",
    "    return epoch_loss / len(dataloader), epoch_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4665308780968189, 0.8722856091277145)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_train_lstm(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=loss_function,\n",
    "    test=0    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3564494103193283, 0.9007537085340289)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_train_lstm(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=loss_function,\n",
    "    test=0    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.333 | Train AUC: 0.92\n",
      "Train Loss: 0.326 | Train AUC: 0.93\n",
      "Train Loss: 0.315 | Train AUC: 0.93\n",
      "Train Loss: 0.306 | Train AUC: 0.94\n",
      "Train Loss: 0.288 | Train AUC: 0.95\n"
     ]
    }
   ],
   "source": [
    "#best_valid_loss = float(\"inf\")\n",
    "#valid_worse_loss = 0  # enable early stopping\n",
    "#stop_num = 6\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    #valid_loss, valid_auc = epoch_val_lstm(\n",
    "    #    model, valid_dataloader, criterion, return_preds=False\n",
    "    #)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    '''\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    log(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )'''\n",
    "    \n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEt SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_id_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = next(iter(train_dataloader))\n",
    "background_ids, background_labels, background_idxes = background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_idxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data, bg_masks = model.get_all_ids_masks(background_idxes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 32])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 919.04it/s]\n"
     ]
    }
   ],
   "source": [
    "explainer = deep_id_pytorch.CustomPyTorchDeepIDExplainer(model, bg_data, bg_masks,\n",
    "                                                         gpu_memory_efficient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # in case that shap complains that autograd cannot be called\n",
    "lstm_values = []\n",
    "features = []\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(train_dataloader)[3]\n",
    "test_ids, test_labels, test_idxes = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_masks = model.get_all_ids_masks(test_idxes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:00, 339.40it/s]\n",
      "128it [00:00, 321.75it/s]\n"
     ]
    }
   ],
   "source": [
    "lstm_shap_values = explainer.shap_values(test_data[0].unsqueeze(0), test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
