{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training using Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! jupyter nbextension enable jupyter-black-master/jupyter-black\n",
    "\n",
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 1\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/{}/train.csv\".format(seq_len)\n",
    "valid_data_path = \"../../data/toy_dataset/data/{}/val.csv\".format(seq_len)\n",
    "test_data_path = \"../../data/toy_dataset/data/{}/test.csv\".format(seq_len)\n",
    "\n",
    "lstm_model_save_path = './output/{}/lstm/models/model'.format(seq_len)\n",
    "lstm_results_save_path = \"./output/{}/lstm/results/\".format(seq_len)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 6\n",
    "stop_num = 2\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "nlayers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "target_colname = 'label'\n",
    "uid_colname = 'patient_id'\n",
    "x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = '1'\n",
    "\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output/900/lstm/models/model\n",
      "New directory created: ./output/900/lstm/results/\n",
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "#LSTM Output Directory\n",
    "for fp in [lstm_model_save_path, lstm_results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../data/toy_dataset/data/900/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/900/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/900/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "                                train_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=None,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "                                valid_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "                                test_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleLSTM Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SimpleLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout)\n",
    "lstm_model = lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.696 | Train AUC: 0.50 \t Val. Loss: 0.699 |  Val. AUC: 0.4912\n",
      "Epoch: 02 | Epoch Time: 0m 25s\n",
      "Saved Model, epoch 1\n",
      "Train Loss: 0.696 | Train AUC: 0.50 \t Val. Loss: 0.693 |  Val. AUC: 0.5007\n",
      "Epoch: 03 | Epoch Time: 0m 25s\n",
      "Train Loss: 0.696 | Train AUC: 0.50 \t Val. Loss: 0.694 |  Val. AUC: 0.4934\n",
      "Epoch: 04 | Epoch Time: 0m 25s\n",
      "Saved Model, epoch 3\n",
      "Train Loss: 0.695 | Train AUC: 0.50 \t Val. Loss: 0.693 |  Val. AUC: 0.5132\n",
      "Epoch: 05 | Epoch Time: 0m 25s\n",
      "Train Loss: 0.694 | Train AUC: 0.51 \t Val. Loss: 0.699 |  Val. AUC: 0.5112\n",
      "Epoch: 06 | Epoch Time: 0m 25s\n",
      "EARLY STOP ------\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "       lstm_model, valid_dataloader, loss_function)#, return_preds=False\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_model.state_dict(), lstm_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.693 | Test AUC: 0.48\n"
     ]
    }
   ],
   "source": [
    "lstm_model.load_state_dict(torch.load(lstm_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   lstm_model, test_dataloader, loss_function)#, return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model_save_path = './output/{}/lstm-att/models/model'.format(seq_len)\n",
    "lstm_att_results_save_path = \"./output/{}/lstm-att/results/\".format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output/900/lstm-att/models/model\n",
      "New directory created: ./output/900/lstm-att/results/\n"
     ]
    }
   ],
   "source": [
    "#LSTM with Attention Output Directory\n",
    "for fp in [lstm_att_model_save_path, lstm_att_results_save_path]:\n",
    "    if not os.path.exists(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model = AttLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout)\n",
    "lstm_att_model = lstm_att_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (attn_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       "  (context_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_att_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 26s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.457 | Train AUC: 0.87 \t Val. Loss: 0.347 |  Val. AUC: 0.8999\n",
      "Epoch: 02 | Epoch Time: 0m 26s\n",
      "Saved Model, epoch 1\n",
      "Train Loss: 0.360 | Train AUC: 0.89 \t Val. Loss: 0.340 |  Val. AUC: 0.8981\n",
      "Epoch: 03 | Epoch Time: 0m 26s\n",
      "Train Loss: 0.409 | Train AUC: 0.89 \t Val. Loss: 0.370 |  Val. AUC: 0.8990\n",
      "Epoch: 04 | Epoch Time: 0m 26s\n",
      "EARLY STOP ------\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_att_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "       lstm_att_model, valid_dataloader, loss_function)#, return_preds=False\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_att_model.state_dict(), lstm_att_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.334 | Test AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "lstm_att_model.load_state_dict(torch.load(lstm_att_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   lstm_att_model, test_dataloader, loss_function)#, return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot_path = 'output/{}/xgboost/data/train_one_hot.csv'.format(seq_len)\n",
    "x_valid_one_hot_path = 'output/{}/xgboost/data/val_one_hot.csv'.format(seq_len)\n",
    "x_test_one_hot_path = 'output/{}/xgboost/data/test_one_hot.csv'.format(seq_len)\n",
    "\n",
    "x_train_data_path = 'output/{}/xgboost/data/train.csv'.format(seq_len)\n",
    "x_valid_data_path = 'output/{}/xgboost/data/val.csv'.format(seq_len)\n",
    "x_test_data_path = 'output/{}/xgboost/data/test.csv'.format(seq_len)\n",
    "\n",
    "s3_output_data_dir = 's3://merck-paper-bucket/{}/data'.format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 903)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>899</th>\n",
       "      <th>898</th>\n",
       "      <th>897</th>\n",
       "      <th>896</th>\n",
       "      <th>895</th>\n",
       "      <th>894</th>\n",
       "      <th>893</th>\n",
       "      <th>892</th>\n",
       "      <th>891</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>pneumonia_H</td>\n",
       "      <td>pneumonia_H</td>\n",
       "      <td>cardiac_rehab_U</td>\n",
       "      <td>0</td>\n",
       "      <td>W522K3U1NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>tachycardia_H</td>\n",
       "      <td>0</td>\n",
       "      <td>4PY82TXWUI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2882</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>apnea_H</td>\n",
       "      <td>1</td>\n",
       "      <td>HD8EMMXS9G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2159</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>1</td>\n",
       "      <td>A07ZWN6OWW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2317</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>1</td>\n",
       "      <td>I37X5VLIV1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 903 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    899    898    897    896    895    894    893    892    891  ...  \\\n",
       "0     58  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1    583  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2   2882  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3   2159  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4   2317  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "                7               6                  5                 4  \\\n",
       "0     hay_fever_N      backache_N      quad_injury_N     quad_injury_N   \n",
       "1        myopia_N   dental_exam_N   peanut_allergy_N     quad_injury_N   \n",
       "2      eye_exam_N      ACL_tear_N  annual_physical_N        ACL_tear_N   \n",
       "3   quad_injury_N  ingrown_nail_N        foot_pain_N        eye_exam_N   \n",
       "4  ingrown_nail_N   quad_injury_N  annual_physical_N  peanut_allergy_N   \n",
       "\n",
       "                3                 2            1                  0 label  \\\n",
       "0      ACL_tear_N       pneumonia_H  pneumonia_H    cardiac_rehab_U     0   \n",
       "1  ingrown_nail_N          myopia_N  hay_fever_N      tachycardia_H     0   \n",
       "2        myopia_N        headache_N  foot_pain_N            apnea_H     1   \n",
       "3      ACL_tear_N  peanut_allergy_N  hay_fever_N           myopia_N     1   \n",
       "4      eye_exam_N        eye_exam_N  foot_pain_N  annual_physical_N     1   \n",
       "\n",
       "   patient_id  \n",
       "0  W522K3U1NM  \n",
       "1  4PY82TXWUI  \n",
       "2  HD8EMMXS9G  \n",
       "3  A07ZWN6OWW  \n",
       "4  I37X5VLIV1  \n",
       "\n",
       "[5 rows x 903 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_tokens(tokens):\n",
    "#     \"\"\"Get all tokens except <pad> and <unk>\"\"\"\n",
    "#     my_tokens = []\n",
    "#     for key, val in tokens.items():\n",
    "#         if val>=2:\n",
    "#             my_tokens.append(key)\n",
    "#     my_tokens\n",
    "#     return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = lstm_model.vocab._vocab\n",
    "my_tokens = get_valid_tokens(tokens)\n",
    "#my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>899</th>\n",
       "      <th>898</th>\n",
       "      <th>897</th>\n",
       "      <th>896</th>\n",
       "      <th>895</th>\n",
       "      <th>894</th>\n",
       "      <th>893</th>\n",
       "      <th>892</th>\n",
       "      <th>891</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>backache_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>pneumonia_H</td>\n",
       "      <td>pneumonia_H</td>\n",
       "      <td>cardiac_rehab_U</td>\n",
       "      <td>0</td>\n",
       "      <td>W522K3U1NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>583</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>tachycardia_H</td>\n",
       "      <td>0</td>\n",
       "      <td>4PY82TXWUI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2882</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>apnea_H</td>\n",
       "      <td>1</td>\n",
       "      <td>HD8EMMXS9G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2159</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>1</td>\n",
       "      <td>A07ZWN6OWW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2317</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>eye_exam_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>1</td>\n",
       "      <td>I37X5VLIV1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 903 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    899    898    897    896    895    894    893    892    891  ...  \\\n",
       "0     58  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "1    583  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "2   2882  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "3   2159  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "4   2317  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  <pad>  ...   \n",
       "\n",
       "                7               6                  5                 4  \\\n",
       "0     hay_fever_N      backache_N      quad_injury_N     quad_injury_N   \n",
       "1        myopia_N   dental_exam_N   peanut_allergy_N     quad_injury_N   \n",
       "2      eye_exam_N      ACL_tear_N  annual_physical_N        ACL_tear_N   \n",
       "3   quad_injury_N  ingrown_nail_N        foot_pain_N        eye_exam_N   \n",
       "4  ingrown_nail_N   quad_injury_N  annual_physical_N  peanut_allergy_N   \n",
       "\n",
       "                3                 2            1                  0 label  \\\n",
       "0      ACL_tear_N       pneumonia_H  pneumonia_H    cardiac_rehab_U     0   \n",
       "1  ingrown_nail_N          myopia_N  hay_fever_N      tachycardia_H     0   \n",
       "2        myopia_N        headache_N  foot_pain_N            apnea_H     1   \n",
       "3      ACL_tear_N  peanut_allergy_N  hay_fever_N           myopia_N     1   \n",
       "4      eye_exam_N        eye_exam_N  foot_pain_N  annual_physical_N     1   \n",
       "\n",
       "   patient_id  \n",
       "0  W522K3U1NM  \n",
       "1  4PY82TXWUI  \n",
       "2  HD8EMMXS9G  \n",
       "3  A07ZWN6OWW  \n",
       "4  I37X5VLIV1  \n",
       "\n",
       "[5 rows x 903 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess!\n",
      "Sucess!\n",
      "Sucess!\n"
     ]
    }
   ],
   "source": [
    "prepare_data(train_data_path, x_train_one_hot_path, x_train_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(valid_data_path, x_valid_one_hot_path, x_valid_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(test_data_path, x_test_one_hot_path, x_test_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'merck-paper-bucket'\n",
    "DATA_PREFIX = '{}/data'.format(seq_len)\n",
    "MODEL_PREFIX = '{}/xgboost/model'.format(seq_len)\n",
    "label = 'label'\n",
    "\n",
    "output_results_path = 'output/{}/xgboost/train/train_results.csv'.format(seq_len)\n",
    "local_model_dir = 'output/{}/xgboost/models/'.format(seq_len)\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, MODEL_PREFIX)\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = 'xgboost'\n",
    "REPO_VERSION = '1.2-1'\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = 'ml.m5.4xlarge'#'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 1#4 #TODO: Remove\n",
    "MAX_TRAIN_JOBS = 1#20\n",
    "\n",
    "EVALUATION_METRIC = 'auc'\n",
    "OBJECTIVE = 'binary:logistic'\n",
    "OBJECTIVE_METRIC_NAME = 'validation:auc'\n",
    "\n",
    "#Update hyperparameter ranges\n",
    "# HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0, 1),\n",
    "#                         'alpha': ContinuousParameter(0, 2),\n",
    "#                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0.1, 0.5),\n",
    "                       'alpha': ContinuousParameter(0, 2),\n",
    "                       'max_depth': IntegerParameter(1, 10),\n",
    "                       'gamma': ContinuousParameter(0, 5),\n",
    "                       'num_round': IntegerParameter(200, 500),\n",
    "                       'colsample_bylevel': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bynode': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "                       'lambda': ContinuousParameter(0, 1000),\n",
    "                       'max_delta_step': IntegerParameter(0, 10),\n",
    "                       'min_child_weight': ContinuousParameter(0, 120),\n",
    "                       'subsample': ContinuousParameter(0.5, 1.0),\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for seq_len=900, label=label...\n",
      ".............................................!\n",
      "Total jobs completed: 1\n",
      "Metric: validation:auc\n",
      "Best AUC: 0.8989\n",
      "Success! Total training time=3.8267794847488403 mins.\n",
      "ALL SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "start = time.time()\n",
    "print('Training for seq_len={}, label={}...'.format(seq_len, label))\n",
    "#Prepare the input train & validation data path\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/val'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "\n",
    "#Class Imbalance\n",
    "scale_pos_weight = 1.0 # negative/positive\n",
    "\n",
    "data_channels = {'train': s3_input_train, 'validation': s3_input_validation}\n",
    "\n",
    "tuner = train_hpo(hyperparameter_ranges=HYPERPARAMETER_RANGES, \n",
    "                  container=container, \n",
    "                  execution_role=role, \n",
    "                  instance_count=TRAIN_INSTANCE_COUNT, \n",
    "                  instance_type=TRAIN_INSTANCE_TYPE, \n",
    "                  output_path=s3_output_path, \n",
    "                  sagemaker_session=sess, \n",
    "                  eval_metric=EVALUATION_METRIC, \n",
    "                  objective=OBJECTIVE, \n",
    "                  objective_metric_name=OBJECTIVE_METRIC_NAME, \n",
    "                  max_train_jobs=MAX_TRAIN_JOBS, \n",
    "                  max_parallel_jobs=MAX_PARALLEL_JOBS, \n",
    "                  scale_pos_weight=scale_pos_weight, \n",
    "                  data_channels=data_channels)\n",
    "\n",
    "#Get the hyperparameter tuner status at regular interval\n",
    "val_auc, best_model_path = get_tuner_status_and_result_until_completion(tuner, seq_len, label)\n",
    "\n",
    "result = [label, seq_len, val_auc, best_model_path]\n",
    "training_results = [result]\n",
    "\n",
    "print('Success! Total training time={} mins.'.format((time.time()-start)/60.0))\n",
    "#Save the results to file\n",
    "df_results = pd.DataFrame(training_results, columns=['class', 'seq_len', 'val_auc', 'best_model_path'])\n",
    "\n",
    "if not os.path.isdir(os.path.split(output_results_path)[0]):\n",
    "    os.makedirs(os.path.split(output_results_path)[0])\n",
    "\n",
    "df_results.to_csv(output_results_path, index=False)\n",
    "print('ALL SUCCESS!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
