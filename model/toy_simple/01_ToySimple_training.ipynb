{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training using Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! jupyter nbextension enable jupyter-black-master/jupyter-black\n",
    "\n",
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 1\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/{}/train.csv\".format(seq_len)\n",
    "valid_data_path = \"../../data/toy_dataset/data/{}/val.csv\".format(seq_len)\n",
    "test_data_path = \"../../data/toy_dataset/data/{}/test.csv\".format(seq_len)\n",
    "\n",
    "lstm_model_save_path = './output/{}/lstm/models/model'.format(seq_len)\n",
    "lstm_results_save_path = \"./output/{}/lstm/results/\".format(seq_len)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 2\n",
    "stop_num = 2\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "nlayers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "target_colname = 'label'\n",
    "uid_colname = 'patient_id'\n",
    "x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = '1'\n",
    "\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New directory created: ./output/30/lstm/results/\n",
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "#LSTM Output Directory\n",
    "for fp in [lstm_model_save_path, lstm_results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../data/toy_dataset/data/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "                                train_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=None,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "                                valid_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "                                test_data_path,\n",
    "                                min_freq=min_freq,\n",
    "                                uid_colname=\"patient_id\",\n",
    "                                target_colname=\"label\",\n",
    "                                max_len=seq_len,\n",
    "                                target_value=target_value,\n",
    "                                vocab=vocab,\n",
    "                                nrows=nrows,\n",
    "                                rev=rev\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleLSTM Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SimpleLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout)\n",
    "lstm_model = lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.416 | Train AUC: 0.88 \t Val. Loss: 0.319 |  Val. AUC: 0.8998\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "Train Loss: 0.355 | Train AUC: 0.90 \t Val. Loss: 0.321 |  Val. AUC: 0.9014\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "       lstm_model, valid_dataloader, loss_function)#, return_preds=False\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_model.state_dict(), lstm_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.320 | Test AUC: 0.91\n"
     ]
    }
   ],
   "source": [
    "lstm_model.load_state_dict(torch.load(lstm_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   lstm_model, test_dataloader, loss_function)#, return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model_save_path = './output/{}/lstm-att/models/model'.format(seq_len)\n",
    "lstm_att_results_save_path = \"./output/{}/lstm-att/results/\".format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM with Attention Output Directory\n",
    "for fp in [lstm_att_model_save_path, lstm_att_results_save_path]:\n",
    "    if not os.path.exists(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model = AttLSTM(embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout)\n",
    "lstm_att_model = lstm_att_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (attn_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       "  (context_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_att_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.371 | Train AUC: 0.90 \t Val. Loss: 0.325 |  Val. AUC: 0.9046\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "Saved Model, epoch 1\n",
      "Train Loss: 0.359 | Train AUC: 0.90 \t Val. Loss: 0.324 |  Val. AUC: 0.9049\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_att_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "       lstm_att_model, valid_dataloader, loss_function)#, return_preds=False\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_att_model.state_dict(), lstm_att_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.329 | Test AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "lstm_att_model.load_state_dict(torch.load(lstm_att_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "   lstm_att_model, test_dataloader, loss_function)#, return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot_path = 'output/{}/xgboost/data/train_one_hot.csv'.format(seq_len)\n",
    "x_valid_one_hot_path = 'output/{}/xgboost/data/val_one_hot.csv'.format(seq_len)\n",
    "x_test_one_hot_path = 'output/{}/xgboost/data/test_one_hot.csv'.format(seq_len)\n",
    "\n",
    "x_train_data_path = 'output/{}/xgboost/data/train.csv'.format(seq_len)\n",
    "x_valid_data_path = 'output/{}/xgboost/data/val.csv'.format(seq_len)\n",
    "x_test_data_path = 'output/{}/xgboost/data/test.csv'.format(seq_len)\n",
    "\n",
    "s3_output_data_dir = 's3://merck-paper-bucket/{}/data'.format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_tokens(tokens):\n",
    "#     \"\"\"Get all tokens except <pad> and <unk>\"\"\"\n",
    "#     my_tokens = []\n",
    "#     for key, val in tokens.items():\n",
    "#         if val>=2:\n",
    "#             my_tokens.append(key)\n",
    "#     my_tokens\n",
    "#     return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = lstm_model.vocab._vocab\n",
    "my_tokens = get_valid_tokens(tokens)\n",
    "#my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data(train_data_path, x_train_one_hot_path, x_train_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(valid_data_path, x_valid_one_hot_path, x_valid_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)\n",
    "prepare_data(test_data_path, x_test_one_hot_path, x_test_data_path, seq_len, target_colname, my_tokens, s3_output_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'merck-paper-bucket'\n",
    "DATA_PREFIX = '{}/data'.format(seq_len)\n",
    "MODEL_PREFIX = '{}/xgboost/model'.format(seq_len)\n",
    "label = 'label'\n",
    "\n",
    "output_results_path = 'output/{}/xgboost/train/train_results.csv'.format(seq_len)\n",
    "local_model_dir = 'output/{}/xgboost/models/'.format(seq_len)\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, MODEL_PREFIX)\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = 'xgboost'\n",
    "REPO_VERSION = '1.2-1'\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = 'ml.m5.4xlarge'#'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 1#4 #TODO: Remove\n",
    "MAX_TRAIN_JOBS = 1#20\n",
    "\n",
    "EVALUATION_METRIC = 'auc'\n",
    "OBJECTIVE = 'binary:logistic'\n",
    "OBJECTIVE_METRIC_NAME = 'validation:auc'\n",
    "\n",
    "#Update hyperparameter ranges\n",
    "# HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0, 1),\n",
    "#                         'alpha': ContinuousParameter(0, 2),\n",
    "#                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0.1, 0.5),\n",
    "                       'alpha': ContinuousParameter(0, 2),\n",
    "                       'max_depth': IntegerParameter(1, 10),\n",
    "                       'gamma': ContinuousParameter(0, 5),\n",
    "                       'num_round': IntegerParameter(200, 500),\n",
    "                       'colsample_bylevel': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bynode': ContinuousParameter(0.1, 1.0),\n",
    "                       'colsample_bytree': ContinuousParameter(0.5, 1.0),\n",
    "                       'lambda': ContinuousParameter(0, 1000),\n",
    "                       'max_delta_step': IntegerParameter(0, 10),\n",
    "                       'min_child_weight': ContinuousParameter(0, 120),\n",
    "                       'subsample': ContinuousParameter(0.5, 1.0),\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "start = time.time()\n",
    "print('Training for seq_len={}, label={}...'.format(seq_len, label))\n",
    "#Prepare the input train & validation data path\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/val'.format(BUCKET, DATA_PREFIX), content_type='csv')\n",
    "\n",
    "#Class Imbalance\n",
    "scale_pos_weight = 1.0 # negative/positive\n",
    "\n",
    "data_channels = {'train': s3_input_train, 'validation': s3_input_validation}\n",
    "\n",
    "tuner = train_hpo(hyperparameter_ranges=HYPERPARAMETER_RANGES, \n",
    "                  container=container, \n",
    "                  execution_role=role, \n",
    "                  instance_count=TRAIN_INSTANCE_COUNT, \n",
    "                  instance_type=TRAIN_INSTANCE_TYPE, \n",
    "                  output_path=s3_output_path, \n",
    "                  sagemaker_session=sess, \n",
    "                  eval_metric=EVALUATION_METRIC, \n",
    "                  objective=OBJECTIVE, \n",
    "                  objective_metric_name=OBJECTIVE_METRIC_NAME, \n",
    "                  max_train_jobs=MAX_TRAIN_JOBS, \n",
    "                  max_parallel_jobs=MAX_PARALLEL_JOBS, \n",
    "                  scale_pos_weight=scale_pos_weight, \n",
    "                  data_channels=data_channels)\n",
    "\n",
    "#Get the hyperparameter tuner status at regular interval\n",
    "val_auc, best_model_path = get_tuner_status_and_result_until_completion(tuner, seq_len, label)\n",
    "\n",
    "result = [label, seq_len, val_auc, best_model_path]\n",
    "training_results = [result]\n",
    "\n",
    "print('Success! Total training time={} mins.'.format((time.time()-start)/60.0))\n",
    "#Save the results to file\n",
    "df_results = pd.DataFrame(training_results, columns=['class', 'seq_len', 'val_auc', 'best_model_path'])\n",
    "\n",
    "if not os.path.isdir(os.path.split(output_results_path)[0]):\n",
    "    os.makedirs(os.path.split(output_results_path)[0])\n",
    "\n",
    "df_results.to_csv(output_results_path, index=False)\n",
    "print('ALL SUCCESS!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
