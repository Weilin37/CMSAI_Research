{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training using Toy Dataset\n",
    "\n",
    "Author: Tesfagabir Meharizghi<br>\n",
    "Last Updated: 01/06/2021\n",
    "\n",
    "This notebook is used to train models using the Toy Dataset:\n",
    "- LSTM\n",
    "- LSTM+Attention\n",
    "- XGB\n",
    "\n",
    "Once the models are trained and saved, they will be used to compute SHAP values to see and compare their features importances.\n",
    "Go to [01_ToySimple_shap_jacc.ipynb](01_ToySimple_shap_jacc.ipynb) to work with the SHAP and Jaccard similarities. \n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Make sure that you have already generated the synthetic toy dataset using the [this ipynb](../../data/toy_dataset/Create_toy_dataset.ipynb).\n",
    "\n",
    "Comments:\n",
    "- Add author, date\n",
    "- Describe purpose: train which models, before the other notebook\n",
    "- Outputs and requirements (other files etc)\n",
    "- Steps\n",
    "- Data used for this notebook\n",
    "- You need to black the notebook for readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install botocore==1.12.201\n",
    "\n",
    "#! pip install shap\n",
    "#! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from att_lstm_models import *\n",
    "from lstm_utils import *\n",
    "from xgboost_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 1\n",
    "\n",
    "seq_len = 30\n",
    "\n",
    "train_data_path = \"../../data/toy_dataset/data/{}/train.csv\".format(seq_len)\n",
    "valid_data_path = \"../../data/toy_dataset/data/{}/val.csv\".format(seq_len)\n",
    "test_data_path = \"../../data/toy_dataset/data/{}/test.csv\".format(seq_len)\n",
    "\n",
    "lstm_model_save_path = \"./output/{}/lstm/models/model\".format(seq_len)\n",
    "lstm_results_save_path = \"./output/{}/lstm/results/\".format(seq_len)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 6\n",
    "stop_num = 2\n",
    "\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "nlayers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "target_colname = \"label\"\n",
    "uid_colname = \"patient_id\"\n",
    "# x_inputs = [str(x) for x in range(29, -1, -1)]\n",
    "target_value = \"1\"\n",
    "\n",
    "rev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# LSTM Output Directory\n",
    "for fp in [lstm_model_save_path, lstm_results_save_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f\"New directory created: {fp}\")\n",
    "        os.makedirs(os.path.split(fp)[0])\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocab and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from ../../data/toy_dataset/data/30/train.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/val.csv..\n",
      "Success!\n",
      "Building dataset from ../../data/toy_dataset/data/30/test.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "    train_data_path,\n",
    "    min_freq=min_freq,\n",
    "    uid_colname=\"patient_id\",\n",
    "    target_colname=\"label\",\n",
    "    max_len=seq_len,\n",
    "    target_value=target_value,\n",
    "    vocab=None,\n",
    "    nrows=nrows,\n",
    "    rev=rev,\n",
    ")\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "    valid_data_path,\n",
    "    min_freq=min_freq,\n",
    "    uid_colname=\"patient_id\",\n",
    "    target_colname=\"label\",\n",
    "    max_len=seq_len,\n",
    "    target_value=target_value,\n",
    "    vocab=vocab,\n",
    "    nrows=nrows,\n",
    "    rev=rev,\n",
    ")\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "    test_data_path,\n",
    "    min_freq=min_freq,\n",
    "    uid_colname=\"patient_id\",\n",
    "    target_colname=\"label\",\n",
    "    max_len=seq_len,\n",
    "    target_value=target_value,\n",
    "    vocab=vocab,\n",
    "    nrows=nrows,\n",
    "    rev=rev,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleLSTM Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SimpleLSTM(\n",
    "    embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout\n",
    ")\n",
    "lstm_model = lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.396 | Train AUC: 0.89 \t Val. Loss: 0.333 |  Val. AUC: 0.9026\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "Saved Model, epoch 1\n",
      "Train Loss: 0.354 | Train AUC: 0.90 \t Val. Loss: 0.319 |  Val. AUC: 0.9008\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "Train Loss: 0.345 | Train AUC: 0.90 \t Val. Loss: 0.322 |  Val. AUC: 0.9022\n",
      "Epoch: 04 | Epoch Time: 0m 4s\n",
      "Saved Model, epoch 3\n",
      "Train Loss: 0.347 | Train AUC: 0.90 \t Val. Loss: 0.317 |  Val. AUC: 0.9029\n",
      "Epoch: 05 | Epoch Time: 0m 4s\n",
      "Train Loss: 0.347 | Train AUC: 0.90 \t Val. Loss: 0.320 |  Val. AUC: 0.9005\n",
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "EARLY STOP ------\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "        lstm_model, valid_dataloader, loss_function\n",
    "    )  # , return_preds=False\n",
    "    # )\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_model.state_dict(), lstm_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.320 | Test AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "lstm_model.load_state_dict(torch.load(lstm_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "    lstm_model, test_dataloader, loss_function\n",
    ")  # , return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model_save_path = \"./output/{}/lstm-att/models/model\".format(seq_len)\n",
    "lstm_att_results_save_path = \"./output/{}/lstm-att/results/\".format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Attention Output Directory\n",
    "for fp in [lstm_att_model_save_path, lstm_att_results_save_path]:\n",
    "    if not os.path.exists(os.path.split(fp)[0]):\n",
    "        print(f\"New directory created: {fp}\")\n",
    "        os.makedirs(os.path.split(fp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_model = AttLSTM(\n",
    "    embedding_dim, hidden_dim, vocab, model_device, nlayers=nlayers, dropout=dropout\n",
    ")\n",
    "lstm_att_model = lstm_att_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttLSTM(\n",
       "  (emb_layer): Embedding(32, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (attn_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       "  (context_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(lstm_att_model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "Saved Model, epoch 0\n",
      "Train Loss: 0.379 | Train AUC: 0.89 \t Val. Loss: 0.320 |  Val. AUC: 0.9066\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "Saved Model, epoch 1\n",
      "Train Loss: 0.344 | Train AUC: 0.90 \t Val. Loss: 0.319 |  Val. AUC: 0.9058\n",
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "Train Loss: 0.344 | Train AUC: 0.90 \t Val. Loss: 0.319 |  Val. AUC: 0.9042\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "EARLY STOP ------\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        lstm_att_model, train_dataloader, optimizer, loss_function\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "        lstm_att_model, valid_dataloader, loss_function\n",
    "    )  # , return_preds=False\n",
    "    # )\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(lstm_att_model.state_dict(), lstm_att_model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.322 | Test AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "lstm_att_model.load_state_dict(torch.load(lstm_att_model_save_path))\n",
    "test_loss, test_auc = epoch_val_lstm(\n",
    "    lstm_att_model, test_dataloader, loss_function\n",
    ")  # , return_preds=False\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test AUC: {test_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot_path = \"output/{}/xgboost/data/train_one_hot.csv\".format(seq_len)\n",
    "x_valid_one_hot_path = \"output/{}/xgboost/data/val_one_hot.csv\".format(seq_len)\n",
    "x_test_one_hot_path = \"output/{}/xgboost/data/test_one_hot.csv\".format(seq_len)\n",
    "\n",
    "x_train_data_path = \"output/{}/xgboost/data/train.csv\".format(seq_len)\n",
    "x_valid_data_path = \"output/{}/xgboost/data/val.csv\".format(seq_len)\n",
    "x_test_data_path = \"output/{}/xgboost/data/test.csv\".format(seq_len)\n",
    "\n",
    "s3_output_data_dir = \"s3://merck-paper-bucket/{}/data\".format(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>29</th>\n",
       "      <th>28</th>\n",
       "      <th>27</th>\n",
       "      <th>26</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>23</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2060</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>normal_bmi_U</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>low_salt_diet_U</td>\n",
       "      <td>0</td>\n",
       "      <td>WQT192TZIU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1873</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>normal_bmi_U</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>PCI_U</td>\n",
       "      <td>ACE_inhibitors_U</td>\n",
       "      <td>0</td>\n",
       "      <td>EHO31C693C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>ARR_A</td>\n",
       "      <td>furosemide_H</td>\n",
       "      <td>1</td>\n",
       "      <td>TREAOF36OI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2781</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>low_salt_diet_U</td>\n",
       "      <td>0</td>\n",
       "      <td>TAF54R2M8X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>812</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>cardiac_rehab_U</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>PCI_U</td>\n",
       "      <td>0</td>\n",
       "      <td>4F38DQWGDM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     29     28     27     26          25             24  \\\n",
       "0   2060  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "1   1873  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "2   1402  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "3   2781  <pad>  <pad>  <pad>  <pad>    myopia_N  dental_exam_N   \n",
       "4    812  <pad>  <pad>  <pad>  <pad>  headache_N    hay_fever_N   \n",
       "\n",
       "                 23              22             21  ...               7  \\\n",
       "0             <pad>           <pad>          <pad>  ...    normal_bmi_U   \n",
       "1             <pad>           <pad>          <pad>  ...     hay_fever_N   \n",
       "2             <pad>           <pad>          <pad>  ...  ankle_sprain_N   \n",
       "3  peanut_allergy_N  ankle_sprain_N    cold_sore_N  ...   quad_injury_N   \n",
       "4   cardiac_rehab_U     hay_fever_N  dental_exam_N  ...   dental_exam_N   \n",
       "\n",
       "                6             5               4                  3  \\\n",
       "0     foot_pain_N    ACL_tear_N    cut_finger_N         headache_N   \n",
       "1   dental_exam_N  cut_finger_N    normal_bmi_U         ACL_tear_N   \n",
       "2  ingrown_nail_N  cut_finger_N  ingrown_nail_N        foot_pain_N   \n",
       "3      ACL_tear_N   foot_pain_N   quad_injury_N  annual_physical_N   \n",
       "4   quad_injury_N    ACL_tear_N    cut_finger_N      dental_exam_N   \n",
       "\n",
       "                2            1                 0 label  patient_id  \n",
       "0      ACL_tear_N  cold_sore_N   low_salt_diet_U     0  WQT192TZIU  \n",
       "1      headache_N        PCI_U  ACE_inhibitors_U     0  EHO31C693C  \n",
       "2     cold_sore_N        ARR_A      furosemide_H     1  TREAOF36OI  \n",
       "3  ankle_sprain_N  cold_sore_N   low_salt_diet_U     0  TAF54R2M8X  \n",
       "4     cold_sore_N  foot_pain_N             PCI_U     0  4F38DQWGDM  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_tokens(tokens):\n",
    "#     \"\"\"Get all tokens except <pad> and <unk>\"\"\"\n",
    "#     my_tokens = []\n",
    "#     for key, val in tokens.items():\n",
    "#         if val>=2:\n",
    "#             my_tokens.append(key)\n",
    "#     my_tokens\n",
    "#     return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = lstm_model.vocab._vocab\n",
    "my_tokens = get_valid_tokens(tokens)\n",
    "# my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>29</th>\n",
       "      <th>28</th>\n",
       "      <th>27</th>\n",
       "      <th>26</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>23</th>\n",
       "      <th>22</th>\n",
       "      <th>21</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2060</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>normal_bmi_U</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>low_salt_diet_U</td>\n",
       "      <td>0</td>\n",
       "      <td>WQT192TZIU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1873</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>normal_bmi_U</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>PCI_U</td>\n",
       "      <td>ACE_inhibitors_U</td>\n",
       "      <td>0</td>\n",
       "      <td>EHO31C693C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1402</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>ingrown_nail_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>ARR_A</td>\n",
       "      <td>furosemide_H</td>\n",
       "      <td>1</td>\n",
       "      <td>TREAOF36OI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2781</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>myopia_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>peanut_allergy_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>...</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>annual_physical_N</td>\n",
       "      <td>ankle_sprain_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>low_salt_diet_U</td>\n",
       "      <td>0</td>\n",
       "      <td>TAF54R2M8X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>812</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>headache_N</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>cardiac_rehab_U</td>\n",
       "      <td>hay_fever_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>...</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>quad_injury_N</td>\n",
       "      <td>ACL_tear_N</td>\n",
       "      <td>cut_finger_N</td>\n",
       "      <td>dental_exam_N</td>\n",
       "      <td>cold_sore_N</td>\n",
       "      <td>foot_pain_N</td>\n",
       "      <td>PCI_U</td>\n",
       "      <td>0</td>\n",
       "      <td>4F38DQWGDM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     29     28     27     26          25             24  \\\n",
       "0   2060  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "1   1873  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "2   1402  <pad>  <pad>  <pad>  <pad>       <pad>          <pad>   \n",
       "3   2781  <pad>  <pad>  <pad>  <pad>    myopia_N  dental_exam_N   \n",
       "4    812  <pad>  <pad>  <pad>  <pad>  headache_N    hay_fever_N   \n",
       "\n",
       "                 23              22             21  ...               7  \\\n",
       "0             <pad>           <pad>          <pad>  ...    normal_bmi_U   \n",
       "1             <pad>           <pad>          <pad>  ...     hay_fever_N   \n",
       "2             <pad>           <pad>          <pad>  ...  ankle_sprain_N   \n",
       "3  peanut_allergy_N  ankle_sprain_N    cold_sore_N  ...   quad_injury_N   \n",
       "4   cardiac_rehab_U     hay_fever_N  dental_exam_N  ...   dental_exam_N   \n",
       "\n",
       "                6             5               4                  3  \\\n",
       "0     foot_pain_N    ACL_tear_N    cut_finger_N         headache_N   \n",
       "1   dental_exam_N  cut_finger_N    normal_bmi_U         ACL_tear_N   \n",
       "2  ingrown_nail_N  cut_finger_N  ingrown_nail_N        foot_pain_N   \n",
       "3      ACL_tear_N   foot_pain_N   quad_injury_N  annual_physical_N   \n",
       "4   quad_injury_N    ACL_tear_N    cut_finger_N      dental_exam_N   \n",
       "\n",
       "                2            1                 0 label  patient_id  \n",
       "0      ACL_tear_N  cold_sore_N   low_salt_diet_U     0  WQT192TZIU  \n",
       "1      headache_N        PCI_U  ACE_inhibitors_U     0  EHO31C693C  \n",
       "2     cold_sore_N        ARR_A      furosemide_H     1  TREAOF36OI  \n",
       "3  ankle_sprain_N  cold_sore_N   low_salt_diet_U     0  TAF54R2M8X  \n",
       "4     cold_sore_N  foot_pain_N             PCI_U     0  4F38DQWGDM  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess!\n",
      "Sucess!\n",
      "Sucess!\n"
     ]
    }
   ],
   "source": [
    "prepare_data(\n",
    "    train_data_path,\n",
    "    x_train_one_hot_path,\n",
    "    x_train_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    my_tokens,\n",
    "    s3_output_data_dir,\n",
    ")\n",
    "prepare_data(\n",
    "    valid_data_path,\n",
    "    x_valid_one_hot_path,\n",
    "    x_valid_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    my_tokens,\n",
    "    s3_output_data_dir,\n",
    ")\n",
    "prepare_data(\n",
    "    test_data_path,\n",
    "    x_test_one_hot_path,\n",
    "    x_test_data_path,\n",
    "    seq_len,\n",
    "    target_colname,\n",
    "    my_tokens,\n",
    "    s3_output_data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"merck-paper-bucket\"\n",
    "DATA_PREFIX = \"{}/data\".format(seq_len)\n",
    "MODEL_PREFIX = \"{}/xgboost/model\".format(seq_len)\n",
    "label = \"label\"\n",
    "\n",
    "output_results_path = \"output/{}/xgboost/train/train_results.csv\".format(seq_len)\n",
    "local_model_dir = \"output/{}/xgboost/models/\".format(seq_len)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(BUCKET, MODEL_PREFIX)\n",
    "\n",
    "###Algorithm config\n",
    "ALGORITHM = \"xgboost\"\n",
    "REPO_VERSION = \"1.2-1\"\n",
    "\n",
    "###Hyperparameter tuning config\n",
    "TRAIN_INSTANCE_TYPE = \"ml.m5.4xlarge\"  #'ml.m4.16xlarge'\n",
    "TRAIN_INSTANCE_COUNT = 1\n",
    "MAX_PARALLEL_JOBS = 1  # 4 #TODO: Remove\n",
    "MAX_TRAIN_JOBS = 1  # 20\n",
    "\n",
    "EVALUATION_METRIC = \"auc\"\n",
    "OBJECTIVE = \"binary:logistic\"\n",
    "OBJECTIVE_METRIC_NAME = \"validation:auc\"\n",
    "\n",
    "# Update hyperparameter ranges\n",
    "# HYPERPARAMETER_RANGES = {'eta': ContinuousParameter(0, 1),\n",
    "#                         'alpha': ContinuousParameter(0, 2),\n",
    "#                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "HYPERPARAMETER_RANGES = {\n",
    "    \"eta\": ContinuousParameter(0.1, 0.5),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"gamma\": ContinuousParameter(0, 5),\n",
    "    \"num_round\": IntegerParameter(200, 500),\n",
    "    \"colsample_bylevel\": ContinuousParameter(0.1, 1.0),\n",
    "    \"colsample_bynode\": ContinuousParameter(0.1, 1.0),\n",
    "    \"colsample_bytree\": ContinuousParameter(0.5, 1.0),\n",
    "    \"lambda\": ContinuousParameter(0, 1000),\n",
    "    \"max_delta_step\": IntegerParameter(0, 10),\n",
    "    \"min_child_weight\": ContinuousParameter(0, 120),\n",
    "    \"subsample\": ContinuousParameter(0.5, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for seq_len=30, label=label...\n",
      "....................................................!\n",
      "Total jobs completed: 1\n",
      "Metric: validation:auc\n",
      "Best AUC: 0.9015\n",
      "Success! Total training time=4.404632727305095 mins.\n",
      "ALL SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "### SageMaker Initialization\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = retrieve(ALGORITHM, region, version=REPO_VERSION)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Training for seq_len={}, label={}...\".format(seq_len, label))\n",
    "# Prepare the input train & validation data path\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/train\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/val\".format(BUCKET, DATA_PREFIX), content_type=\"csv\"\n",
    ")\n",
    "\n",
    "# Class Imbalance\n",
    "scale_pos_weight = 1.0  # negative/positive\n",
    "\n",
    "data_channels = {\"train\": s3_input_train, \"validation\": s3_input_validation}\n",
    "\n",
    "tuner = train_hpo(\n",
    "    hyperparameter_ranges=HYPERPARAMETER_RANGES,\n",
    "    container=container,\n",
    "    execution_role=role,\n",
    "    instance_count=TRAIN_INSTANCE_COUNT,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,\n",
    "    output_path=s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    eval_metric=EVALUATION_METRIC,\n",
    "    objective=OBJECTIVE,\n",
    "    objective_metric_name=OBJECTIVE_METRIC_NAME,\n",
    "    max_train_jobs=MAX_TRAIN_JOBS,\n",
    "    max_parallel_jobs=MAX_PARALLEL_JOBS,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    data_channels=data_channels,\n",
    ")\n",
    "\n",
    "# Get the hyperparameter tuner status at regular interval\n",
    "val_auc, best_model_path = get_tuner_status_and_result_until_completion(\n",
    "    tuner, seq_len, label\n",
    ")\n",
    "\n",
    "result = [label, seq_len, val_auc, best_model_path]\n",
    "training_results = [result]\n",
    "\n",
    "print(\"Success! Total training time={} mins.\".format((time.time() - start) / 60.0))\n",
    "# Save the results to file\n",
    "df_results = pd.DataFrame(\n",
    "    training_results, columns=[\"class\", \"seq_len\", \"val_auc\", \"best_model_path\"]\n",
    ")\n",
    "\n",
    "if not os.path.isdir(os.path.split(output_results_path)[0]):\n",
    "    os.makedirs(os.path.split(output_results_path)[0])\n",
    "\n",
    "df_results.to_csv(output_results_path, index=False)\n",
    "print(\"ALL SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
