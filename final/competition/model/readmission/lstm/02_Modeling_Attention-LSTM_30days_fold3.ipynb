{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main notebook to run Attention-LSTM models: Single Fold\n",
    "\n",
    "Author: Lin Lee Cheong <br>\n",
    "Last Updated: 11/23/2020 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import text_classification\n",
    "from torchtext.vocab import Vocab\n",
    "from attn_lstm_model import AttentionRNN\n",
    "from model_utils import (\n",
    "    log,\n",
    "    build_lstm_dataset,\n",
    "    epoch_train_lstm,\n",
    "    epoch_val_lstm,\n",
    "    generate_batch,\n",
    "    count_parameters,\n",
    "    epoch_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 500\n",
    "device_id = None\n",
    "\n",
    "train_data_path = \"../../../data/readmission/fold_3/train/raw_train_data_1000_30days.csv\"\n",
    "valid_data_path = \"../../../data/readmission/fold_3/test/raw_test_data_1000_30days.csv\"\n",
    "model_save_path = './lstm_model_30days/gen_attn_lstm_fold3'\n",
    "results_path = './lstm_results_30days/gen_attn_lstm_results_fold3'\n",
    "\n",
    "batch_size = 2046 \n",
    "N_EPOCHS = 20 \n",
    "\n",
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 30\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.3 # TODO: remove dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "if device_id is None:\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    DEVICE = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in [model_save_path, results_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READ IN TO GENERATE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00: Build token list\n",
      "    6.41: Build counter\n",
      "    6.68: Build vocab\n",
      "    6.68: Build data\n",
      "    7.31: Build pytorch dataset\n",
      "    7.31: Skipped 0 invalid patients\n",
      "    7.31: Skipped 0 dead patients\n",
      "    7.31: Done\n",
      "    7.34: Build token list\n",
      "    8.93: Build data\n",
      "    9.08: Build pytorch dataset\n",
      "    9.08: Skipped 0 invalid patients\n",
      "    9.08: Skipped 0 dead patients\n",
      "    9.08: Done\n",
      "    9.09: vocab length: 5142\n"
     ]
    }
   ],
   "source": [
    "train_dataset = build_lstm_dataset(\n",
    "    datapath=train_data_path, min_freq=500, nrows=nrows, rev=False\n",
    ")\n",
    "\n",
    "valid_dataset = build_lstm_dataset(\n",
    "    datapath=valid_data_path,\n",
    "    min_freq=500,\n",
    "    nrows=nrows,\n",
    "    vocab=train_dataset._vocab,\n",
    "    rev=False,\n",
    ")\n",
    "\n",
    "log('vocab length:', len(train_dataset._vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build LSTM dataset to use a provided vocabulary to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SAVE dataset, vocab\n",
    "# torch.save(train_dataset, './tmp_train_dataset.pt')\n",
    "# torch.save(valid_dataset,'./tmp_valid_datset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generate_batch,\n",
    "    num_workers=8,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generate_batch,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9.09: True\n",
      "    9.09: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "log(torch.cuda.is_available())\n",
    "log(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(train_dataset._vocab) \n",
    "OUTPUT_DIM = len(train_dataset._labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9.14: AttentionRNN(\n",
      "  (embedding): Embedding(5142, 30, padding_idx=0)\n",
      "  (rnn): LSTM(30, 30, dropout=0.3)\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "    9.14: Nb of params: 161731\n"
     ]
    }
   ],
   "source": [
    "model = AttentionRNN(       \n",
    "    INPUT_DIM, \n",
    "    EMBEDDING_DIM, \n",
    "    HIDDEN_DIM, \n",
    "    OUTPUT_DIM, \n",
    "    BIDIRECTIONAL, \n",
    "    DROPOUT,\n",
    "    padding_idx=0,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "model =  model.to(DEVICE)\n",
    "\n",
    "log(model)\n",
    "log(f'Nb of params: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)\n",
    "\n",
    "#    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "#    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.9) #LLC-2/12: less aggresive drops\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9.14: Train\n",
      "Epoch: 01 | Epoch Time: 2m 8s\n",
      "Saved Model, epoch 0\n",
      "   11.29: Train Loss: 0.411 | Train AUC: 0.62 \t Val. Loss: 0.803 |  Val. AUC: 0.6603\n",
      "Epoch: 02 | Epoch Time: 2m 3s\n",
      "Saved Model, epoch 1\n",
      "   13.34: Train Loss: 0.402 | Train AUC: 0.66 \t Val. Loss: 0.802 |  Val. AUC: 0.6652\n",
      "Epoch: 03 | Epoch Time: 2m 8s\n",
      "Saved Model, epoch 2\n",
      "   15.48: Train Loss: 0.400 | Train AUC: 0.67 \t Val. Loss: 0.800 |  Val. AUC: 0.6665\n",
      "Epoch: 04 | Epoch Time: 2m 8s\n",
      "Saved Model, epoch 3\n",
      "   17.63: Train Loss: 0.399 | Train AUC: 0.67 \t Val. Loss: 0.799 |  Val. AUC: 0.6668\n",
      "Epoch: 05 | Epoch Time: 2m 18s\n",
      "Saved Model, epoch 4\n",
      "   19.93: Train Loss: 0.398 | Train AUC: 0.67 \t Val. Loss: 0.798 |  Val. AUC: 0.6677\n",
      "Epoch: 06 | Epoch Time: 2m 18s\n",
      "Saved Model, epoch 5\n",
      "   22.23: Train Loss: 0.398 | Train AUC: 0.67 \t Val. Loss: 0.798 |  Val. AUC: 0.6679\n",
      "Epoch: 07 | Epoch Time: 2m 15s\n",
      "Saved Model, epoch 6\n",
      "   24.50: Train Loss: 0.398 | Train AUC: 0.67 \t Val. Loss: 0.798 |  Val. AUC: 0.6682\n",
      "Epoch: 08 | Epoch Time: 2m 16s\n",
      "Saved Model, epoch 7\n",
      "   26.77: Train Loss: 0.397 | Train AUC: 0.67 \t Val. Loss: 0.798 |  Val. AUC: 0.6681\n",
      "Epoch: 09 | Epoch Time: 2m 16s\n",
      "   29.05: Train Loss: 0.397 | Train AUC: 0.68 \t Val. Loss: 0.798 |  Val. AUC: 0.6691\n",
      "Epoch: 10 | Epoch Time: 2m 18s\n",
      "Saved Model, epoch 9\n",
      "   31.35: Train Loss: 0.397 | Train AUC: 0.68 \t Val. Loss: 0.797 |  Val. AUC: 0.6691\n",
      "Epoch: 11 | Epoch Time: 2m 17s\n",
      "   33.64: Train Loss: 0.396 | Train AUC: 0.68 \t Val. Loss: 0.797 |  Val. AUC: 0.6687\n",
      "Epoch: 12 | Epoch Time: 2m 17s\n",
      "   35.93: Train Loss: 0.396 | Train AUC: 0.68 \t Val. Loss: 0.799 |  Val. AUC: 0.6681\n",
      "Epoch: 13 | Epoch Time: 2m 16s\n",
      "   38.20: Train Loss: 0.396 | Train AUC: 0.68 \t Val. Loss: 0.797 |  Val. AUC: 0.6688\n",
      "Epoch: 14 | Epoch Time: 2m 16s\n",
      "   40.47: Train Loss: 0.396 | Train AUC: 0.68 \t Val. Loss: 0.797 |  Val. AUC: 0.6688\n",
      "Epoch: 15 | Epoch Time: 2m 17s\n",
      "   42.76: Train Loss: 0.396 | Train AUC: 0.68 \t Val. Loss: 0.798 |  Val. AUC: 0.6685\n",
      "Epoch: 16 | Epoch Time: 2m 16s\n",
      "EARLY STOP ------\n"
     ]
    }
   ],
   "source": [
    "log('Train')\n",
    "best_valid_loss = float(\"inf\")\n",
    "valid_worse_loss = 0  # enable early stopping\n",
    "stop_num = 6\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_auc = epoch_train_lstm(\n",
    "        model, train_dataloader, optimizer, criterion\n",
    "    )\n",
    "\n",
    "    valid_loss, valid_auc = epoch_val_lstm(\n",
    "        model, valid_dataloader, criterion, return_preds=False\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Saved Model, epoch {}\".format(epoch))\n",
    "        valid_worse_loss = 0\n",
    "\n",
    "    else:\n",
    "        valid_worse_loss += 1\n",
    "        if valid_worse_loss == stop_num:\n",
    "            print(\"EARLY STOP ------\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "    log(\n",
    "        f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best model on val set: predictions, feature importance etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = ( ids, predictions, labels, attn, events)\n",
    "valid_loss, valid_auc, valid_results = epoch_val_lstm(\n",
    "        model,\n",
    "        valid_dataloader,\n",
    "        criterion,\n",
    "        return_preds=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(valid_results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6691183503346562\n"
     ]
    }
   ],
   "source": [
    "print(valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7966008131800134\n"
     ]
    }
   ],
   "source": [
    "print(valid_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
