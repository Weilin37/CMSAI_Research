{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readmission Modeling: Script 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import boto3\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that GPUs are availble, will use all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''Positional encoding to be used in transfromer model class\n",
    "    '''\n",
    "    def __init__(self, d_model, seq_length, dim, dropout=0.1, max_len=5000):\n",
    "        '''\n",
    "        Args:\n",
    "            d_model: embedding size\n",
    "            seq_length: length of events\n",
    "            dim: num_events possible per day\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # initialize positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.tensor(\n",
    "            [float(i) for i in range(dim)] * seq_length + \n",
    "            [0.0] * (max_len - seq_length * dim), \n",
    "            dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # standard positional encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, seq_length=366, num_events=10, dropout=0.5):  \n",
    "        '''\n",
    "        Initialize a transformer model for hospital readmissions. The model consists of the following:\n",
    "        - Transformer encoder layers\n",
    "        - Single 1D CNN layer\n",
    "        - Final fully connected layer to determine probability of readmissions\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            ntoken: number of tokens in embedding layer (vocabulary size)\n",
    "            ninp: embedding dimension (number of inputs)\n",
    "            \n",
    "            nhead: number of heads in transformers\n",
    "            nhid: number of transformer linear dimensions\n",
    "            \n",
    "            nlayers: number of layers in transfromer\n",
    "            \n",
    "            num_classes: number of classes to predict (in this case, binary)\n",
    "            \n",
    "            seq_length: length of sequence in batched data\n",
    "            num_events: maximum number of events per day\n",
    "            \n",
    "            dropout: strength of regularization\n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        print(\"parameters: embsize:{}, nhead:{}, nhid:{}, nlayers:{}, dropout:{}\".format(\n",
    "            ninp, nhead, nhid, nlayers, dropout))\n",
    "        \n",
    "        # Inputs into transformer: positional encoding and embeddings\n",
    "        self.pos_encoder = PositionalEncoding(ninp, seq_length, num_events, dropout)\n",
    "        self.seq_emb = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        # Transformer layer\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        # CNN & fully connected layers\n",
    "        self.ff = nn.Linear(int(seq_length) * num_events, int(seq_length))\n",
    "        self.fc = nn.Linear(int(seq_length), num_classes)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.Conv1d = nn.Conv1d(ninp, 1, 1, stride=1)\n",
    "        \n",
    "        # record\n",
    "        self.ninp = ninp\n",
    "        self.dropout = dropout\n",
    "        self.num_events= num_events\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # initalize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def seq_embedding(self, seq): \n",
    "        '''Convert the sequence of events into embedding vectors, into single row per observation'''\n",
    "        batch, length_seq, dim = seq.size()\n",
    "        seq = seq.contiguous().view(batch * length_seq, dim)\n",
    "        \n",
    "        seq = self.seq_emb(seq)\n",
    "        \n",
    "        seq = seq.contiguous().view(batch, -1, self.ninp)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''Initialize weights in embedding and fully connected layers'''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.seq_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.ff.bias.data.zero_()\n",
    "        self.ff.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, mask=None): \n",
    "        '''\n",
    "        Forward propagation steps:\n",
    "        - convert events into embedding vectors & positional encoding\n",
    "        - transformer encoder layers\n",
    "        - CNN layer\n",
    "        - final \n",
    "        '''        \n",
    "        # create mask to remove padded entries from calculations for interpretability\n",
    "        if mask is not None:\n",
    "            mask= mask.view(mask.size()[0], -1)\n",
    "            src_mask = (mask == 0)\n",
    "            src_mask = src_mask.view(src_mask.size()[0], -1)\n",
    "            out_mask = mask.float().masked_fill(mask == 0.0, float(-100.0)).masked_fill(mask == 1.0, float(0.0)).view(mask.size()[0], -1)\n",
    "               \n",
    "        src = self.seq_embedding(src).transpose(0,1) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        trans_output = self.transformer_encoder(src, src_key_padding_mask=src_mask).transpose(0, 1).transpose(1,2)\n",
    "        \n",
    "        final_feature_map = self.Conv1d(trans_output).squeeze()\n",
    "        \n",
    "        #if out_mask is not None:\n",
    "            # extract normalized feature importances per prediction\n",
    "        importance_out = self.softmax(final_feature_map + out_mask)\n",
    "        \n",
    "        \n",
    "        output = self.ff(final_feature_map)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # ensure no accidental additional dimensions\n",
    "        if len(output.size()) != 2:\n",
    "            output = output.view(1, 2)\n",
    "        \n",
    "        return output, importance_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and data loader functions to create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildDataset(Dataset):\n",
    "    '''\n",
    "    Read in dataset, if data is already split into train, test, and/or validation sets.\n",
    "    \n",
    "    ProcessData: extract input, labels, mask from an existing Python object (via pickle or otherwise)\n",
    "    ReadNewData: extract data directly from file.\n",
    "    '''\n",
    "    def __init__(self, data_file, seq_length=366, event_length=5, data_list=None, mode='read'): \n",
    "        '''mode: 'read' will process data'''\n",
    "        if mode != 'read' and data_list != None:\n",
    "            self.data, self.label, self.mask = self.ProcessData(data_list, seq_length, event_length)\n",
    "        else:\n",
    "            self.data, self.label, self.mask = self.ReadNewData(data_file, seq_length, event_length)\n",
    "            \n",
    "    def ReadNewData(self, file_dir, seq_length, event_length):\n",
    "        # file needs to be in binary format, pickled from script #1\n",
    "        with open(file_dir, 'rb') as f:\n",
    "            data, label, mask = pickle.load(f)\n",
    "            cut_data = data[:,-seq_length:,:event_length]\n",
    "            cut_mask = mask[:,-seq_length:,:event_length]\n",
    "            label = label.astype(int)\n",
    "        return cut_data, label, cut_mask\n",
    "    \n",
    "    def ProcessData(self, data_list, seq_length, event_length):\n",
    "        input_data, labels, mask = (data_list[0][:,-seq_length:,:event_length], \n",
    "                                    data_list[1], \n",
    "                                    data_list[2][:,-seq_length:,:event_length])\n",
    "        return input_data, labels, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.label[idx]), torch.tensor(self.mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(file_dir, seq_length, event_length):\n",
    "    '''\n",
    "    Function to read is specific number of recent events from specific number of days\n",
    "    \n",
    "    Used to read a large dataset, and script will apply kfold instead.\n",
    "    '''\n",
    "    with open(file_dir, 'rb') as f:\n",
    "        ids, data, label, mask = pickle.load(f)\n",
    "        ids = ids.astype(str)\n",
    "        cut_data = data[:,-seq_length:,:event_length]\n",
    "        cut_mask = mask[:,-seq_length:,:event_length]\n",
    "        label = label.astype(int)\n",
    "        \n",
    "    return ids, cut_data, label, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EopochTrain(model, dataloader, optimizer, criterion, device=DEVICE, metric='acc'):\n",
    "    '''\n",
    "    Model training, called by ModelProcess function\n",
    "    \n",
    "    Note: Does not return prediction importance scores\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # initialize lists to compare predictions & ground truth labels for metric calculation\n",
    "    order_labels = []\n",
    "    prediction_scores = []\n",
    "    \n",
    "    for idx, [seq, labels, mask] in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = labels.squeeze().long()\n",
    "        seq, labels, mask = seq.cuda(), labels.cuda(), mask.cuda()\n",
    "        \n",
    "        predictions, _ = model(seq, mask=mask)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        order_labels.extend(labels.cpu().numpy())\n",
    "        prediction_scores.extend(F.softmax(predictions, dim=-1).detach().cpu().numpy()[:,1])\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # calculate results\n",
    "    if metric == 'acc':\n",
    "        epoch_metric = get_average_accuracy(prediction_scores, order_labels)\n",
    "    elif metric == 'auc':\n",
    "        epoch_metric = roc_auc_score(order_labels, prediction_scores)\n",
    "    \n",
    "    return epoch_loss / len(dataloader), epoch_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EopochVal(model, dataloader, optimizer, criterion, device=DEVICE, metric='auc'):\n",
    "    '''\n",
    "    Evaluate model performance, called by ModelProcess function\n",
    "    \n",
    "    Returns predictions, metrics and importance scores\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # initialize lists to compare predictions & ground truth labels\n",
    "    # and extract importance scores for prediction\n",
    "    order_labels = []\n",
    "    prediction_scores = []\n",
    "    events = []\n",
    "    important_scores = []\n",
    "    \n",
    "    for idx, [seq, labels, mask] in enumerate(dataloader):\n",
    "        # data formatting/loading\n",
    "        labels = labels.squeeze().long()\n",
    "        events.extend(seq.view(seq.size()[0], -1).squeeze().numpy())\n",
    "        seq, labels, mask = seq.cuda(), labels.cuda(), mask.cuda()\n",
    "        \n",
    "        predictions, importance = model(seq, mask=mask)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        important_scores.extend(importance.detach().cpu().numpy())\n",
    "        order_labels.extend(labels.cpu().numpy())\n",
    "        prediction_scores.extend(F.softmax(predictions, dim=-1).detach().cpu().numpy()[:,1])\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if metric == 'acc':\n",
    "        epoch_metric = get_average_accuracy(prediction_scores, order_labels)\n",
    "    elif metric == 'auc':\n",
    "        epoch_metric = roc_auc_score(order_labels, prediction_scores)\n",
    "        \n",
    "    return epoch_loss / len(dataloader), epoch_metric, [order_labels, events, important_scores, prediction_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingProcess(model, epoch, dataloaders:list, device=DEVICE, metric='auc'):\n",
    "    '''\n",
    "    Main function to call for model training.\n",
    "    \n",
    "    Must have at least training & test dataloaders\n",
    "    \n",
    "    Args:\n",
    "        model: instantiation of model to be trained\n",
    "        epoch: total number of epochs to train\n",
    "        dataloaders: at least training & test dataloader, validation dataloader optional\n",
    "        device: cpu or gpu\n",
    "        metric: auc or accuracy (acc)\n",
    "    \n",
    "    Returns:\n",
    "        tuple containing:\n",
    "            (training metrics, val metrics, test_metrics, feature importances evaluated on test data)\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss().cuda() if device != 'cpu' else nn.CrossEntropyLoss()\n",
    "    \n",
    "    lr = 0.001\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "    print(\"device: \", device)\n",
    "    \n",
    "    # initialize lists for documenting training performance\n",
    "    pre_test_metric = 0.0\n",
    "    train_loss=[]\n",
    "    train_metric=[]\n",
    "    val_loss=[]\n",
    "    val_metric=[]\n",
    "    test_loss=[]\n",
    "    test_metric=[]\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(i+1, epoch))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        epoch_train_loss, epoch_train_metric = EopochTrain(\n",
    "            model, dataloaders[0], optimizer, criterion, device=DEVICE, metric=metric)\n",
    "        train_metric.append(epoch_train_metric)\n",
    "        print('epoch_train_loss:',np.mean(epoch_train_loss),\n",
    "              'epoch_train_metric:', np.mean(epoch_train_metric))\n",
    "        \n",
    "        # validation also provided\n",
    "        if len(dataloaders) > 2:\n",
    "            epoch_val_loss, epoch_val_metric, val_results = EopochVal(\n",
    "                model, dataloaders[1], optimizer, criterion, device=DEVICE, metric=metric)\n",
    "            \n",
    "            torch.save(model.module.state_dict(), \n",
    "                       './model_weights/emsize-{}_head-{}_layers-{}_epoch-{}_valauc-{}.pth'.format(\n",
    "                           emsize, nhead, nlayers, str(i), np.round(epoch_val_metric, decimals=3)))\n",
    "            \n",
    "            val_loss.append(epoch_val_loss)\n",
    "            val_metric.append(epoch_val_metric)\n",
    "            \n",
    "            print('epoch_val_loss:',np.mean(epoch_val_loss),\n",
    "                  'epoch_val_metric:', np.mean(epoch_val_metric))\n",
    "            \n",
    "        # predictions on test data \n",
    "        epoch_test_loss, epoch_test_metric, importance_results = EopochVal(\n",
    "            model, dataloaders[-1], optimizer, criterion, device=DEVICE, metric=metric)\n",
    "        test_loss.append(epoch_test_loss)\n",
    "        test_metric.append(epoch_test_metric)\n",
    "        print('epoch_test_loss:',np.mean(epoch_test_loss),\n",
    "              'epoch_test_metric:', np.mean(epoch_test_metric))\n",
    "        \n",
    "        if epoch_test_metric > pre_test_metric:\n",
    "            print(\"updated\")\n",
    "            pre_test_metric = epoch_test_metric\n",
    "            final_importance_results = importance_results\n",
    "            torch.save(model.module.state_dict(), \n",
    "                       './model_weights/emsize-{}_head-{}_layers-{}_epoch-{}_auc-{}.pth'.format(\n",
    "                           emsize, nhead, nlayers, str(i), np.round(epoch_test_metric, decimals=3)))\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    return train_metric, val_metric, test_metric, final_importance_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole data is done\n"
     ]
    }
   ],
   "source": [
    "whole_ids, whole_data, whole_labels, whole_mask = ReadData('data/np_re_last30_non3digit_latest.pkl', \n",
    "                                                           seq_length=30, \n",
    "                                                           event_length=30)\n",
    "  \n",
    "print('whole data is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1562223, 30, 30)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d_0389'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torch.load('data/pos_vocab_last30_non3digit')\n",
    "vocab.itos[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "train_batch_size = 1280\n",
    "test_batch_size = 1280\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 8 # embedding dimension\n",
    "nhid = 16 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 1 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "n_class = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: embsize:8, nhead:1, nhid:16, nlayers:1, dropout:0.1\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "epoch_train_loss: 0.4066599204816008 epoch_train_metric: 0.6407312776102858\n",
      "epoch_test_loss: 0.40719436029998624 epoch_test_metric: 0.663379431053796\n",
      "updated\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "epoch_train_loss: 0.39838327590982403 epoch_train_metric: 0.6702953738264921\n",
      "epoch_test_loss: 0.4057706110331477 epoch_test_metric: 0.6654229397910136\n",
      "updated\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "epoch_train_loss: 0.39644428733793474 epoch_train_metric: 0.6767852635864537\n",
      "epoch_test_loss: 0.4056453086891953 epoch_test_metric: 0.6645787184083483\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "epoch_train_loss: 0.39492144704841176 epoch_train_metric: 0.681439028781885\n",
      "epoch_test_loss: 0.4069599960531507 epoch_test_metric: 0.6631115803219891\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "epoch_train_loss: 0.39377244818906093 epoch_train_metric: 0.6849433009579863\n",
      "epoch_test_loss: 0.40850276740229857 epoch_test_metric: 0.6631676212519053\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "epoch_train_loss: 0.3928761940102494 epoch_train_metric: 0.6877233306662318\n",
      "epoch_test_loss: 0.40692423302300124 epoch_test_metric: 0.6622229619430026\n",
      "parameters: embsize:8, nhead:1, nhid:16, nlayers:1, dropout:0.1\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "epoch_train_loss: 0.409725029215964 epoch_train_metric: 0.6344445207322266\n",
      "epoch_test_loss: 0.4050462669255782 epoch_test_metric: 0.6467965024274388\n",
      "updated\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "epoch_train_loss: 0.3983197358943057 epoch_train_metric: 0.6712372546236531\n",
      "epoch_test_loss: 0.40663484754611035 epoch_test_metric: 0.6515629980082234\n",
      "updated\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "epoch_train_loss: 0.39619504414048873 epoch_train_metric: 0.6783310515182921\n",
      "epoch_test_loss: 0.40357567832178 epoch_test_metric: 0.6518578166764677\n",
      "updated\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "epoch_train_loss: 0.39454362888419153 epoch_train_metric: 0.6835252576441746\n",
      "epoch_test_loss: 0.40580116577294406 epoch_test_metric: 0.6510515782609507\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "epoch_train_loss: 0.3933771444806789 epoch_train_metric: 0.6868655700161815\n",
      "epoch_test_loss: 0.40546093510121717 epoch_test_metric: 0.6506780029496207\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "epoch_train_loss: 0.392187964586787 epoch_train_metric: 0.6902835960778436\n",
      "epoch_test_loss: 0.40465935547741094 epoch_test_metric: 0.6494751729537911\n",
      "parameters: embsize:8, nhead:1, nhid:16, nlayers:1, dropout:0.1\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "epoch_train_loss: 0.4070238654664741 epoch_train_metric: 0.6405346625345771\n",
      "epoch_test_loss: 0.40358904472419194 epoch_test_metric: 0.647340808201109\n",
      "updated\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "epoch_train_loss: 0.39689551331979833 epoch_train_metric: 0.6760466511940493\n",
      "epoch_test_loss: 0.4043479555115408 epoch_test_metric: 0.6495823480278156\n",
      "updated\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "epoch_train_loss: 0.39444164765527817 epoch_train_metric: 0.6836607055678463\n",
      "epoch_test_loss: 0.4034024362661401 epoch_test_metric: 0.6495142442442782\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "epoch_train_loss: 0.3928138914469443 epoch_train_metric: 0.6883922576414075\n",
      "epoch_test_loss: 0.4050826169398366 epoch_test_metric: 0.649113585930248\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "epoch_train_loss: 0.39138979772096283 epoch_train_metric: 0.6922439450420738\n",
      "epoch_test_loss: 0.4054746323094076 epoch_test_metric: 0.6483099539807552\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "epoch_train_loss: 0.3903437724438026 epoch_train_metric: 0.6950670348384539\n",
      "epoch_test_loss: 0.40574820698524006 epoch_test_metric: 0.6472529548480176\n",
      "parameters: embsize:8, nhead:1, nhid:16, nlayers:1, dropout:0.1\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "epoch_train_loss: 0.407206780224764 epoch_train_metric: 0.6377913814818961\n",
      "epoch_test_loss: 0.3975782173020499 epoch_test_metric: 0.6722477918416491\n",
      "updated\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "epoch_train_loss: 0.39912747131887455 epoch_train_metric: 0.667713469553152\n",
      "epoch_test_loss: 0.39666418664309444 epoch_test_metric: 0.6736502027727156\n",
      "updated\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "epoch_train_loss: 0.3968237760184121 epoch_train_metric: 0.6751943005981754\n",
      "epoch_test_loss: 0.3970001296121247 epoch_test_metric: 0.6721563373062649\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "epoch_train_loss: 0.39526435174619895 epoch_train_metric: 0.6801613477957977\n",
      "epoch_test_loss: 0.39900739643038535 epoch_test_metric: 0.669121211845151\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "epoch_train_loss: 0.39409407912281663 epoch_train_metric: 0.6837867188112114\n",
      "epoch_test_loss: 0.3980945607229155 epoch_test_metric: 0.6690796865228148\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "epoch_train_loss: 0.3930342166304222 epoch_train_metric: 0.6868609665218307\n",
      "epoch_test_loss: 0.4005219081834871 epoch_test_metric: 0.666519475301127\n",
      "parameters: embsize:8, nhead:1, nhid:16, nlayers:1, dropout:0.1\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "epoch_train_loss: 0.4109145461170798 epoch_train_metric: 0.6222313582448541\n",
      "epoch_test_loss: 0.3956577931131635 epoch_test_metric: 0.690736965552254\n",
      "updated\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "epoch_train_loss: 0.40072882230352597 epoch_train_metric: 0.6609950977585968\n",
      "epoch_test_loss: 0.39635020804648496 epoch_test_metric: 0.6946907299021222\n",
      "updated\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "epoch_train_loss: 0.3978893739479225 epoch_train_metric: 0.6705659685455071\n",
      "epoch_test_loss: 0.3931980190228443 epoch_test_metric: 0.6936906292221818\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "epoch_train_loss: 0.3962117837114734 epoch_train_metric: 0.6761279654095129\n",
      "epoch_test_loss: 0.3959189501343941 epoch_test_metric: 0.6916584087543409\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "epoch_train_loss: 0.394607826777588 epoch_train_metric: 0.6808204849131569\n",
      "epoch_test_loss: 0.39691773507059835 epoch_test_metric: 0.6900859354403417\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "epoch_train_loss: 0.3934833839947516 epoch_train_metric: 0.6841796359582142\n",
      "epoch_test_loss: 0.39899480823351413 epoch_test_metric: 0.6876135789079385\n",
      "[0.663647208795009, 0.6502370118794154, 0.6485189825387039, 0.6704624509316203, 0.6914127079631965] 0.664855672421589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data for 5 fold cross validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=9999)\n",
    "splits = list(skf.split(whole_data, whole_labels))\n",
    "\n",
    "test_metrics = []\n",
    "results = []\n",
    "for train_index, test_index in splits:\n",
    "    ids = whole_ids[test_index]\n",
    "    torch.save(ids, 'data/discharge_ids_test_' + str(count))\n",
    "    X_train, y_train, mask_train = whole_data[train_index], whole_labels[train_index], whole_mask[train_index]\n",
    "    X_test, y_test, mask_test = whole_data[test_index], whole_labels[test_index], whole_mask[test_index]\n",
    "\n",
    "\n",
    "    # datasets\n",
    "    train_dataset = BuildDataset('', seq_length=30, event_length=30, data_list=[X_train, y_train, mask_train], mode='load')\n",
    "    test_dataset = BuildDataset('', seq_length=30, event_length=30, data_list=[X_test, y_test, mask_test], mode='load')\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # create model\n",
    "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, n_class, seq_length=30, num_events=30, dropout=dropout).to(DEVICE)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    if torch.cuda.device_count()>1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # training\n",
    "    epoch=6\n",
    "    train_metric, val_metric, test_metric, final_importance_results = TrainingProcess(\n",
    "        model, epoch, [train_dataloader, test_dataloader])\n",
    "\n",
    "    # save data\n",
    "    final_importance_results.insert(0, ids[:len(final_importance_results[0])].tolist())\n",
    "    test_metrics.append(np.mean(test_metric))\n",
    "    results.append(final_importance_results)\n",
    "    torch.save(final_importance_results, 'data/explain/final_importance_results_kfold' + str(count))\n",
    "    #break\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# final output result    \n",
    "print(test_metrics, np.mean(test_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual checking results and uploading data back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,  30,  60,  90, 120, 150, 180, 210, 240, 270, 300, 330, 360,\n",
      "       390, 420, 450, 480, 481, 482, 510, 540, 570, 600, 630, 660, 690,\n",
      "       691, 692, 693, 694, 695, 720, 750, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 810, 811, 812, 813, 840, 841, 842, 843, 844,\n",
      "       845, 846, 847, 848, 849, 850, 870, 871, 872]),)\n",
      "1 [0.14690582 0.11748163 0.10799016 0.10100966 0.07234918 0.06466733\n",
      " 0.0642831  0.06264017 0.05430687 0.05067316 0.03408067 0.02019857\n",
      " 0.0186831  0.01547983 0.01340081 0.00656008 0.00517903 0.00486624\n",
      " 0.00425511 0.00425511 0.00378504 0.002817   0.0013862  0.00106932\n",
      " 0.0007394  0.00069025 0.0003895  0.0003895  0.0003895  0.0003895\n",
      " 0.0003895  0.0003895  0.0003895  0.0003895  0.0003895  0.0003895\n",
      " 0.0003895  0.0003895  0.0003895  0.0003895  0.0003895  0.0003895\n",
      " 0.0003895  0.0003895  0.0003895  0.0003895  0.0003895  0.0003895\n",
      " 0.0003895  0.0003895 ] 0.17865418\n"
     ]
    }
   ],
   "source": [
    "#id, label, event, importance, probability\n",
    "import random\n",
    "row = random.randint(0, len(results[-1][2]))\n",
    "row = 0\n",
    "indices = np.nonzero(results[-1][2][row])\n",
    "print(indices)\n",
    "arg_list = np.argsort(results[-1][3][row])[::-1]\n",
    "print(results[-1][1][row], np.array(results[-1][3][row])[arg_list[:50]], results[-1][4][row])\n",
    "for i, v in enumerate(arg_list[:50]):\n",
    "    if v not in indices[0]:\n",
    "        print(i, v, vocab.itos[results[-1][2][row][v]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.51421535\n",
      "0.5024973\n",
      "0.52611023\n",
      "0.5035754\n",
      "0.5024492\n",
      "0.5106884\n",
      "0.5038529\n",
      "0.50124544\n",
      "46799\n"
     ]
    }
   ],
   "source": [
    "#np.argwhere(np.array(results[-1][4]) > 0.5)\n",
    "print(len(results))\n",
    "for i in results[-1][4]:\n",
    "    if i > 0.5:\n",
    "        print(i)\n",
    "print(np.count_nonzero(results[-1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double check to make sure all the lengths agree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312300 312300 312300 312300 312300\n"
     ]
    }
   ],
   "source": [
    "print(len(final_importance_results[0]), len(final_importance_results[1]), len(final_importance_results[2]), len(final_importance_results[3]), len(final_importance_results[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The user-provided path data/pos_vocab_late30_non3digit does not exist.\n"
     ]
    }
   ],
   "source": [
    "#torch.save(final_importance_results, 'data/explainability')\n",
    "! aws s3 cp Readmit_Transformer_whole.ipynb s3://cmsai-mrk-amzn/xianzeng/ lcheong/explain_transformer.ipynb .\n",
    "!aws s3 cp data/pos_vocab_late30_non3digit s3://cmsai-mrk-amzn/xianzeng/\n",
    "!aws s3 cp data/explain/final_importance_results_whole s3://cmsai-mrk-amzn/xianzeng/explain/explainablity #--recursive\n",
    "! aws s3 cp data/discharge_ids_test_0 s3://cmsai-mrk-amzn/xianzeng/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auroc</th>\n",
       "      <th>avgpr</th>\n",
       "      <th>precis</th>\n",
       "      <th>precis 0.5%</th>\n",
       "      <th>precis 1%</th>\n",
       "      <th>precis 2%</th>\n",
       "      <th>precis 5%</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall 0.5%</th>\n",
       "      <th>recall 1%</th>\n",
       "      <th>recall 2%</th>\n",
       "      <th>recall 5%</th>\n",
       "      <th>calibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.649582</td>\n",
       "      <td>0.23218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      auroc    avgpr  precis  precis 0.5%  precis 1%  precis 2%  precis 5%  \\\n",
       "0  0.649582  0.23218     0.0          0.0        0.0        0.0        0.0   \n",
       "\n",
       "   recall  recall 0.5%  recall 1%  recall 2%  recall 5%  calibration  \n",
       "0     0.0          0.0        0.0        0.0        0.0     0.018027  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, scores = np.array(results[-3][1]), np.array(results[-3][4])\n",
    "df = metrics.compute_metrics(labels, scores, 0.5, target_names=None, risk_list=[0.5, 1, 2, 5])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./Readmit_Transformer_whole_commented.ipynb to s3://cmsai-mrk-amzn/xianzeng/Readmit_Transformer_whole_commented.ipynb\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp Readmit_Transformer_whole_commented.ipynb s3://cmsai-mrk-amzn/xianzeng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
