{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import boto3\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify that GPUs are availble, will use all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, num_events=500, seq_length=120, dropout=0.5):\n",
    "        '''\n",
    "        Initialize a transformer model for adverse events. The model consists of the following:\n",
    "        - Transformer encoder layers\n",
    "        - Single 1D CNN layer\n",
    "        - Final fully connected layer to determine probability of readmissions\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            ntoken: number of tokens in embedding layer (vocabulary size)\n",
    "            ninp: embedding dimension (number of inputs)\n",
    "            \n",
    "            nhead: number of heads in transformers\n",
    "            nhid: number of transformer linear dimensions\n",
    "            \n",
    "            nlayers: number of layers in transfromer\n",
    "            \n",
    "            num_classes: number of classes to predict (in this case, binary)\n",
    "            \n",
    "            seq_length: dimension of linear layer output\n",
    "            num_events: maximum number of events per patient\n",
    "            \n",
    "            dropout: strength of regularization\n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        print(\"parameters: embsize:{}, nhead:{}, nhid:{}, nlayers:{}, dropout:{}\".format(ninp, nhead, nhid, nlayers, dropout))\n",
    "        \n",
    "        # Inputs into transformer: mask for padding and embeddings\n",
    "        self.src_mask = None\n",
    "        self.event_emb = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        # Transformer layer\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "         # CNN & fully connected layers\n",
    "        \n",
    "        self.ff = nn.Linear(int(num_events), int(seq_length))\n",
    "        self.fc = nn.Linear(int(seq_length), num_classes)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.Conv1d = nn.Conv1d(ninp, 1, 1, stride=1)\n",
    "        \n",
    "        # record\n",
    "        self.ninp = ninp\n",
    "        self.dropout = dropout\n",
    "        self.num_events= num_events\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # initalize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "\n",
    "    def init_weights(self):\n",
    "        '''Initialize weights in embedding and fully connected layers'''\n",
    "        initrange = 0.1\n",
    "        self.event_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.ff.bias.data.zero_()\n",
    "        self.ff.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        \n",
    "    def forward(self, src, mask=None, pos=None): \n",
    "        '''\n",
    "        Forward propagation steps:\n",
    "        - convert events into embedding vectors & positional encoding\n",
    "        - transformer encoder layers\n",
    "        - CNN layer\n",
    "        - final \n",
    "        Notes:\n",
    "        no position encoding here, no obvious clues for sequential or order found \n",
    "        '''        \n",
    "        if mask is not None:\n",
    "            #src_key_padding_mask needs boolean mask\n",
    "            src_mask = (mask == 0)\n",
    "\n",
    "        src = self.event_emb(src).transpose(0,1) * math.sqrt(self.ninp)\n",
    "        \n",
    "        trans_output = self.transformer_encoder(src, src_key_padding_mask=src_mask).transpose(0, 1).transpose(1,2)\n",
    "        final_feature_map = self.Conv1d(trans_output).squeeze()\n",
    "        \n",
    "        out_mask = mask.float().masked_fill(mask == 0.0, float(-100.0)).masked_fill(mask == 1.0, float(0.0)).view(mask.size()[0], -1)\n",
    "        # extract normalized feature importances per prediction\n",
    "        importance_out = self.softmax(final_feature_map+out_mask)\n",
    "        \n",
    "        output = self.ff(final_feature_map)\n",
    "        output = self.nonlinear(output)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, importance_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "no position encoding here, but save it for the future in case\n",
    "'''\n",
    "from torch.autograd import Variable\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, events, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.pe = torch.zeros(max_len, d_model)\n",
    "        #self.pe = torch.zeros(pos.size()[0], , self.d_model)\n",
    "        ''' \n",
    "        #position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        position = torch.tensor([float(i) for i in range(dim)] * seq_length + [0.0] * (max_len - seq_length * dim), dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)'''\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "\n",
    "        #position1 = torch.tensor([float(i) for i in range(30)] * 30 + [0.0] * (5000 - 30 * 30), dtype=torch.float).unsqueeze(1)\n",
    "        pe = torch.zeros(x.size()).cuda()\n",
    "        pe = pe.transpose(0, 1)\n",
    "        position = pos.unsqueeze(2)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model)).cuda()\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term).cuda()\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term).cuda()\n",
    "        pe = pe.transpose(0, 1)\n",
    "        #print(position1.size(), position.size(), div_term.size())\n",
    "        #self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        #self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        #self.pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #self.register_buffer('pe', pe)\n",
    "        x = x + pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and data loader functions to create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildDataset(Dataset):\n",
    "    '''\n",
    "    Read in dataset, if data is already split into train, test, and/or validation sets.\n",
    "    \n",
    "    ProcessData: extract input, labels, mask from an existing Python object (via pickle or otherwise)\n",
    "    '''\n",
    "    def __init__(self, data_file, event_length=500, data_list=None, mode='read'): \n",
    "        if mode != 'read' and data_list != None:\n",
    "            self.data, self.label, self.mask = self.ProcessData(data_list, event_length)\n",
    "            \n",
    "    def ProcessData(self, data_list, event_length):\n",
    "        input_data, labels, mask = data_list[0][:,-event_length:], data_list[1], data_list[2][:,-event_length:]\n",
    "        return input_data, labels, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.label[idx]), torch.tensor(self.mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the saved pickle file, the order is [patietn_id, input_data, labels, mask_for_padding, position(no used here)]\n",
    "def ReadData(file_dir, event_length):\n",
    "    with open(file_dir, 'rb') as f:\n",
    "        ids, data, label, mask = pickle.load(f)\n",
    "        ids = ids.astype(str)\n",
    "        cut_data = data[:,-event_length:]\n",
    "        cut_mask = mask[:,-event_length:]\n",
    "        #cut_pos = pos[:,-event_length:]\n",
    "        label = label.astype(int)\n",
    "    return ids, cut_data, label, cut_mask#, cut_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EopochTrain(model, dataloader, optimizer, criterion, device=\"cuda\", metric='acc'):\n",
    "    '''\n",
    "    Model training, called by ModelProcess function\n",
    "    \n",
    "    Note: Does not return prediction importance scores\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    order_labels = None\n",
    "    prediction_scores = None\n",
    "    for idx, [seq, labels, mask] in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = labels.squeeze().float()\n",
    "        seq, labels, mask = seq.cuda(), labels.cuda(), mask.cuda()\n",
    "        \n",
    "        predictions, _ = model(seq, mask=mask)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if order_labels is None:\n",
    "            order_labels = labels.cpu().numpy()\n",
    "            prediction_scores = torch.sigmoid(predictions).detach().cpu().numpy()\n",
    "        else:\n",
    "            order_labels = np.concatenate((order_labels, labels.cpu().numpy()))\n",
    "            prediction_scores =np.concatenate((prediction_scores, torch.sigmoid(predictions).detach().cpu().numpy()))\n",
    "    \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if metric == 'acc':\n",
    "        epoch_metric = get_average_accuracy(prediction_scores, order_labels)\n",
    "    elif metric == 'auc':\n",
    "        epoch_metric = roc_auc_score(order_labels, prediction_scores, average='weighted')\n",
    "    return epoch_loss / len(dataloader), epoch_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EopochVal(model, dataloader, optimizer, criterion, device=\"cuda\", metric='auc'):\n",
    "    '''\n",
    "    Evaluate model performance, called by ModelProcess function\n",
    "    \n",
    "    Returns predictions, metrics and importance scores\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_metric = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    order_labels = None\n",
    "    prediction_scores = None\n",
    "    events = None\n",
    "    important_scores = None\n",
    "    \n",
    "    for idx, [seq, labels, mask] in enumerate(dataloader):\n",
    "        labels = labels.squeeze().float()\n",
    "        seq, labels, mask = seq.cuda(), labels.cuda(), mask.cuda()\n",
    "        predictions, importance = model(seq, mask=mask)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        if order_labels is None:\n",
    "            order_labels = labels.cpu().numpy()\n",
    "            prediction_scores = torch.sigmoid(predictions).detach().cpu().numpy()\n",
    "            events = seq.cpu().numpy()\n",
    "            important_scores = importance.detach().cpu().numpy()\n",
    "        else:\n",
    "            order_labels = np.concatenate((order_labels, labels.cpu().numpy()))\n",
    "            prediction_scores =np.concatenate((prediction_scores, torch.sigmoid(predictions).detach().cpu().numpy()))\n",
    "            events = np.concatenate((events, seq.cpu().numpy()))\n",
    "            important_scores = np.concatenate((important_scores, importance.detach().cpu().numpy()))\n",
    "            \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if metric == 'acc':\n",
    "        epoch_metric = get_average_accuracy(prediction_scores, order_labels)\n",
    "    elif metric == 'auc':\n",
    "        epoch_metric = roc_auc_score(order_labels, prediction_scores)\n",
    "        \n",
    "    return epoch_loss / len(dataloader), epoch_metric, [order_labels, events, important_scores, prediction_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingProcess(model, epoch, criterion, dataloaders:list, month):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device=\"cuda\"\n",
    "    else:\n",
    "        device=\"cpu\"\n",
    "    \n",
    "    print(\"device: \", device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "    pre_val_metric = 0.0\n",
    "    final_test_metric = 0.0\n",
    "    train_loss=[]\n",
    "    train_metrics=[]\n",
    "    #val_loss=[]\n",
    "    #test_loss=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(i+1, epoch))\n",
    "        print('-' * 10)\n",
    "        epoch_train_loss, epoch_train_metric = EopochTrain(model, dataloaders[0], optimizer, criterion, device=\"cuda\", metric='auc')\n",
    "\n",
    "        train_metrics.append(epoch_train_metric)\n",
    "        print('epoch_train_loss:',np.mean(epoch_train_loss), 'epoch_train_metric:', np.mean(epoch_train_metric))\n",
    "        \n",
    "        epoch_val_loss, epoch_val_metric, val_results = EopochVal(model, dataloaders[1], optimizer, criterion, device=\"cuda\", metric='auc')\n",
    "        #val_loss.append(epoch_val_loss)\n",
    "        print('epoch_val_loss:',np.mean(epoch_val_loss), 'epoch_val_metric:', np.mean(epoch_val_metric))\n",
    "        \n",
    "        epoch_test_loss, epoch_test_metric, importance_results = EopochVal(model, dataloaders[-1], optimizer, criterion, device=\"cuda\", metric='auc')\n",
    "        #test_loss.append(epoch_test_loss)\n",
    "        print('epoch_test_loss:',np.mean(epoch_test_loss), 'epoch_test_metric:', np.mean(epoch_test_metric))\n",
    "        \n",
    "        #save model based on val results\n",
    "        #epoch_val_metric = epoch_train_metric\n",
    "        if epoch_val_metric > pre_val_metric:\n",
    "            \n",
    "            print(\"updated\")\n",
    "            pre_val_metric = epoch_val_metric\n",
    "            final_test_metric = epoch_test_metric\n",
    "            final_importance_results = importance_results\n",
    "            \n",
    "            torch.save(model.module.state_dict(), './vocab120_model_weights_month-{}_emsize-{}_head-{}_layers-{}_valauc-{}.pth'.format(month, emsize, nhead, nlayers, np.round(epoch_val_metric, decimals=3)))\n",
    "        #scheduler.step()\n",
    "        \n",
    "    return train_metrics, pre_val_metric, final_test_metric, final_importance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h_99284'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load vocab\n",
    "vocab = torch.load('data/ae_pos_vocab_last120_whole_non3')\n",
    "vocab.itos[52]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: embsize:16, nhead:1, nhid:32, nlayers:1, dropout:0.5\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.08049642046292622 epoch_train_metric: 0.5050934634299812\n",
      "epoch_val_loss: 0.04875092721525945 epoch_val_metric: 0.6793677014205476\n",
      "epoch_test_loss: 0.04875092721525945 epoch_test_metric: 0.6793677014205476\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.046796630483631994 epoch_train_metric: 0.6218608865031598\n",
      "epoch_val_loss: 0.04630436706265398 epoch_val_metric: 0.6983356397889089\n",
      "epoch_test_loss: 0.04630436706265398 epoch_test_metric: 0.6983356397889089\n",
      "updated\n",
      "[0.6983356397889089] 0.6983356397889089\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04704082406619016 epoch_train_metric: 0.6715232380852993\n",
      "epoch_val_loss: 0.04735779590612533 epoch_val_metric: 0.700783466856492\n",
      "epoch_test_loss: 0.04735779590612533 epoch_test_metric: 0.700783466856492\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04686643493672212 epoch_train_metric: 0.679135511600596\n",
      "epoch_val_loss: 0.04720619198519225 epoch_val_metric: 0.7019522083482965\n",
      "epoch_test_loss: 0.04720619198519225 epoch_test_metric: 0.7019522083482965\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965] 0.7001439240686027\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04671551987528801 epoch_train_metric: 0.6802612210540262\n",
      "epoch_val_loss: 0.046928039267659184 epoch_val_metric: 0.7032236733780004\n",
      "epoch_test_loss: 0.046928039267659184 epoch_test_metric: 0.7032236733780004\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04664213769137859 epoch_train_metric: 0.6832888533751523\n",
      "epoch_val_loss: 0.04705809708684683 epoch_val_metric: 0.7044382141381441\n",
      "epoch_test_loss: 0.04705809708684683 epoch_test_metric: 0.7044382141381441\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441] 0.7015753540917832\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.043194367794446575 epoch_train_metric: 0.6851044574971706\n",
      "epoch_val_loss: 0.043283099833044035 epoch_val_metric: 0.7062001229523577\n",
      "epoch_test_loss: 0.043283099833044035 epoch_test_metric: 0.7062001229523577\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.043131631265566184 epoch_train_metric: 0.6874454048315748\n",
      "epoch_val_loss: 0.04325289111041907 epoch_val_metric: 0.7071207775861122\n",
      "epoch_test_loss: 0.04325289111041907 epoch_test_metric: 0.7071207775861122\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122] 0.7029617099653654\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.043880025641276285 epoch_train_metric: 0.6872189098954864\n",
      "epoch_val_loss: 0.04406264907895373 epoch_val_metric: 0.7059299233164046\n",
      "epoch_test_loss: 0.04406264907895373 epoch_test_metric: 0.7059299233164046\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04381202014449697 epoch_train_metric: 0.6896770397730303\n",
      "epoch_val_loss: 0.04407287071136615 epoch_val_metric: 0.7063116464968212\n",
      "epoch_test_loss: 0.04407287071136615 epoch_test_metric: 0.7063116464968212\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212] 0.7036316972716566\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.0434983564796401 epoch_train_metric: 0.6883520251009855\n",
      "epoch_val_loss: 0.04369169556736654 epoch_val_metric: 0.7069274237983361\n",
      "epoch_test_loss: 0.04369169556736654 epoch_test_metric: 0.7069274237983361\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04339382092596269 epoch_train_metric: 0.6919434348754908\n",
      "epoch_val_loss: 0.04377385821449114 epoch_val_metric: 0.7066208282072631\n",
      "epoch_test_loss: 0.04377385821449114 epoch_test_metric: 0.7066208282072631\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361] 0.7041809850261033\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.03954966040780243 epoch_train_metric: 0.6942311813536409\n",
      "epoch_val_loss: 0.03964873272202258 epoch_val_metric: 0.7104879357118427\n",
      "epoch_test_loss: 0.03964873272202258 epoch_test_metric: 0.7104879357118427\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.03948578433793725 epoch_train_metric: 0.6968868033072226\n",
      "epoch_val_loss: 0.039682085707205016 epoch_val_metric: 0.7107492304565859\n",
      "epoch_test_loss: 0.039682085707205016 epoch_test_metric: 0.7107492304565859\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361, 0.7107492304565859] 0.7051193058018865\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.041876699589192865 epoch_train_metric: 0.6925806293296731\n",
      "epoch_val_loss: 0.04204828957095742 epoch_val_metric: 0.708514304239159\n",
      "epoch_test_loss: 0.04204828957095742 epoch_test_metric: 0.708514304239159\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04185962911695242 epoch_train_metric: 0.6934245412367083\n",
      "epoch_val_loss: 0.042077747220173475 epoch_val_metric: 0.7087510537304634\n",
      "epoch_test_loss: 0.042077747220173475 epoch_test_metric: 0.7087510537304634\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361, 0.7107492304565859, 0.7087510537304634] 0.7055732742929586\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04155153497343972 epoch_train_metric: 0.6919764650345741\n",
      "epoch_val_loss: 0.04162708286728178 epoch_val_metric: 0.7098929752010891\n",
      "epoch_test_loss: 0.04162708286728178 epoch_test_metric: 0.7098929752010891\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04148273251595951 epoch_train_metric: 0.6948130212936486\n",
      "epoch_val_loss: 0.041586371484611716 epoch_val_metric: 0.7104587415024174\n",
      "epoch_test_loss: 0.041586371484611716 epoch_test_metric: 0.7104587415024174\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361, 0.7107492304565859, 0.7087510537304634, 0.7104587415024174] 0.7061161039828985\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04216600000858307 epoch_train_metric: 0.6915484485509446\n",
      "epoch_val_loss: 0.042389388913288716 epoch_val_metric: 0.7083004868209732\n",
      "epoch_test_loss: 0.042389388913288716 epoch_test_metric: 0.7083004868209732\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04210557341575623 epoch_train_metric: 0.693681360149227\n",
      "epoch_val_loss: 0.04242023883387446 epoch_val_metric: 0.7086746354095619\n",
      "epoch_test_loss: 0.04242023883387446 epoch_test_metric: 0.7086746354095619\n",
      "updated\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361, 0.7107492304565859, 0.7087510537304634, 0.7104587415024174, 0.7086746354095619] 0.7063719571255648\n",
      "whole data is done\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04134845810959924 epoch_train_metric: 0.6933257828906092\n",
      "epoch_val_loss: 0.041670264403845345 epoch_val_metric: 0.7107752654262895\n",
      "epoch_test_loss: 0.041670264403845345 epoch_test_metric: 0.7107752654262895\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04128517249142224 epoch_train_metric: 0.6962212076894809\n",
      "epoch_val_loss: 0.041649347021824344 epoch_val_metric: 0.7106215809128458\n",
      "epoch_test_loss: 0.041649347021824344 epoch_test_metric: 0.7106215809128458\n",
      "[0.6983356397889089, 0.7019522083482965, 0.7044382141381441, 0.7071207775861122, 0.7063116464968212, 0.7069274237983361, 0.7107492304565859, 0.7087510537304634, 0.7104587415024174, 0.7086746354095619, 0.7107752654262895] 0.7067722578801762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_batch_size = 10000\n",
    "test_batch_size = 10000\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 16 # embedding dimension\n",
    "nhid = 32 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 1 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "n_class = 20\n",
    "\n",
    "test_metrics = []\n",
    "results = []\n",
    "\n",
    "\n",
    "#criterion to use\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=None).cuda()\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, n_class, num_events=500, seq_length=120, dropout=0.5).to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "if torch.cuda.device_count()>1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "# Training month by month\n",
    "for i in range(1, 12):\n",
    "    if i < 10:\n",
    "        m = '0' + str(i)\n",
    "    else:\n",
    "        m = str(i)\n",
    "    whole_ids, whole_data, whole_labels, whole_mask = ReadData('data/ae_process/np_ae_last120_vocab120_non3digit_month' + m + '.pkl', event_length=500)\n",
    "    \n",
    "    train_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    val_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    test_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    \n",
    "    train_index = np.where(np.in1d(whole_ids, train_ids))[0]\n",
    "    val_index = np.where(np.in1d(whole_ids, val_ids))[0]\n",
    "    test_index = np.where(np.in1d(whole_ids, test_ids))[0]\n",
    "    print('whole data is done')\n",
    "    \n",
    "\n",
    "    X_train, y_train, mask_train  = whole_data[train_index], whole_labels[train_index], whole_mask[train_index]\n",
    "    X_val, y_val, mask_val = whole_data[val_index], whole_labels[val_index], whole_mask[val_index]\n",
    "    X_test, y_test, mask_test = whole_data[test_index], whole_labels[test_index], whole_mask[test_index]\n",
    "\n",
    "    train_dataset = BuildDataset('', event_length=500, data_list=[X_train, y_train, mask_train], mode='load')\n",
    "    val_dataset = BuildDataset('', event_length=500, data_list=[X_val, y_val, mask_val], mode='load')\n",
    "    test_dataset = BuildDataset('', event_length=500, data_list=[X_test, y_test, mask_test], mode='load')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "    epoch=2\n",
    "    train_metrics, val_metric, test_metric, final_importance_results = TrainingProcess(model, epoch, criterion, [train_dataloader, val_dataloader, test_dataloader], m)\n",
    "\n",
    "    final_importance_results.insert(0, whole_ids[test_index])\n",
    "    test_metrics.append(test_metric)\n",
    "    results.append(final_importance_results)\n",
    "\n",
    "    torch.save(final_importance_results, 'data/explain/final_importance_results_vocab120_month' + m)\n",
    "\n",
    "    print(test_metrics, np.mean(test_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: embsize:16, nhead:1, nhid:32, nlayers:1, dropout:0.5\n",
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.04575593143917797 epoch_train_metric: 0.6520541037977222\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.040199166994374785 epoch_train_metric: 0.6932024366433348\n",
      "updated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_batch_size = 2000\n",
    "test_batch_size = 2000\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 16 # embedding dimension\n",
    "nhid = 32 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 1 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "n_class = 20\n",
    "\n",
    "test_metrics = []\n",
    "results = []\n",
    "\n",
    "\n",
    "#criterion to use\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=None).cuda()\n",
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, n_class, num_events=500, seq_length=120, dropout=0.5).to(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "# if torch.cuda.device_count()>1:\n",
    "#     model = nn.DataParallel(model)\n",
    "    \n",
    "# Training month by month\n",
    "for i in [12]:\n",
    "    if i < 10:\n",
    "        m = '0' + str(i)\n",
    "    else:\n",
    "        m = str(i)\n",
    "    whole_ids, whole_data, whole_labels, whole_mask = ReadData('data/ae_process/np_ae_last120_vocab120_non3digit_month' + m + '.pkl', event_length=500)\n",
    "    \n",
    "    #train_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    #val_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    #test_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_2011' + m + '01.csv').to_numpy()\n",
    "    \n",
    "#     train_index = np.where(np.in1d(whole_ids, train_ids))[0]\n",
    "#     val_index = np.where(np.in1d(whole_ids, val_ids))[0]\n",
    "#     test_index = np.where(np.in1d(whole_ids, test_ids))[0]\n",
    "#     print('whole data is done')\n",
    "    \n",
    "    X_train, y_train, mask_train  = whole_data, whole_labels, whole_mask\n",
    "    #X_train, y_train, mask_train  = whole_data[train_index], whole_labels[train_index], whole_mask[train_index]\n",
    "#     X_val, y_val, mask_val = whole_data[val_index], whole_labels[val_index], whole_mask[val_index]\n",
    "#     X_test, y_test, mask_test = whole_data[test_index], whole_labels[test_index], whole_mask[test_index]\n",
    "\n",
    "    train_dataset = BuildDataset('', event_length=500, data_list=[X_train, y_train, mask_train], mode='load')\n",
    "#     val_dataset = BuildDataset('', event_length=500, data_list=[X_val, y_val, mask_val], mode='load')\n",
    "#     test_dataset = BuildDataset('', event_length=500, data_list=[X_test, y_test, mask_test], mode='load')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=12, drop_last=True)\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=12, drop_last=True)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=12, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "    epoch=2\n",
    "    train_metrics, val_metric, test_metric, final_importance_results = TrainingProcess(model, epoch, criterion, [train_dataloader], m)\n",
    "\n",
    "   # final_importance_results.insert(0, whole_ids[test_index])\n",
    "    #test_metrics.append(test_metric)\n",
    "    #results.append(final_importance_results)\n",
    "\n",
    "    torch.save(final_importance_results, 'data/explain/final_importance_results_vocab120_month' + m)\n",
    "\n",
    "   # print(test_metrics, np.mean(test_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "----------\n",
      "Epoch 1/2\n",
      "----------\n",
      "epoch_train_loss: 0.040213182440769706 epoch_train_metric: 0.6922007540921401\n",
      "updated\n",
      "----------\n",
      "Epoch 2/2\n",
      "----------\n",
      "epoch_train_loss: 0.04014175395066002 epoch_train_metric: 0.6948991662836519\n",
      "updated\n"
     ]
    }
   ],
   "source": [
    "epoch=2\n",
    "train_metrics, val_metric, test_metric, final_importance_results = TrainingProcess(model, epoch, criterion, [train_dataloader], m)\n",
    "\n",
    "# final_importance_results.insert(0, whole_ids[test_index])\n",
    "#test_metrics.append(test_metric)\n",
    "#results.append(final_importance_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_test_loss: 0.04407568524646408 epoch_test_metric: 0.7277205219804261\n"
     ]
    }
   ],
   "source": [
    "# whole_ids, whole_data, whole_labels, whole_mask, whole_pos = ReadData('data/ae_process/np_ae_last120_non3digit_month01.pkl', event_length=500)\n",
    "    \n",
    "# #train_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_20110101.csv').to_numpy()\n",
    "# #val_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_20110101.csv').to_numpy()\n",
    "# test_ids = pd.read_csv('data/splits_new/splits_per_month/train_ae_patients_365_20110101.csv').to_numpy()\n",
    "\n",
    "# #train_index = np.where(np.in1d(whole_ids, train_ids))[0]\n",
    "# #val_index = np.where(np.in1d(whole_ids, val_ids))[0]\n",
    "# test_index = np.where(np.in1d(whole_ids, test_ids))[0]\n",
    "# print('whole data is done')\n",
    "\n",
    "\n",
    "# #X_train, y_train, mask_train  = whole_data[train_index], whole_labels[train_index], whole_mask[train_index]\n",
    "# #X_val, y_val, mask_val = whole_data[val_index], whole_labels[val_index], whole_mask[val_index]\n",
    "# X_test, y_test, mask_test = whole_data[test_index], whole_labels[test_index], whole_mask[test_index]\n",
    "\n",
    "# #train_dataset = BuildDataset('', event_length=500, data_list=[X_train, y_train, mask_train], mode='load')\n",
    "# #val_dataset = BuildDataset('', event_length=500, data_list=[X_val, y_val, mask_val], mode='load')\n",
    "# test_dataset = BuildDataset('', event_length=500, data_list=[X_test, y_test, mask_test], mode='load')\n",
    "\n",
    "# #train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=6, drop_last=True)\n",
    "# #val_dataloader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=6, drop_last=True)\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "epoch_test_loss, epoch_test_metric, importance_results = EopochVal(model, test_dataloader, optimizer, criterion, device=\"cuda\", metric='auc')\n",
    "        #test_loss.append(epoch_test_loss)\n",
    "print('epoch_test_loss:',np.mean(epoch_test_loss), 'epoch_test_metric:', np.mean(epoch_test_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/ae_pos_vocab_last90_whole_non3 to s3://cmsai-mrk-amzn/ae_tranf_submission_2/ae_pos_vocab_last90_whole_non3\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/ae_pos_vocab_last90_whole_non3 s3://cmsai-mrk-amzn/ae_tranf_submission_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/explain/final_importance_results_month11 to s3://cmsai-mrk-amzn/xianzeng/explain/final_importance_results_month11\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/explain/final_importance_results_month11 s3://cmsai-mrk-amzn/xianzeng/explain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, scores = np.array(results[-1][1]), np.array(results[-1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256500, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metrics.compute_metrics(labels, scores, 0.2, target_names=None, risk_list=[0.5, 1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auroc</th>\n",
       "      <th>avgpr</th>\n",
       "      <th>precis</th>\n",
       "      <th>precis 0.5%</th>\n",
       "      <th>precis 1%</th>\n",
       "      <th>precis 2%</th>\n",
       "      <th>precis 5%</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall 0.5%</th>\n",
       "      <th>recall 1%</th>\n",
       "      <th>recall 2%</th>\n",
       "      <th>recall 5%</th>\n",
       "      <th>calibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705216</td>\n",
       "      <td>0.056086</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.113796</td>\n",
       "      <td>0.113840</td>\n",
       "      <td>0.102534</td>\n",
       "      <td>0.081871</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775094</td>\n",
       "      <td>0.078213</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.168355</td>\n",
       "      <td>0.148538</td>\n",
       "      <td>0.115595</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.874975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.140016</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.311769</td>\n",
       "      <td>0.240936</td>\n",
       "      <td>0.165887</td>\n",
       "      <td>0.102924</td>\n",
       "      <td>0.855250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.229075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.674638</td>\n",
       "      <td>0.044499</td>\n",
       "      <td>0.020187</td>\n",
       "      <td>0.098207</td>\n",
       "      <td>0.090448</td>\n",
       "      <td>0.078752</td>\n",
       "      <td>0.063938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.656049</td>\n",
       "      <td>0.031679</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>0.063133</td>\n",
       "      <td>0.064327</td>\n",
       "      <td>0.053801</td>\n",
       "      <td>0.044366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.442189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.648587</td>\n",
       "      <td>0.034082</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.098987</td>\n",
       "      <td>0.078363</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.045458</td>\n",
       "      <td>0.999670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.394406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.779144</td>\n",
       "      <td>0.060517</td>\n",
       "      <td>0.015906</td>\n",
       "      <td>0.152767</td>\n",
       "      <td>0.123587</td>\n",
       "      <td>0.096101</td>\n",
       "      <td>0.068538</td>\n",
       "      <td>0.865598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.298706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.672521</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.043648</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.038596</td>\n",
       "      <td>0.038285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.737316</td>\n",
       "      <td>0.055144</td>\n",
       "      <td>0.037138</td>\n",
       "      <td>0.076383</td>\n",
       "      <td>0.075244</td>\n",
       "      <td>0.065107</td>\n",
       "      <td>0.059805</td>\n",
       "      <td>0.937537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774836</td>\n",
       "      <td>0.029763</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>0.054191</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.923852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.272082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.777856</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.061574</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.033528</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.881232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.256928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.722740</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>0.046765</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>0.999045</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.310426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.782317</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.045986</td>\n",
       "      <td>0.037427</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.871750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.643275</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.441428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.685633</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.603314</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.017927</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.014581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.472049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.789250</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.021044</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.593874</td>\n",
       "      <td>0.014802</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.019486</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.739352</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.028839</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>0.012242</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.889552</td>\n",
       "      <td>0.025591</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.033515</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.008733</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.162864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       auroc     avgpr    precis  precis 0.5%  precis 1%  precis 2%  \\\n",
       "0   0.705216  0.056086  0.020033     0.113796   0.113840   0.102534   \n",
       "1   0.775094  0.078213  0.029508     0.168355   0.148538   0.115595   \n",
       "2   0.830647  0.140016  0.025907     0.311769   0.240936   0.165887   \n",
       "3   0.674638  0.044499  0.020187     0.098207   0.090448   0.078752   \n",
       "4   0.656049  0.031679  0.015552     0.063133   0.064327   0.053801   \n",
       "5   0.648587  0.034082  0.011806     0.098987   0.078363   0.062963   \n",
       "6   0.779144  0.060517  0.015906     0.152767   0.123587   0.096101   \n",
       "7   0.672521  0.028438  0.014051     0.043648   0.042105   0.038596   \n",
       "8   0.737316  0.055144  0.037138     0.076383   0.075244   0.065107   \n",
       "9   0.774836  0.029763  0.004474     0.070148   0.054191   0.039376   \n",
       "10  0.777856  0.023545  0.004000     0.061574   0.045614   0.033528   \n",
       "11  0.722740  0.016650  0.004089     0.046765   0.038207   0.030799   \n",
       "12  0.782317  0.018427  0.003926     0.045986   0.037427   0.025146   \n",
       "13  0.643275  0.009247  0.005318     0.017147   0.015205   0.014620   \n",
       "14  0.685633  0.010948  0.002830     0.031177   0.021832   0.017544   \n",
       "15  0.603314  0.009528  0.005692     0.017927   0.016764   0.016179   \n",
       "16  0.789250  0.013916  0.001557     0.021044   0.018324   0.012671   \n",
       "17  0.593874  0.014802  0.010667     0.019486   0.018713   0.018324   \n",
       "18  0.739352  0.010728  0.002009     0.028839   0.022612   0.016764   \n",
       "19  0.889552  0.025591  0.002687     0.033515   0.022222   0.016179   \n",
       "\n",
       "    precis 5%    recall  recall 0.5%  recall 1%  recall 2%  recall 5%  \\\n",
       "0    0.081871  0.999805          1.0        1.0        1.0        1.0   \n",
       "1    0.100663  0.874975          1.0        1.0        1.0        1.0   \n",
       "2    0.102924  0.855250          1.0        1.0        1.0        1.0   \n",
       "3    0.063938  1.000000          1.0        1.0        1.0        1.0   \n",
       "4    0.044366  1.000000          1.0        1.0        1.0        1.0   \n",
       "5    0.045458  0.999670          1.0        1.0        1.0        1.0   \n",
       "6    0.068538  0.865598          1.0        1.0        1.0        1.0   \n",
       "7    0.038285  1.000000          1.0        1.0        1.0        1.0   \n",
       "8    0.059805  0.937537          1.0        1.0        1.0        1.0   \n",
       "9    0.025341  0.923852          1.0        1.0        1.0        1.0   \n",
       "10   0.019339  0.881232          1.0        1.0        1.0        1.0   \n",
       "11   0.020585  0.999045          1.0        1.0        1.0        1.0   \n",
       "12   0.015127  0.871750          1.0        1.0        1.0        1.0   \n",
       "13   0.011463  1.000000          1.0        1.0        1.0        1.0   \n",
       "14   0.012710  1.000000          1.0        1.0        1.0        1.0   \n",
       "15   0.014581  1.000000          1.0        1.0        1.0        1.0   \n",
       "16   0.007641  0.867470          1.0        1.0        1.0        1.0   \n",
       "17   0.017778  0.999635          1.0        1.0        1.0        1.0   \n",
       "18   0.012242  0.992000          1.0        1.0        1.0        1.0   \n",
       "19   0.008733  0.860697          1.0        1.0        1.0        1.0   \n",
       "\n",
       "    calibration  \n",
       "0      0.371622  \n",
       "1      0.285384  \n",
       "2      0.229075  \n",
       "3      0.405784  \n",
       "4      0.442189  \n",
       "5      0.394406  \n",
       "6      0.298706  \n",
       "7      0.400970  \n",
       "8      0.311747  \n",
       "9      0.272082  \n",
       "10     0.256928  \n",
       "11     0.310426  \n",
       "12     0.230854  \n",
       "13     0.441428  \n",
       "14     0.317200  \n",
       "15     0.472049  \n",
       "16     0.249003  \n",
       "17     0.499299  \n",
       "18     0.293855  \n",
       "19     0.162864  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auroc</th>\n",
       "      <th>avgpr</th>\n",
       "      <th>precis</th>\n",
       "      <th>precis 0.5%</th>\n",
       "      <th>precis 1%</th>\n",
       "      <th>precis 2%</th>\n",
       "      <th>precis 5%</th>\n",
       "      <th>recall</th>\n",
       "      <th>recall 0.5%</th>\n",
       "      <th>recall 1%</th>\n",
       "      <th>recall 2%</th>\n",
       "      <th>recall 5%</th>\n",
       "      <th>calibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678534</td>\n",
       "      <td>0.26095</td>\n",
       "      <td>0.392842</td>\n",
       "      <td>0.431222</td>\n",
       "      <td>0.40704</td>\n",
       "      <td>0.392842</td>\n",
       "      <td>0.392842</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.352699</td>\n",
       "      <td>0.027826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      auroc    avgpr    precis  precis 0.5%  precis 1%  precis 2%  precis 5%  \\\n",
       "0  0.678534  0.26095  0.392842     0.431222    0.40704   0.392842   0.392842   \n",
       "\n",
       "     recall  recall 0.5%  recall 1%  recall 2%  recall 5%  calibration  \n",
       "0  0.040343          1.0        1.0   0.778878   0.352699     0.027826  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
