{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main notebook to run Attention-LSTM models: Single Fold\n",
    "\n",
    "Author: Lin Lee Cheong <br>\n",
    "Last Updated: 11/23/2020 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pytorch LSTM/Embeddings Layer:\n",
    "\n",
    "Issue: RuntimeError: only Tensors of floating point dtype can require gradients\n",
    "\n",
    "Resource: https://gitmemory.com/issue/slundberg/shap/595/494334554\n",
    "https://github.com/slundberg/shap/issues/595\n",
    "\n",
    "* RNN/Embedding Layer not supported?\n",
    "    \n",
    "* nn.embedding changed to one-hot-encoding * V (matrix multiplication)\n",
    "\n",
    "    https://discuss.pytorch.org/t/backwards-through-embedding/30364/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import text_classification\n",
    "from torchtext.vocab import Vocab\n",
    "from attn_lstm_model import AttentionRNN\n",
    "from model_utils import (\n",
    "    log,\n",
    "    build_lstm_dataset_v2,\n",
    "    epoch_train_lstm,\n",
    "    epoch_val_lstm,\n",
    "    generate_batch,\n",
    "    count_parameters,\n",
    "    epoch_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1e9\n",
    "min_freq = 500\n",
    "device_id = None\n",
    "\n",
    "train_data_x_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/final_allvocab_x_train.npy'\n",
    "train_data_y_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/final_allvocab_y_train.npy'\n",
    "val_data_x_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/final_allvocab_x_val.npy'\n",
    "val_data_y_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/final_allvocab_y_val.npy'\n",
    "test_data_x_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/cms_test_x.npy'\n",
    "test_data_y_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/cms_test_y.npy'\n",
    "vocab_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/anonymize/AE/Data/Preprocessed/Anonymized/ae_all_vocab_last180_whole'\n",
    "model_save_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/final-global/re/1000/training/lstm/lstm_model_30days/gen_attn_lstm'\n",
    "results_path = '/home/ec2-user/SageMaker/CMSAI/modeling/tes/data/final-global/re/1000/training/lstm/lstm_results_30days/gen_attn_lstm_results'\n",
    "\n",
    "target_names = ['d_5990', 'd_78605', 'd_486', 'd_78650', 'd_78079', 'd_78900', 'd_78609', 'd_7862', 'd_1101', 'd_78701', \n",
    "                'd_5789', 'd_78791', 'd_6826', 'd_78659', 'd_78907', 'd_7840', 'd_28860', 'd_4660', 'd_6829', 'd_00845']\n",
    "\n",
    "batch_size = 2048\n",
    "N_EPOCHS = 20\n",
    "\n",
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 30\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.0#0.3 # TODO: remove dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "if device_id is None:\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    DEVICE = torch.device(f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in [model_save_path, results_path]:\n",
    "    if not os.path.isdir(os.path.split(fp)[0]):\n",
    "        print(f'New directory created: {fp}')\n",
    "        os.makedirs(os.path.split(fp)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READ IN TO GENERATE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00: Load data and vocab\n",
      "    0.49: Build data\n",
      "    0.49: Build pytorch dataset\n",
      "    0.49: Done\n",
      "    0.50: Load data and vocab\n",
      "    0.54: Build data\n",
      "    0.55: Build pytorch dataset\n",
      "    0.55: Done\n",
      "    0.55: vocab length: 31534\n"
     ]
    }
   ],
   "source": [
    "train_dataset = build_lstm_dataset_v2(train_data_x_path, \n",
    "                                      train_data_y_path,\n",
    "                                      vocab_path, \n",
    "                                      target_names)\n",
    "\n",
    "valid_dataset = build_lstm_dataset_v2(val_data_x_path, \n",
    "                                      val_data_y_path,\n",
    "                                      vocab_path, \n",
    "                                      target_names)\n",
    "\n",
    "# test_dataset = build_lstm_dataset_v2(test_data_x_path, \n",
    "#                                       test_data_y_path,\n",
    "#                                       vocab_path, \n",
    "#                                       target_names)\n",
    "\n",
    "log('vocab length:', len(train_dataset._vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build LSTM dataset to use a provided vocabulary to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SAVE dataset, vocab\n",
    "# torch.save(train_dataset, './tmp_train_dataset.pt')\n",
    "# torch.save(valid_dataset,'./tmp_valid_datset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generate_batch,\n",
    "    num_workers=8,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generate_batch,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=generate_batch,\n",
    "#     num_workers=8\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.55: True\n",
      "    0.55: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "log(torch.cuda.is_available())\n",
    "log(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(train_dataset._vocab) \n",
    "OUTPUT_DIM = len(train_dataset._labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.61: AttentionRNN(\n",
      "  (embedding): Embedding(31534, 30, padding_idx=0)\n",
      "  (rnn): LSTM(30, 30)\n",
      "  (fc): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "    0.61: Nb of params: 954080\n"
     ]
    }
   ],
   "source": [
    "model = AttentionRNN(       \n",
    "    INPUT_DIM, \n",
    "    EMBEDDING_DIM, \n",
    "    HIDDEN_DIM, \n",
    "    OUTPUT_DIM, \n",
    "    BIDIRECTIONAL, \n",
    "    DROPOUT,\n",
    "    padding_idx=0,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "model =  model.to(DEVICE)\n",
    "\n",
    "log(model)\n",
    "log(f'Nb of params: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.9)\n",
    "\n",
    "#    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "#    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.9) #LLC-2/12: less aggresive drops\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log('Train')\n",
    "# best_valid_loss = float(\"inf\")\n",
    "# valid_worse_loss = 0  # enable early stopping\n",
    "# stop_num = 6\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#     print('Training Epoch {}...'.format(epoch+1))\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     train_loss, train_auc = epoch_train_lstm(\n",
    "#         model, train_dataloader, optimizer, criterion\n",
    "#     )\n",
    "\n",
    "#     valid_loss, valid_auc = epoch_val_lstm(\n",
    "#         model, valid_dataloader, criterion, return_preds=False\n",
    "#     )\n",
    "\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "#     print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), model_save_path)\n",
    "#         print(\"Saved Model, epoch {}\".format(epoch))\n",
    "#         valid_worse_loss = 0\n",
    "\n",
    "#     else:\n",
    "#         valid_worse_loss += 1\n",
    "#         if valid_worse_loss == stop_num:\n",
    "#             print(\"EARLY STOP ------\")\n",
    "#             break\n",
    "\n",
    "#     scheduler.step()\n",
    "#     log(\n",
    "#         f\"Train Loss: {train_loss:.3f} | Train AUC: {train_auc:.2f} \\t Val. Loss: {valid_loss:.3f} |  Val. AUC: {valid_auc:.4f}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best model on val set: predictions, feature importance etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = ( ids, predictions, labels, attn, events)\n",
    "# valid_loss, valid_auc, valid_results = epoch_val_lstm(\n",
    "#         model,\n",
    "#         valid_dataloader,\n",
    "#         criterion,\n",
    "#         return_preds=True\n",
    "#     )\n",
    "# valid_loss, valid_auc, valid_results = epoch_val_lstm(\n",
    "#         model,\n",
    "#         valid_dataloader,\n",
    "#         criterion,\n",
    "#         return_preds=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(valid_results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Attn_weights\n",
    "# print(valid_results[3][0].shape)\n",
    "# valid_results[3][0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked = np.argsort(valid_results[3][0])\n",
    "# ranked = ranked[::-1]\n",
    "# ranked[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, torch.Size([2048, 500]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0]), batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4, 12, 21, 24])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 20])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    Needed to prevent RNN+Attention backpropagating between batches.\n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, text, text_lengths, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_lengths, labels = (\n",
    "    text.to(model.device),\n",
    "    text_lengths.to(model.device),\n",
    "    labels.to(model.device),\n",
    ")\n",
    "hidden = model.init_hidden(text.shape[0])\n",
    "hidden = repackage_hidden(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 500])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048, 30]), torch.Size([1, 2048, 30]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape, hidden[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified: ~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py\n",
    "#Added a line with: data = data[0]\n",
    "#data = [text, text_lengths, hidden]\n",
    "data = text#, text_lengths]#, torch.Tensor(text.shape[0])]\n",
    "explainer = shap.DeepExplainer(model, data)\n",
    "#shap.GradientExplainer(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.explainer.multi_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py(150)shap_values()\n",
      "-> if not self.multi_input:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'repackage_hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-38e48c2c15c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# check if we have multiple inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Expected a single tensor model input!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/shap/explainers/_deep/deep_pytorch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m#TODO: Modified here...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m#X = [x.detach().to(self.device) for x in X]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mranked_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repackage_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         for idx, (ids, text, text_lengths, labels) in enumerate(dataloader):\n",
    "\n",
    "#             text, text_lengths, labels = (\n",
    "#                 text.to(model.device),\n",
    "#                 text_lengths,\n",
    "#                 labels.to(model.device),\n",
    "#             )\n",
    "\n",
    "#             hidden = model.init_hidden(text.shape[0])\n",
    "#             hidden = repackage_hidden(hidden)\n",
    "\n",
    "#             predictions, hidden, attn_weights = model(\n",
    "#                 text, text_lengths, hidden, explain=True\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, *batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap_values_pos = explainer.shap_values(X_pos)\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap_values_pos = explainer.shap_values(X_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Visualizing the shap value of the first 10 predictions of the positive examples\n",
    "columns = df_data_pos.columns.tolist()\n",
    "patient_id_idx = columns.index('patient_id')\n",
    "for j in range(10):\n",
    "    patient_id = df_data_pos.iloc[j, patient_id_idx]\n",
    "    vis_path = os.path.join(shap_dir, 'shap_{}.png'.format(patient_id))\n",
    "    shap.force_plot(explainer.expected_value, shap_values_pos[j,:], X_pos.iloc[j,:], matplotlib=True, show=False)\n",
    "    plt.savefig(vis_path, bbox_inches='tight')\n",
    "    plt.close(\"all\")\n",
    "\n",
    "shap_path = os.path.join(FINAL_RESULTS_DIR, 'shap_{}.csv'.format(SPLIT))\n",
    "df_shap = pd.DataFrame(shap_values_pos, columns=feature_names)\n",
    "df_shap['patient_id'] = df_data_pos['patient_id'].tolist()\n",
    "columns = ['patient_id'] + feature_names\n",
    "df_shap = df_shap[columns]\n",
    "df_shap.to_csv(shap_path, index=False)\n",
    "# # visualize the training set predictions\n",
    "# #shap.force_plot(explainer.expected_value, shap_values, X) ## Out-of-memory Error\n",
    "\n",
    "# # create a dependence plot to show the effect of a single feature across the whole dataset\n",
    "# vis_path = os.path.join(shap_dir, target+'_per_feature_shap.png')\n",
    "# shap.dependence_plot(feature_names[0], shap_values, X, show=False)\n",
    "# plt.savefig(vis_path, bbox_inches='tight')\n",
    "# plt.close(\"all\")\n",
    "\n",
    "# # summarize the effects of all the features\n",
    "# shap.summary_plot(shap_values, X, show=False)\n",
    "# vis_path = os.path.join(shap_dir, target+'_all_features_shap.png')\n",
    "# plt.savefig(vis_path, bbox_inches='tight')\n",
    "# plt.close(\"all\")\n",
    "\n",
    "#Compute the mean absolute value of the SHAP values for each feature to get a standard bar plot\n",
    "print('Computing feature importance')\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "vis_path = os.path.join(FINAL_RESULTS_DIR, 'feature_importance.png')\n",
    "plt.savefig(vis_path, bbox_inches='tight')\n",
    "plt.close(\"all\")\n",
    "\n",
    "# print('Shap Values and Visualizations Successfully Saved to {}!'.format(shap_dir))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since shuffle=True, this is a random sample of test data\n",
    "batch = next(iter(test_loader))\n",
    "images, _ = batch\n",
    "\n",
    "background = images[:100]\n",
    "test_images = images[100:103]\n",
    "\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(test_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
