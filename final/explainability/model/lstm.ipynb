{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP, LRP & Attention Scores with Attention-Based LSTM\n",
    "**Authors:**<br>\n",
    "Lin Lee Cheong <br>\n",
    "Tesfagabir Meharizghi<br>\n",
    "\n",
    "**Date:**<br>\n",
    "03/31/2021\n",
    "\n",
    "This notebook trains attention-based LSTM model based on the given data (synthetic/real). In addition, it also computes the SHAP, LRP and attention weights for each trained model. Once the importance scores are computed, it also calculates intersection similarity where it gives a score of similarity between the predicted features and ground truth.\n",
    "\n",
    "Finally, it visualizes the different feature importance scores and per-observation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO]\n",
    "* ~~Change lstm similarity function with the new version~~\n",
    "* ~~Update XGB notebook with the new similarity function~~\n",
    "* ~~Add early stopping to lstm~~\n",
    "* Add Self-Attention to LSTM (Need to Debug errors)\n",
    "* ~~Add scripts to compute all SHAP~~\n",
    "* ~~Cleanup functions and remove those not needed~~\n",
    "* ~~Add comments to the functions~~\n",
    "* ~~Update text/description of the ipynbs~~\n",
    "* Add contents of the CMSAI competitions:\n",
    "    - Python modules and ipynbs\n",
    "    - Update descriptions if needed\n",
    "    - Anonymize texts if needed\n",
    "* ~~Add script to compute shap for whole data (LSTM)~~\n",
    "    - ADD link to the appropriate positions\n",
    "* Black Code\n",
    "* Push to Merck and MLSL Github\n",
    "* Update the main README.md\n",
    "* Update the Confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nb_black\n",
    "#! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.parse import urlparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "import deep_id_pytorch\n",
    "\n",
    "from lstm_models import *\n",
    "from lstm_lrp_models import *\n",
    "from lstm_att_models import *\n",
    "from lstm_self_att_models import *\n",
    "from lstm_utils import *\n",
    "import imp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_SYNTHETIC = True  # If dataset is synthetic/real\n",
    "# MODEL_NAME = 'lstm'\n",
    "MODEL_NAME = \"lstm-self-att\"\n",
    "USE_SELF_ATTENTION = True\n",
    "\n",
    "NROWS = 1e9\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "SEQ_LEN = 30\n",
    "DATA_TYPE = \"event\"  # event/sequence\n",
    "\n",
    "TRAIN_DATA_PATH = f\"../data/synthetic/sample_dataset/{DATA_TYPE}/{SEQ_LEN}/train.csv\"\n",
    "VALID_DATA_PATH = f\"../data/synthetic/sample_dataset/{DATA_TYPE}/{SEQ_LEN}/val.csv\"\n",
    "TEST_DATA_PATH = f\"../data/synthetic/sample_dataset/{DATA_TYPE}/{SEQ_LEN}/test.csv\"\n",
    "VOCAB_PATH = f\"../data/synthetic/sample_dataset/{DATA_TYPE}/{SEQ_LEN}/vocab.pkl\"\n",
    "\n",
    "MODEL_SAVE_PATH_PATTERN = (\n",
    "    f\"./output/synthetic/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/model_weights/model_{'{}'}.pkl\"\n",
    ")\n",
    "IMP_SAVE_DIR_PATTERN = f\"./output/synthetic/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/importances/{'{}'}_imp_{'{}'}.pkl\"  # Feature importance values path for a given dataset split\n",
    "\n",
    "OUTPUT_RESULTS_PATH = (\n",
    "    f\"./output/synthetic/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/train_results/results.csv\"\n",
    ")\n",
    "PARAMS_PATH = (\n",
    "    f\"./output/synthetic/{DATA_TYPE}/{SEQ_LEN}/{MODEL_NAME}/train_results/model_params.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Test Patients for Visualization (from Test Set)\n",
    "SELECTED_PATIENTS = [\"VHCSIEPRI8\", \"MIP0F9ZOUM\", \"79A4PHXNE6\", \"CW780GL0AX\"]\n",
    "\n",
    "N_EPOCHS = 5\n",
    "EARLY_STOPPING_NUM = 1  # For early stopping\n",
    "\n",
    "TARGET_COLNAME = \"label\"\n",
    "UID_COLNAME = \"patient_id\"\n",
    "TARGET_VALUE = \"1\"\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "MODEL_PARAMS = {\n",
    "    # Dataset/vocab related\n",
    "    \"min_freq\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    # Model related parameters\n",
    "    \"embedding_dim\": 8,\n",
    "    \"hidden_dim\": 16,\n",
    "    \"nlayers\": 2,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"linear_bias\": False,\n",
    "    \"init_type\": \"zero\",  # zero/learned\n",
    "    \"learning_rate\": 0.01,  # 0.01,\n",
    "    \"scheduler_step\": 3,\n",
    "    \"clip\": False,\n",
    "    \"rev\": False,\n",
    "    # SHAP-related parameters\n",
    "    \"n_background\": 300,  # Number of background examples\n",
    "    \"background_negative_only\": True,  # If negative examples are used as background\n",
    "    \"background_positive_only\": False,\n",
    "    \"num_eval_val\": 64,  # 100,  # 200, #number of validation examples for shap\n",
    "    \"num_eval_test\": 64,  # 100,  # 200,#number of test examples for shap\n",
    "    \"test_positive_only\": False,\n",
    "    \"is_test_random\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Created: ./output/event/30/lstm-self-att/model_weights\n",
      "Directory Created: ./output/event/30/lstm-self-att/importances\n",
      "Directory Created: ./output/event/30/lstm-self-att/train_results\n"
     ]
    }
   ],
   "source": [
    "# Create output directories if needed\n",
    "model_dir = os.path.dirname(MODEL_SAVE_PATH_PATTERN)\n",
    "imp_dir = os.path.dirname(IMP_SAVE_DIR_PATTERN)\n",
    "output_dir = os.path.dirname(OUTPUT_RESULTS_PATH)\n",
    "if TRAIN_MODEL:\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "    if os.path.exists(imp_dir):\n",
    "        shutil.rmtree(imp_dir)\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(model_dir)\n",
    "    os.makedirs(imp_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory Created: {model_dir}\")\n",
    "    print(f\"Directory Created: {imp_dir}\")\n",
    "    print(f\"Directory Created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "model_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Selected Patients for SHAP visualization\n",
    "selected_patients_path = os.path.join(output_dir, \"selected_test_patients.csv\")\n",
    "if TRAIN_MODEL:\n",
    "    # create the test examples set\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    test_df[test_df.patient_id.isin(SELECTED_PATIENTS)].to_csv(\n",
    "        selected_patients_path, index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define and create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset from /home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/open_source/sample_dataset/event/30/train.csv..\n",
      "Success!\n",
      "Building dataset from /home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/open_source/sample_dataset/event/30/val.csv..\n",
      "Success!\n",
      "Building dataset from /home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/open_source/sample_dataset/event/30/test.csv..\n",
      "Success!\n",
      "Building dataset from ./output/event/30/lstm-self-att/train_results/selected_test_patients.csv..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dataset, vocab = build_lstm_dataset(\n",
    "    TRAIN_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=None,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    "    is_synthetic=IS_SYNTHETIC,\n",
    ")\n",
    "\n",
    "valid_dataset, _ = build_lstm_dataset(\n",
    "    VALID_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    "    is_synthetic=IS_SYNTHETIC,\n",
    ")\n",
    "\n",
    "test_dataset, _ = build_lstm_dataset(\n",
    "    TEST_DATA_PATH,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    "    is_synthetic=IS_SYNTHETIC,\n",
    ")\n",
    "\n",
    "example_dataset, _ = build_lstm_dataset(\n",
    "    selected_patients_path,\n",
    "    min_freq=MODEL_PARAMS[\"min_freq\"],\n",
    "    uid_colname=UID_COLNAME,\n",
    "    target_colname=TARGET_COLNAME,\n",
    "    max_len=SEQ_LEN,\n",
    "    target_value=TARGET_VALUE,\n",
    "    vocab=vocab,\n",
    "    nrows=NROWS,\n",
    "    rev=MODEL_PARAMS[\"rev\"],\n",
    "    is_synthetic=IS_SYNTHETIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab._vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset, batch_size=MODEL_PARAMS[\"batch_size\"], shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Vocabulary Saved to /home/ec2-user/SageMaker/CMSAI_Research/data/toy_dataset/open_source/sample_dataset/event/30/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # Save vocab\n",
    "    with open(VOCAB_PATH, \"wb\") as fp:\n",
    "        pickle.dump(vocab, fp)\n",
    "    print(f\"Dataset Vocabulary Saved to {VOCAB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and load LRP LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    # LOAD Model Parameters\n",
    "    with open(PARAMS_PATH, \"r\") as fp:\n",
    "        MODEL_PARAMS = json.load(fp)\n",
    "\n",
    "if USE_SELF_ATTENTION:\n",
    "    lstm_model = SelfAttentionLSTM(\n",
    "        MODEL_PARAMS[\"embedding_dim\"],\n",
    "        MODEL_PARAMS[\"hidden_dim\"],\n",
    "        vocab,\n",
    "        model_device,\n",
    "        bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "        nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "        dropout=MODEL_PARAMS[\"dropout\"],\n",
    "        init_type=MODEL_PARAMS[\"init_type\"],\n",
    "        linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    "    )\n",
    "else:\n",
    "    lstm_model = AttNoHtLSTM(\n",
    "        MODEL_PARAMS[\"embedding_dim\"],\n",
    "        MODEL_PARAMS[\"hidden_dim\"],\n",
    "        vocab,\n",
    "        model_device,\n",
    "        bidi=MODEL_PARAMS[\"bidirectional\"],\n",
    "        nlayers=MODEL_PARAMS[\"nlayers\"],\n",
    "        dropout=MODEL_PARAMS[\"dropout\"],\n",
    "        init_type=MODEL_PARAMS[\"init_type\"],\n",
    "        linear_bias=MODEL_PARAMS[\"linear_bias\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttentionLSTM(\n",
       "  (emb_layer): Embedding(47, 8, padding_idx=0)\n",
       "  (lstm): LSTM(8, 16, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (pred_layer): Linear(in_features=32, out_features=1, bias=False)\n",
       "  (self_att): Attn(\n",
       "    (main): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dpt): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lstm_model.parameters(), lr=MODEL_PARAMS[\"learning_rate\"], weight_decay=0.03\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, MODEL_PARAMS[\"scheduler_step\"], gamma=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = lstm_model.to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000, 34), (7000, 34), (7000, 34), (4, 34))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(TRAIN_DATA_PATH).shape, pd.read_csv(VALID_DATA_PATH).shape, pd.read_csv(\n",
    "    TEST_DATA_PATH\n",
    ").shape, pd.read_csv(selected_patients_path).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "multigpu_lst = []\n",
    "for gpu in range(n_gpus):\n",
    "    multigpu_lst.append(f\"cuda:{gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size mismatch, m1: [1 x 64], m2: [32 x 1] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
      "\n",
      "size mismatch, m1: [1 x 64], m2: [32 x 1] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
      "\n",
      "size mismatch, m1: [1 x 64], m2: [32 x 1] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
      "\n",
      "size mismatch, m1: [1 x 64], m2: [32 x 1] at /opt/conda/conda-bld/pytorch_1579022034529/work/aten/src/THC/generic/THCTensorMathBlas.cu:290\n",
      "\n",
      "\n",
      "> \u001b[0;32m/home/ec2-user/SageMaker/CMSAI_Research/model/toy_simple/synthetic_data_v3/open_source/imp_utils.py\u001b[0m(796)\u001b[0;36mget_lstm_features_and_shap_scores_mp\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    794 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    795 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 796 \u001b[0;31m        \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    797 \u001b[0;31m        \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    798 \u001b[0;31m        \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  len(test_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TypeError: object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/synthetic_data_v3/open_source/imp_utils.py\u001b[0m in \u001b[0;36mget_lstm_features_and_shap_scores_mp\u001b[0;34m(model, tr_dataloader, test, seq_len, shap_path, save_output, n_background, background_negative_only, test_positive_only, is_test_random, output_explainer, multigpu_lst)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/synthetic_data_v3/open_source/imp_utils.py\u001b[0m in \u001b[0;36mget_lstm_features_and_shap_scores_mp\u001b[0;34m(model, tr_dataloader, test, seq_len, shap_path, save_output, n_background, background_negative_only, test_positive_only, is_test_random, output_explainer, multigpu_lst)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a1e4f8d898ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mtest_positive_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_positive_only\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mis_test_random\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_test_random\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mmultigpu_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultigpu_lst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/CMSAI_Research/model/toy_simple/synthetic_data_v3/open_source/imp_utils.py\u001b[0m in \u001b[0;36mget_lstm_features_and_shap_scores_mp\u001b[0;34m(model, tr_dataloader, test, seq_len, shap_path, save_output, n_background, background_negative_only, test_positive_only, is_test_random, output_explainer, multigpu_lst)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model Training Results - Validation/Test\n",
    "valid_results = {}\n",
    "test_results = {}\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    best_valid = float(\"inf\")  # For early stopping\n",
    "    worse_valid = 0\n",
    "\n",
    "    # Save Model Parameters\n",
    "    with open(PARAMS_PATH, \"w\") as fp:\n",
    "        json.dump(MODEL_PARAMS, fp)\n",
    "\n",
    "    train_auc_lst = []\n",
    "    train_loss_lst = []\n",
    "\n",
    "    val_auc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_lrp_sim_lst = []\n",
    "    val_shap_sim_lst = []\n",
    "    val_lrp_shap_tau_lst = []\n",
    "    val_att_sim_lst = []\n",
    "\n",
    "    test_auc_lst = []\n",
    "    test_loss_lst = []\n",
    "    test_lrp_sim_lst = []\n",
    "    test_shap_sim_lst = []\n",
    "    test_lrp_shap_tau_lst = []\n",
    "    test_att_sim_lst = []\n",
    "\n",
    "    val_patient_ids, val_labels, val_idxed_text = imp_utils.get_eval_data(\n",
    "        valid_dataloader, MODEL_PARAMS[\"num_eval_val\"]\n",
    "    )\n",
    "    test_patient_ids, test_labels, test_idxed_text = imp_utils.get_eval_data(\n",
    "        test_dataloader, MODEL_PARAMS[\"num_eval_test\"]\n",
    "    )\n",
    "    # val_patient_ids, val_labels, val_idxed_text = next(iter(valid_dataloader))\n",
    "    # test_patient_ids, test_labels, test_idxed_text = next(iter(test_dataloader))\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        lstm_model.train()\n",
    "        # model training & perf evaluation\n",
    "        train_loss, train_auc = epoch_train_lstm(\n",
    "            lstm_model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            clip=MODEL_PARAMS[\"clip\"],\n",
    "            device=model_device,\n",
    "        )\n",
    "        train_auc_lst.append(train_auc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "\n",
    "        valid_loss, valid_auc = epoch_val_lstm(\n",
    "            lstm_model, valid_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        val_auc_lst.append(valid_auc)\n",
    "        val_loss_lst.append(valid_loss)\n",
    "\n",
    "        test_loss, test_auc = epoch_val_lstm(\n",
    "            lstm_model, test_dataloader, loss_function, device=model_device\n",
    "        )\n",
    "        test_auc_lst.append(test_auc)\n",
    "        test_loss_lst.append(test_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # save model\n",
    "        save_path = MODEL_SAVE_PATH_PATTERN.format(str(epoch).zfill(2))\n",
    "        torch.save(lstm_model.state_dict(), save_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # calculate relevancy and SHAP\n",
    "        lstm_model.eval()\n",
    "        lrp_model = LSTM_LRP_MultiLayer(lstm_model.cpu())\n",
    "\n",
    "        # Save valid/test results\n",
    "        valid_results[epoch] = {}\n",
    "        test_results[epoch] = {}\n",
    "\n",
    "        for sel_idx in range(len(val_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in val_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if val_patient_ids[sel_idx] not in valid_results[epoch]:\n",
    "                valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]] = {}\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"label\"] = val_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            valid_results[epoch][val_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        for sel_idx in range(len(test_labels)):\n",
    "            one_text = [\n",
    "                int(token.numpy())\n",
    "                for token in test_idxed_text[sel_idx]\n",
    "                if int(token.numpy()) != 0\n",
    "            ]\n",
    "            lrp_model.set_input(one_text)\n",
    "            lrp_model.forward_lrp()\n",
    "\n",
    "            Rx, Rx_rev, _ = lrp_model.lrp(one_text, 0, eps=1e-6, bias_factor=0)\n",
    "            R_words = np.sum(Rx + Rx_rev, axis=1)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df[\"lrp_scores\"] = R_words\n",
    "            df[\"idx\"] = one_text\n",
    "            df[\"seq_idx\"] = [x for x in range(len(one_text))]\n",
    "            df[\"token\"] = [lstm_model.vocab.itos(x) for x in one_text]\n",
    "            df[\"att_weights\"] = lrp_model.get_attn_values()\n",
    "\n",
    "            if test_patient_ids[sel_idx] not in test_results[epoch]:\n",
    "                test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]] = {}\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"label\"] = test_labels[\n",
    "                sel_idx\n",
    "            ]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"pred\"] = lrp_model.s[0]\n",
    "            test_results[epoch][test_patient_ids[sel_idx]][\"imp\"] = df.copy()\n",
    "\n",
    "        shap_start_time = time.time()\n",
    "        if len(multigpu_lst) <= 1:\n",
    "            (\n",
    "                val_features,\n",
    "                val_scores,\n",
    "                val_patients,\n",
    "            ) = imp_utils.get_lstm_features_and_shap_scores(\n",
    "                lstm_model.cuda(),\n",
    "                train_dataloader,\n",
    "                valid_dataloader,\n",
    "                SEQ_LEN,\n",
    "                \"\",\n",
    "                save_output=False,\n",
    "                n_background=MODEL_PARAMS[\"n_background\"],\n",
    "                background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "                n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "                test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "                is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            )\n",
    "\n",
    "            (\n",
    "                test_features,\n",
    "                test_scores,\n",
    "                test_patients,\n",
    "            ) = imp_utils.get_lstm_features_and_shap_scores(\n",
    "                lstm_model.cuda(),\n",
    "                train_dataloader,\n",
    "                test_dataloader,\n",
    "                SEQ_LEN,\n",
    "                \"\",\n",
    "                save_output=False,\n",
    "                n_background=MODEL_PARAMS[\"n_background\"],\n",
    "                background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "                n_test=MODEL_PARAMS[\"n_valid_examples\"],\n",
    "                test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "                is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "            )\n",
    "        else:\n",
    "            (\n",
    "                val_features,\n",
    "                val_scores,\n",
    "                val_patients,\n",
    "            ) = imp_utils.get_lstm_features_and_shap_scores_mp(\n",
    "                lstm_model.cpu(),\n",
    "                train_dataloader,\n",
    "                (val_patient_ids, val_labels, val_idxed_text),\n",
    "                SEQ_LEN,\n",
    "                \"\",\n",
    "                save_output=False,\n",
    "                n_background=MODEL_PARAMS[\"n_background\"],\n",
    "                background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "                test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "                is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "                multigpu_lst=multigpu_lst,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                test_features,\n",
    "                test_scores,\n",
    "                test_patients,\n",
    "            ) = imp_utils.get_lstm_features_and_shap_scores_mp(\n",
    "                lstm_model.cpu(),\n",
    "                train_dataloader,\n",
    "                (test_patient_ids, test_labels, test_idxed_text),\n",
    "                SEQ_LEN,\n",
    "                \"\",\n",
    "                save_output=False,\n",
    "                n_background=MODEL_PARAMS[\"n_background\"],\n",
    "                background_negative_only=MODEL_PARAMS[\"background_negative_only\"],\n",
    "                test_positive_only=MODEL_PARAMS[\"test_positive_only\"],\n",
    "                is_test_random=MODEL_PARAMS[\"is_test_random\"],\n",
    "                multigpu_lst=multigpu_lst,\n",
    "            )\n",
    "\n",
    "        shap_end_time = time.time()\n",
    "        shap_mins, shap_secs = epoch_time(shap_start_time, shap_end_time)\n",
    "\n",
    "        for idx, pid in enumerate(val_patients):\n",
    "            df = valid_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(val_scores[idx])\n",
    "            df[\"shap_scores\"] = val_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        for idx, pid in enumerate(test_patients):\n",
    "            df = test_results[epoch][pid][\"imp\"]\n",
    "            assert len(df) == len(test_scores[idx])\n",
    "            df[\"shap_scores\"] = test_scores[idx]\n",
    "            df = df[\n",
    "                [\"idx\", \"seq_idx\", \"token\", \"att_weights\", \"lrp_scores\", \"shap_scores\"]\n",
    "            ]\n",
    "            test_results[epoch][pid][\"imp\"] = df.copy()\n",
    "\n",
    "        # calculate similarity indexes for val\n",
    "        epoch_val_lrp_shap_t_corr = []\n",
    "        #         epoch_val_lrp_shap_rbo = []\n",
    "        epoch_val_lrp_sim = []\n",
    "        epoch_val_shap_sim = []\n",
    "        epoch_val_att_sim = []\n",
    "\n",
    "        for pid in valid_results[epoch].keys():\n",
    "            imp_df = valid_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            valid_results[epoch][pid][\"lrp_shap_t_corr\"] = imp_utils.get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            epoch_val_lrp_shap_t_corr.append(\n",
    "                valid_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "\n",
    "            # gt similarity\n",
    "            shap_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.shap_scores, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "            lrp_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.lrp_scores, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "            att_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.att_weights, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "\n",
    "            if shap_sim != -1:\n",
    "                epoch_val_shap_sim.append(shap_sim)\n",
    "            if lrp_sim != -1:\n",
    "                epoch_val_lrp_sim.append(lrp_sim)\n",
    "            if att_sim != -1:\n",
    "                epoch_val_att_sim.append(att_sim)\n",
    "\n",
    "            valid_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            valid_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "            valid_results[epoch][pid][\"att_sim\"] = att_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        valid_shap_path = IMP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(valid_results[epoch], fp)\n",
    "\n",
    "        val_lrp_shap_tau_lst.append(np.mean(epoch_val_lrp_shap_t_corr))\n",
    "        val_lrp_sim_lst.append(np.mean(epoch_val_lrp_sim))\n",
    "        val_shap_sim_lst.append(np.mean(epoch_val_shap_sim))\n",
    "        val_att_sim_lst.append(np.mean(epoch_val_att_sim))\n",
    "\n",
    "        # calculate similarity indexes for test\n",
    "        epoch_test_lrp_shap_t_corr = []\n",
    "        #         epoch_test_lrp_shap_rbo = []\n",
    "        epoch_test_lrp_sim = []\n",
    "        epoch_test_shap_sim = []\n",
    "        epoch_test_att_sim = []\n",
    "\n",
    "        for pid in test_results[epoch].keys():\n",
    "            imp_df = test_results[epoch][pid][\"imp\"]\n",
    "            imp_df[\"u_token\"] = [\n",
    "                str(seq) + \"_\" + str(token)\n",
    "                for seq, token in zip(imp_df[\"seq_idx\"], imp_df[\"token\"])\n",
    "            ]\n",
    "            test_results[epoch][pid][\"lrp_shap_t_corr\"] = imp_utils.get_wtau(\n",
    "                imp_df[\"lrp_scores\"], imp_df[\"shap_scores\"]\n",
    "            )\n",
    "\n",
    "            epoch_test_lrp_shap_t_corr.append(\n",
    "                test_results[epoch][pid][\"lrp_shap_t_corr\"]\n",
    "            )\n",
    "\n",
    "            # gt similarity\n",
    "            shap_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.shap_scores, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "            lrp_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.lrp_scores, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "            att_sim = imp_utils.get_intersection_similarity(\n",
    "                imp_df.att_weights, imp_df.token, freedom=0, is_synthetic=IS_SYNTHETIC\n",
    "            )\n",
    "            if shap_sim != -1:\n",
    "                epoch_test_shap_sim.append(shap_sim)\n",
    "            if lrp_sim != -1:\n",
    "                epoch_test_lrp_sim.append(lrp_sim)\n",
    "            if att_sim != -1:\n",
    "                epoch_test_att_sim.append(att_sim)\n",
    "            test_results[epoch][pid][\"lrp_sim\"] = lrp_sim\n",
    "            test_results[epoch][pid][\"shap_sim\"] = shap_sim\n",
    "            test_results[epoch][pid][\"att_sim\"] = att_sim\n",
    "\n",
    "        # Save training results to file.\n",
    "        test_shap_path = IMP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_shap_path, \"wb\") as fp:\n",
    "            pickle.dump(test_results[epoch], fp)\n",
    "\n",
    "        test_lrp_shap_tau_lst.append(np.mean(epoch_test_lrp_shap_t_corr))\n",
    "        test_lrp_sim_lst.append(np.mean(epoch_test_lrp_sim))\n",
    "        test_shap_sim_lst.append(np.mean(epoch_test_shap_sim))\n",
    "        test_att_sim_lst.append(np.mean(epoch_test_att_sim))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | \"\n",
    "            + f\"SHAP Time: {shap_mins}m {shap_secs}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} \"\n",
    "            + f\"\\t Val. Loss: {valid_loss:.4f} | Val. AUC: {valid_auc:.4f} \"\n",
    "            + f\"| Val LRP Sim: {np.mean(epoch_val_lrp_sim):.4f} | Val SHAP Sim: {np.mean(epoch_val_shap_sim):.4f}\"\n",
    "        )\n",
    "\n",
    "        if valid_loss < best_valid:\n",
    "            best_valid = valid_loss\n",
    "            worse_valid = 0\n",
    "        else:\n",
    "            worse_valid += 1\n",
    "            if worse_valid == EARLY_STOPPING_NUM:\n",
    "                N_EPOCHS = epoch + 1  # Update n_epochs if early stopping\n",
    "                print(\"EARLY STOP ------\")\n",
    "                break\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results[\"epoch\"] = [x for x in range(N_EPOCHS)]\n",
    "    df_results[\"train_AUC\"] = train_auc_lst\n",
    "    df_results[\"train_Loss\"] = train_loss_lst\n",
    "    df_results[\"val_AUC\"] = val_auc_lst\n",
    "    df_results[\"val_Loss\"] = val_loss_lst\n",
    "    df_results[\"test_AUC\"] = test_auc_lst\n",
    "    df_results[\"test_Loss\"] = test_loss_lst\n",
    "    #     df_results[\"val_lrp_shap_rbo\"] = val_lrp_shap_rbo_lst\n",
    "    df_results[\"val_lrp_shap_tau\"] = val_lrp_shap_tau_lst\n",
    "    #     df_results[\"test_lrp_shap_rbo\"] = test_lrp_shap_rbo_lst\n",
    "    df_results[\"test_lrp_shap_tau\"] = test_lrp_shap_tau_lst\n",
    "    df_results[\"val_GT_lrp_sim\"] = val_lrp_sim_lst\n",
    "    df_results[\"val_GT_shap_sim\"] = val_shap_sim_lst\n",
    "    df_results[\"val_GT_att_sim\"] = val_att_sim_lst\n",
    "    df_results[\"test_GT_lrp_sim\"] = test_lrp_sim_lst\n",
    "    df_results[\"test_GT_shap_sim\"] = test_shap_sim_lst\n",
    "    df_results[\"test_GT_att_sim\"] = test_att_sim_lst\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    # save results summary\n",
    "    df_results.to_csv(OUTPUT_RESULTS_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"Loading Training results....\")\n",
    "    df_results = pd.read_csv(OUTPUT_RESULTS_PATH)\n",
    "    df_results.set_index(\"epoch\", inplace=True)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Load valid results.\n",
    "        valid_imp_path = IMP_SAVE_DIR_PATTERN.format(\"val\", epoch)\n",
    "        with open(valid_imp_path, \"rb\") as fp:\n",
    "            valid_results[epoch] = pickle.load(fp)\n",
    "        # Load test results.\n",
    "        test_imp_path = IMP_SAVE_DIR_PATTERN.format(\"test\", epoch)\n",
    "        with open(test_imp_path, \"rb\") as fp:\n",
    "            test_results[epoch] = pickle.load(fp)\n",
    "    print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.shape)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10, 5)\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_AUC\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation AUC for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"train_Loss\", \"val_Loss\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Training vs Validation Loss for LSTM\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_shap_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\n",
    "    f\"SHAP vs GT Similarity on {MODEL_PARAMS['num_eval_val']} Validation Examples\"\n",
    ")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_GT_lrp_sim\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(f\"LRP vs GT Similarity on {MODEL_PARAMS['num_eval_val']} Validation Examples\")\n",
    "\n",
    "df_results.reset_index().plot(\n",
    "    figsize=figsize,\n",
    "    x=\"epoch\",\n",
    "    y=[\"val_lrp_shap_tau\", \"val_AUC\"],\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"LRP/SHAP with Kendall-T vs Validation AUC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "train_df = imp_utils.ret_label_df(df_results.reset_index(), \"train_AUC\", \"training\")\n",
    "val_df = imp_utils.ret_label_df(df_results.reset_index(), \"val_AUC\", \"validation\")\n",
    "plot_df = pd.concat([train_df, val_df])\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x=\"epoch\",\n",
    "    y=\"AUC\",\n",
    "    hue=\"dataset\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    "    style=\"dataset\",\n",
    ")\n",
    "ax.set_title(\"Prediction performance for Dot-Attention-LSTM model\")\n",
    "ax.set_xlabel(\"round\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(\n",
    "    data=df_results.reset_index(),\n",
    "    x=\"epoch\",\n",
    "    y=\"val_GT_shap_sim\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1,\n",
    ")\n",
    "ax.set(ylim=(0.2, 1.0))\n",
    "ax.set_xlabel(\"round\")\n",
    "ax.set_ylabel(\"SHAP similarity to ground truth labels\", color=\"mediumslateblue\")\n",
    "for i, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"mediumslateblue\")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(\n",
    "    data=df_results.reset_index(),\n",
    "    x=\"epoch\",\n",
    "    y=\"val_AUC\",\n",
    "    ax=ax2,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax.set_title(\"Overlap between ground truth and SHAP features on validation set\")\n",
    "ax.title.set_position([0.5, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(\n",
    "    data=df_results.reset_index(),\n",
    "    x=\"epoch\",\n",
    "    y=\"test_GT_shap_sim\",\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1,\n",
    ")\n",
    "ax.set(ylim=(0.2, 1.0))\n",
    "ax.set_xlabel(\"round\")\n",
    "ax.set_ylabel(\"SHAP similarity to ground truth labels\", color=\"mediumslateblue\")\n",
    "for i, label in enumerate(ax.get_yticklabels()):\n",
    "    label.set_color(\"mediumslateblue\")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(\n",
    "    data=df_results.reset_index(),\n",
    "    x=\"epoch\",\n",
    "    y=\"test_AUC\",\n",
    "    ax=ax2,\n",
    "    color=\"black\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "ax.set_title(\"Overlap between ground truth and SHAP features on test set\")\n",
    "ax.title.set_position([0.5, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best epoch first\n",
    "best_epoch = imp_utils.get_best_epoch(df_results, by=\"val_AUC\")\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = valid_results[best_epoch]\n",
    "all_features = []\n",
    "lrp_all_scores = []\n",
    "shap_all_scores = []\n",
    "attn_all_scores = []\n",
    "for pat_id, scores in val_scores.items():\n",
    "    shap_all_scores.append(\n",
    "        valid_results[best_epoch][pat_id][\"imp\"][\"shap_scores\"].tolist()\n",
    "    )\n",
    "    lrp_all_scores.append(\n",
    "        valid_results[best_epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist()\n",
    "    )\n",
    "    attn_all_scores.append(\n",
    "        valid_results[best_epoch][pat_id][\"imp\"][\"att_weights\"].tolist()\n",
    "    )\n",
    "    all_features.append(valid_results[best_epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "\n",
    "shap_global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, shap_all_scores, absolute=True\n",
    ")\n",
    "lrp_global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, lrp_all_scores, absolute=True\n",
    ")\n",
    "\n",
    "att_global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, attn_all_scores, absolute=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in shap_global_scores.keys():\n",
    "    rows.append(\n",
    "        [\n",
    "            token,\n",
    "            shap_global_scores[token],\n",
    "            lrp_global_scores[token],\n",
    "            att_global_scores[token],\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "als_df = pd.DataFrame(rows)\n",
    "als_df.columns = [\"features\", \"shap_scores\", \"lrp_scores\", \"attention_weights\"]\n",
    "als_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for pid in valid_results[best_epoch].keys():\n",
    "    imp_df = valid_results[best_epoch][pid][\"imp\"]\n",
    "\n",
    "    pid_data = valid_results[best_epoch][pid]\n",
    "\n",
    "    rows.append([pid, pid_data[\"lrp_sim\"], pid_data[\"shap_sim\"], pid_data[\"att_sim\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = pd.DataFrame(rows)\n",
    "sims.columns = [\"patient_id\", \"lrp_sim\", \"shap_sim\", \"att_sim\"]\n",
    "sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True, figsize=(10, 5))\n",
    "sns.histplot(\n",
    "    sims.lrp_sim, ax=ax1, binwidth=0.04, color=\"mediumslateblue\", stat=\"probability\"\n",
    ")\n",
    "ax1.set_xlabel(\"similarity to ground truth\")\n",
    "ax1.set_title(\"LRP\")\n",
    "ax1.set_ylabel(\"distribution\")\n",
    "sns.histplot(\n",
    "    sims.shap_sim, ax=ax2, binwidth=0.04, color=\"mediumslateblue\", stat=\"probability\"\n",
    ")\n",
    "ax2.set_xlabel(\"similarity to ground truth\")\n",
    "ax2.set_title(\"SHAP\")\n",
    "sns.histplot(\n",
    "    sims.att_sim, ax=ax3, binwidth=0.04, color=\"mediumslateblue\", stat=\"probability\"\n",
    ")\n",
    "ax3.set_xlabel(\"similarity to ground truth\")\n",
    "ax3.set_title(\"Attention\")\n",
    "\n",
    "plt.suptitle(\"Similarity to ground truth histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_df.sort_values(\"lrp_scores\", ascending=True, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "axes = fig.subplots(ncols=3, sharey=True)\n",
    "axes[0].barh(\n",
    "    als_df.features,\n",
    "    als_df.lrp_scores,\n",
    "    color=\"mediumslateblue\",\n",
    "    align=\"center\",\n",
    "    zorder=10,\n",
    ")\n",
    "axes[0].set(title=\"LRP\", xlabel=\"LRP scores\")\n",
    "axes[1].barh(als_df.features, als_df.shap_scores, color=\"orange\", align=\"center\")\n",
    "axes[1].set(title=\"SHAP\", xlabel=\"SHAP scores\")\n",
    "# axes[0].invert_xaxis()\n",
    "# axes[1].set_yticks([])\n",
    "\n",
    "axes[2].barh(\n",
    "    als_df.features, als_df.attention_weights, color=\"deepskyblue\", align=\"center\"\n",
    ")\n",
    "axes[2].set(title=\"Dot-Product Attention\", xlabel=\"Attention weights\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.09)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global For each Explainability Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_scores = []\n",
    "for pat_id, scores in val_scores.items():\n",
    "    all_scores.append(valid_results[best_epoch][pat_id][\"imp\"][\"lrp_scores\"].tolist())\n",
    "    all_features.append(valid_results[best_epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, all_scores, absolute=True\n",
    ")\n",
    "title = f\"LRP (Epoch={best_epoch})\"\n",
    "imp_utils.plot_global_feature_importance(global_scores, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_scores = []\n",
    "for pat_id, scores in val_scores.items():\n",
    "    all_scores.append(valid_results[best_epoch][pat_id][\"imp\"][\"shap_scores\"].tolist())\n",
    "    all_features.append(valid_results[best_epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "    # print(valid_results[best_epoch][pat_id][\"imp\"][\"token\"])\n",
    "global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, all_scores, absolute=True\n",
    ")\n",
    "title = f\"SHAP (Epoch={best_epoch})\"\n",
    "imp_utils.plot_global_feature_importance(global_scores, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "all_scores = []\n",
    "for pat_id, scores in val_scores.items():\n",
    "    all_scores.append(valid_results[best_epoch][pat_id][\"imp\"][\"att_weights\"].tolist())\n",
    "    all_features.append(valid_results[best_epoch][pat_id][\"imp\"][\"token\"].tolist())\n",
    "global_scores = imp_utils.get_global_feature_importance(\n",
    "    all_features, all_scores, absolute=True\n",
    ")\n",
    "title = f\"Attention (Epoch={best_epoch})\"\n",
    "imp_utils.plot_global_feature_importance(global_scores, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Feature Importance (Per-Observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = {}\n",
    "for pat_id in SELECTED_PATIENTS:\n",
    "    example_results[pat_id] = {}\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if pat_id in test_results[epoch].keys():\n",
    "            example_results[pat_id][epoch] = test_results[epoch][pat_id]\n",
    "example_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    df[\"lrp_scores\"] = example_results[uid][best_epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][best_epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][best_epoch][\"imp\"][\"seq_idx\"]\n",
    "    df[\"shap_scores\"] = example_results[uid][best_epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"att_scores\"] = example_results[uid][best_epoch][\"imp\"][\"att_weights\"]\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    df[[\"lrp_scores\", \"shap_scores\", \"att_scores\"]].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10)\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP/SHAP/Attn scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot LSTM LRP & SHAP, LRP & SHAP & Attention Scores Separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [best_epoch]\n",
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"shap_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10), legend=False\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"SHAP SCORES for {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.axisbelow\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"lrp_scores\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    # df[example_results[uid].keys()].plot.bar(figsize=(17, 7))\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10), legend=False\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in example_results.keys():\n",
    "    df = pd.DataFrame()\n",
    "    for epoch in example_results[uid].keys():\n",
    "        df[epoch] = example_results[uid][epoch][\"imp\"][\"att_weights\"]\n",
    "    df[\"token\"] = example_results[uid][epoch][\"imp\"][\"token\"]\n",
    "    df[\"seq_idx\"] = example_results[uid][epoch][\"imp\"][\"seq_idx\"]\n",
    "\n",
    "    df[selected_epochs].plot.bar(\n",
    "        align=\"center\", width=0.9, edgecolor=\"black\", figsize=(17, 10), legend=False\n",
    "    )\n",
    "    plt.xticks(df[\"seq_idx\"], df.token.values.tolist(), rotation=90, zorder=1)\n",
    "\n",
    "    lab = example_results[uid][best_epoch][\"label\"]\n",
    "    pred = np.round(\n",
    "        torch.sigmoid(torch.tensor(example_results[uid][best_epoch][\"pred\"])).numpy(), 4\n",
    "    )\n",
    "\n",
    "    plt.title(f\"LRP scores {uid}: label={lab[0]}, pred={pred:.4f}\")\n",
    "    plt.grid(color=\"lightgray\", linestyle=\"--\", zorder=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
